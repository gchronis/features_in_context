{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3251a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 22.9.0\n",
      "  latest version: 23.7.4\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Retrieving notices: ...working... done\n"
     ]
    }
   ],
   "source": [
    "!conda install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a46e36f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import re\n",
    "import csv   \n",
    "\n",
    "\n",
    "\n",
    "def get_context(token_ids, target_position, sequence_length=128):\n",
    "    \"\"\"\n",
    "    Given a text containing a target word, return the sentence snippet which surrounds the target word\n",
    "    (and the target word's position in the snippet).\n",
    "\n",
    "    :param token_ids: list of token ids (for an entire line of text)\n",
    "    :param target_position: index of the target word's position in `tokens`\n",
    "    :param sequence_length: desired length for output sequence (e.g. 128, 256, 512)\n",
    "    :return: (context_ids, new_target_position)\n",
    "                context_ids: list of token ids for the output sequence\n",
    "                new_target_position: index of the target word's position in `context_ids`\n",
    "    \"\"\"\n",
    "    # -2 as [CLS] and [SEP] tokens will be added later; /2 as it's a one-sided window\n",
    "    window_size = int((sequence_length - 2) / 2)\n",
    "    context_start = max([0, target_position - window_size])\n",
    "    padding_offset = max([0, window_size - target_position])\n",
    "    padding_offset += max([0, target_position + window_size - len(token_ids)])\n",
    "\n",
    "    context_ids = token_ids[context_start:target_position + window_size]\n",
    "    context_ids += padding_offset * [0]\n",
    "\n",
    "    new_target_position = target_position - context_start\n",
    "\n",
    "    return context_ids, new_target_position\n",
    "\n",
    "\n",
    "def collect_from_coha(target_words,\n",
    "                      decades,\n",
    "                      sequence_length,\n",
    "                      pretrained_weights='models/bert-base-uncased',\n",
    "                      coha_dir='data/coha',\n",
    "                      output_path=None,\n",
    "                      buffer_size=1024):\n",
    "    \"\"\"\n",
    "    Collect usages of target words from the COHA dataset.\n",
    "\n",
    "    :param target_words: list of words whose usages are to be collected\n",
    "    :param decades: list of year integers (e.g. list(np.arange(1910, 2001, 10)))\n",
    "    :param sequence_length: the number of tokens in the context of a word occurrence\n",
    "    :param pretrained_weights: path to model folder with weights and config file\n",
    "    :param coha_dir: path to COHA directory (containing `all_1810.txt`, ..., `all_2000.txt`)\n",
    "    :param output_path: path to output file for `usages` dictionary. If provided, data is stored\n",
    "                        in this file incrementally (use e.g. to avoid out of memory errors)\n",
    "    :param buffer_size: (max) number of usages to process in a single model run\n",
    "    :return: usages: a dictionary from target words to lists of usage tuples\n",
    "                     lemma -> [(vector, sentence, word_position, decade), (v, s, p, d), ...]\n",
    "    \"\"\"\n",
    "\n",
    "    # load model and tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained(pretrained_weights)\n",
    "    model = BertModel.from_pretrained(pretrained_weights)\n",
    "    if torch.cuda.is_available():\n",
    "        model.to('cuda')\n",
    "\n",
    "    # build word-index vocabulary for target words\n",
    "    i2w = {}\n",
    "    for t, t_id in zip(target_words, tokenizer.encode(' '.join(target_words))[1:-1]): #changed for new hugginface api\n",
    "        i2w[t_id] = t\n",
    "\n",
    "    # buffers for batch processing\n",
    "    batch_input_ids = []\n",
    "    batch_tokens = []\n",
    "    batch_pos = []\n",
    "    batch_snippets = []\n",
    "    batch_decades = []\n",
    "\n",
    "    usages = defaultdict(list)  # w -> (vector, sentence, word_position, decade)\n",
    "    \n",
    "    # do collection\n",
    "    for T, decade in enumerate(decades):\n",
    "        # one time interval at a time\n",
    "        print('Decade {}...'.format(decade))\n",
    "\n",
    "        \n",
    "        ### gabriella changes\n",
    "        ### my coha is organized differently. \n",
    "        ### the decades have random numbers for the alphabet index places , so i have to use regex\n",
    "        ### to ignore that. \n",
    "        print(coha_dir)\n",
    "        print(decade)\n",
    "        my_regex = r'text_' + re.escape(str(decade)) + 's.*'\n",
    "\n",
    "        #print(\"running through decade \", decade)\n",
    "\n",
    "        # iterate through directories\n",
    "        for decade_dir in os.listdir(coha_dir):\n",
    "\n",
    "            if re.match(my_regex, decade_dir):\n",
    "                # get all the text files for that decade\n",
    "                # iterate through text files for this decade\n",
    "                this_decade_files = os.listdir(os.path.join(coha_dir, decade_dir))\n",
    "                for filename in tqdm(this_decade_files):\n",
    "                    #print(filename)\n",
    "                    \n",
    "                    with open(os.path.join(coha_dir, decade_dir, filename), 'r') as f:\n",
    "                        lines = f.readlines()\n",
    "                        #print(\"gets here\")\n",
    "\n",
    "                        # get the usages from this file\n",
    "                        for L, line in enumerate(lines):\n",
    "                            #print(\"gets to line: \", L)\n",
    "\n",
    "\n",
    "\n",
    "                            # tokenize line and convert to token ids\n",
    "                            tokens = tokenizer.encode(line)\n",
    "\n",
    "                            for pos, token in enumerate(tokens):\n",
    "                                #print(token)\n",
    "                                # store usage info of target words only\n",
    "                                if token in i2w:\n",
    "                                    context_ids, pos_in_context = get_context(tokens, pos, sequence_length)\n",
    "\n",
    "                                    input_ids = [101] + context_ids + [102]\n",
    "\n",
    "\n",
    "                                    # convert later to save storage space\n",
    "                                    snippet = tokenizer.convert_ids_to_tokens(context_ids)\n",
    "                                    #print(i2w[token])\n",
    "                                    #print(' '.join(snippet))\n",
    "\n",
    "                                    # add usage info to buffers\n",
    "                                    batch_input_ids.append(input_ids)\n",
    "                                    batch_tokens.append(i2w[token])\n",
    "                                    batch_pos.append(pos_in_context)\n",
    "                                    batch_snippets.append(snippet)\n",
    "                                    batch_decades.append(decade)\n",
    "\n",
    "                                # if the buffers are full...             or if we're at the end of the dataset\n",
    "                                if (len(batch_input_ids) >= buffer_size) or (L == len(lines) - 1 and T == len(decades) - 1):\n",
    "\n",
    "#                                     with torch.no_grad():\n",
    "#                                         # collect list of input ids into a single batch tensor\n",
    "#                                         input_ids_tensor = torch.tensor(batch_input_ids)\n",
    "#                                         if torch.cuda.is_available():\n",
    "#                                             input_ids_tensor = input_ids_tensor.to('cuda')\n",
    "\n",
    "#                                         # run usages through language model\n",
    "#                                         outputs = model(input_ids_tensor,  output_hidden_states=True )\n",
    "#                                         #print(len(outputs.hidden_states)) # items in the tuple = 1 + num layers\n",
    "#                                         if torch.cuda.is_available():\n",
    "#                                             hidden_states = [l.detach().cpu().clone().numpy() for l in outputs[2]]\n",
    "#                                         else:\n",
    "#                                             #print(\"fjekl\")\n",
    "#                                             hidden_states = [l.clone().numpy() for l in outputs.hidden_states]\n",
    "\n",
    "#                                         # get usage vectors from hidden states\n",
    "#                                         hidden_states = np.stack(hidden_states)  # (13, B, |s|, 768)\n",
    "#                                         print('Expected hidden states size: (13, B, |s|, 768). Got {}'.format(hidden_states.shape))\n",
    "#                                         # usage_vectors = np.sum(hidden_states, 0)  # (B, |s|, 768)\n",
    "#                                         # usage_vectors = hidden_states.view(hidden_states.shape[1],\n",
    "#                                         #                                    hidden_states.shape[2],\n",
    "#                                         #                                    -1)\n",
    "#                                         usage_vectors = np.sum(hidden_states[1:, :, :, :], axis=0)\n",
    "#                                         # usage_vectors = hidden_states.reshape((hidden_states.shape[1], hidden_states.shape[2], -1))\n",
    "#                                         #print(\"makes usage vectors\")\n",
    "#                                         print(usage_vectors.shape)\n",
    "\n",
    "#                                     if output_path and os.path.isfile(output_path):\n",
    "#                                         with open(output_path, 'rb') as f:\n",
    "#                                             usages = pickle.load(f)\n",
    "\n",
    "                                    # store usage tuples in a dictionary: lemma -> (vector, snippet, position, decade)\n",
    "                                    #print(len(batch_input_ids))\n",
    "                                    for b in np.arange(len(batch_input_ids)):\n",
    "                                        #usage_vector = usage_vectors[b, batch_pos[b]+1, :] # get the right position\n",
    "                                        usages[batch_tokens[b]].append(\n",
    "                                             (batch_snippets[b], batch_pos[b], batch_decades[b]))\n",
    "                                    \n",
    "#                                     print(usages)\n",
    "                        \n",
    "                                    # finally, empty the batch buffers\n",
    "                                    batch_input_ids, batch_tokens, batch_pos, batch_snippets, batch_decades = [], [], [], [], []\n",
    "                        \n",
    "        print(\"saving usages for decade\")\n",
    "        if os.path.exists(output_path):\n",
    "            append_write = 'ab' # append if already exists\n",
    "        else:\n",
    "            append_write = 'wb' # make a new file if not\n",
    "\n",
    "        # and store data incrementally\n",
    "        if output_path:\n",
    "            #print(append_write)\n",
    "            with open(output_path, append_write) as f:\n",
    "                pickle.dump(usages, file=f)\n",
    "    \n",
    "\n",
    "    return usages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "815298a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target words: we want to collect tokens of each of these words from COHA\n",
    "\n",
    "targets = ['net', 'virtual', 'disk', 'card', 'optical', 'virus',\n",
    "           'signal', 'mirror', 'energy', 'compact', 'leaf',\n",
    "           'brick', 'federal', 'sphere', 'coach', 'spine', 'parent', 'sleep']\n",
    "\n",
    "decades = [decade for decade in np.arange(1910, 2009, 10)]\n",
    "\n",
    "buffer_size=1024\n",
    "sequence_length=128\n",
    "\n",
    "\n",
    "coha_dir = '/home/shared/corpora/Corpus of Historical American English/TEXTS'\n",
    "bert_dir = 'bert-base-uncased'\n",
    "output_dir = '../data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0889b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decade 1910...\n",
      "/home/shared/corpora/Corpus of Historical American English/TEXTS\n",
      "1910\n",
      "saving usages for decade\n",
      "saving usages for decade\n",
      "saving usages for decade\n",
      "saving usages for decade\n",
      "saving usages for decade\n",
      "saving usages for decade\n",
      "saving usages for decade\n",
      "saving usages for decade\n",
      "saving usages for decade\n",
      "saving usages for decade\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                  | 0/3355 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (7482 > 512). Running this sequence through the model will result in indexing errors\n",
      "  3%|█████▋                                                                                                                                                                  | 114/3355 [00:08<03:51, 13.99it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m### collect just the usages and not the vectors. \u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m decade \u001b[38;5;129;01min\u001b[39;00m decades:\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mcollect_from_coha\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                      \u001b[49m\u001b[43m[\u001b[49m\u001b[43mdecade\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                      \u001b[49m\u001b[43msequence_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msequence_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mpretrained_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mcoha_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoha_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                      \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m/usages_16_len\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m.dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecade\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffer_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 119\u001b[0m, in \u001b[0;36mcollect_from_coha\u001b[0;34m(target_words, decades, sequence_length, pretrained_weights, coha_dir, output_path, buffer_size)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m#print(\"gets here\")\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# get the usages from this file\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m L, line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(lines):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;66;03m#print(\"gets to line: \", L)\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \n\u001b[1;32m    116\u001b[0m \n\u001b[1;32m    117\u001b[0m \n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m# tokenize line and convert to token ids\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m pos, token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tokens):\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;66;03m#print(token)\u001b[39;00m\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;66;03m# store usage info of target words only\u001b[39;00m\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m i2w:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2332\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m   2295\u001b[0m \u001b[38;5;129m@add_end_docstrings\u001b[39m(\n\u001b[1;32m   2296\u001b[0m     ENCODE_KWARGS_DOCSTRING,\n\u001b[1;32m   2297\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2315\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2316\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m   2317\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2318\u001b[0m \u001b[38;5;124;03m    Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\u001b[39;00m\n\u001b[1;32m   2319\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2330\u001b[0m \u001b[38;5;124;03m            method).\u001b[39;00m\n\u001b[1;32m   2331\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2332\u001b[0m     encoded_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2335\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2337\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2338\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2339\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2340\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2341\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2342\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m encoded_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2740\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2730\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2731\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2732\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   2733\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2737\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2738\u001b[0m )\n\u001b[0;32m-> 2740\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2742\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2743\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2744\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2745\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2755\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2756\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2757\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2758\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2759\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/tokenization_utils.py:649\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_offsets_mapping:\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    642\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    643\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/huggingface/transformers/pull/2674\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 649\u001b[0m first_ids \u001b[38;5;241m=\u001b[39m \u001b[43mget_input_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    650\u001b[0m second_ids \u001b[38;5;241m=\u001b[39m get_input_ids(text_pair) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_for_model(\n\u001b[1;32m    653\u001b[0m     first_ids,\n\u001b[1;32m    654\u001b[0m     pair_ids\u001b[38;5;241m=\u001b[39msecond_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    668\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    669\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/tokenization_utils.py:616\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus.<locals>.get_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_input_ids\u001b[39m(text):\n\u001b[1;32m    615\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 616\u001b[0m         tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    617\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(tokens)\n\u001b[1;32m    618\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/tokenization_utils.py:517\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.tokenize\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m    514\u001b[0m     text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(pattern, \u001b[38;5;28;01mlambda\u001b[39;00m m: m\u001b[38;5;241m.\u001b[39mgroups()[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m m\u001b[38;5;241m.\u001b[39mgroups()[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mlower(), text)\n\u001b[1;32m    516\u001b[0m no_split_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munique_no_split_tokens)\n\u001b[0;32m--> 517\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokens_trie\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;66;03m# [\"This is something\", \"<special_token_1>\", \"  else\"]\u001b[39;00m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tokens):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/tokenization_utils.py:226\u001b[0m, in \u001b[0;36mTrie.split\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[38;5;28;01mdel\u001b[39;00m states[start]\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;66;03m# If this character is a starting character within the trie\u001b[39;00m\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;66;03m# start keeping track of this partial match.\u001b[39;00m\n\u001b[0;32m--> 226\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m current \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m skip \u001b[38;5;129;01mand\u001b[39;00m current_char \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata:\n\u001b[1;32m    227\u001b[0m         states[current] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[current_char]\n\u001b[1;32m    229\u001b[0m \u001b[38;5;66;03m# We have a cut at the end with states.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### collect just the usages and not the vectors. \n",
    "\n",
    "for decade in decades:\n",
    "    collect_from_coha(targets,\n",
    "                      [decade],\n",
    "                      sequence_length=sequence_length,\n",
    "                      pretrained_weights=bert_dir,\n",
    "                      coha_dir=coha_dir,\n",
    "                      output_path='{}/usages_16_len{}_{}.dict'.format(output_dir, sequence_length, decade),\n",
    "                      buffer_size=buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82d37886",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "\n",
    "l.append([1,2,3])\n",
    "l.append([4,5,6])\n",
    "l\n",
    "\n",
    "with open('file', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d959de78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# import argparse\n",
    "# import numpy as np\n",
    "# from usage_collector import collect_from_coha\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--seqlen', type=int, default=128)\n",
    "# parser.add_argument('--bertdir', type=str, default='models/bert-base-uncased')\n",
    "# parser.add_argument('--cohadir', type=str, default='data/coha')\n",
    "# parser.add_argument('--outdir', type=str, default='data')\n",
    "# parser.add_argument('--buffer', type=int, default=1024)\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# targets = ['net', 'virtual', 'disk', 'card', 'optical', 'virus',\n",
    "#            'signal', 'mirror', 'energy', 'compact', 'leaf',\n",
    "#            'brick', 'federal', 'sphere', 'coach', 'spine', 'parent', 'sleep']\n",
    "\n",
    "# print('{}\\nSEQUENCE LENGTH: {}\\n{}'.format('-' * 30, args.seqlen, '-' * 30))\n",
    "\n",
    "# # decades = list(np.arange(1910, 2001, 10))\n",
    "# # decades = list(np.arange(1810, 1811, 10))\n",
    "\n",
    "# for decade in np.arange(1910, 2009, 10):\n",
    "#     collect_from_coha(targets,\n",
    "#                       [decade],\n",
    "#                       sequence_length=args.seqlen,\n",
    "#                       pretrained_weights=args.bertdir,\n",
    "#                       coha_dir=args.cohadir,\n",
    "#                       output_path='{}/concat/usages_16_len{}_{}.dict'.format(args.outdir, args.seqlen, decade),\n",
    "#                       buffer_size=args.buffer)\n",
    "\n",
    "#     # # Save usages\n",
    "#     # with open('{}/concat/usages_16_len{}_{}.dict'.format(args.outdir, args.seqlen, decade), 'wb') as f:\n",
    "#     #     pickle.dump(usages, file=f)\n",
    "#     # usages = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d167699",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/gabriellachronis/Box Sync/src/cwr4lsc/test_output', 'rb') as file:\n",
    "\n",
    "    # dump information to that file\n",
    "    data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31073d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gabriella_cwr4lsc",
   "language": "python",
   "name": "gabriella_cwr4lsc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
