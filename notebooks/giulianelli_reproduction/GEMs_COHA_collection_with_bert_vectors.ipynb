{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3251a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 22.9.0\n",
      "  latest version: 23.7.4\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Retrieving notices: ...working... done\n"
     ]
    }
   ],
   "source": [
    "!conda install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a46e36f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import re\n",
    "import csv   \n",
    "\n",
    "\n",
    "\n",
    "def get_context(token_ids, target_position, sequence_length=128):\n",
    "    \"\"\"\n",
    "    Given a text containing a target word, return the sentence snippet which surrounds the target word\n",
    "    (and the target word's position in the snippet).\n",
    "\n",
    "    :param token_ids: list of token ids (for an entire line of text)\n",
    "    :param target_position: index of the target word's position in `tokens`\n",
    "    :param sequence_length: desired length for output sequence (e.g. 128, 256, 512)\n",
    "    :return: (context_ids, new_target_position)\n",
    "                context_ids: list of token ids for the output sequence\n",
    "                new_target_position: index of the target word's position in `context_ids`\n",
    "    \"\"\"\n",
    "    # -2 as [CLS] and [SEP] tokens will be added later; /2 as it's a one-sided window\n",
    "    window_size = int((sequence_length - 2) / 2)\n",
    "    context_start = max([0, target_position - window_size])\n",
    "    padding_offset = max([0, window_size - target_position])\n",
    "    padding_offset += max([0, target_position + window_size - len(token_ids)])\n",
    "\n",
    "    context_ids = token_ids[context_start:target_position + window_size]\n",
    "    context_ids += padding_offset * [0]\n",
    "\n",
    "    new_target_position = target_position - context_start\n",
    "\n",
    "    return context_ids, new_target_position\n",
    "\n",
    "def get_usage_vectors( model,\n",
    "                       batch_input_ids,\n",
    "                       batch_tokens,\n",
    "                       batch_snippets,\n",
    "                       batch_pos, \n",
    "                       batch_decades):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # collect list of input ids into a single batch tensor\n",
    "        input_ids_tensor = torch.tensor(batch_input_ids)\n",
    "        if torch.cuda.is_available():\n",
    "            input_ids_tensor = input_ids_tensor.to('cuda')\n",
    "\n",
    "            \n",
    "        #print(input_ids_tensor)\n",
    "        # run usages through language model\n",
    "        outputs = model(input_ids_tensor,  output_hidden_states=True )\n",
    "        #print(len(outputs.hidden_states)) # items in the tuple = 1 + num layers\n",
    "        if torch.cuda.is_available():\n",
    "            hidden_states = [l.detach().cpu().clone().numpy() for l in outputs[2]]\n",
    "        else:\n",
    "            #print(\"fjekl\")\n",
    "            hidden_states = [l.clone().numpy() for l in outputs.hidden_states]\n",
    "\n",
    "        # get usage vectors from hidden states\n",
    "        hidden_states = np.stack(hidden_states)  # (13, B, |s|, 768)\n",
    "        #print('Expected hidden states size: (13, B, |s|, 768). Got {}'.format(hidden_states.shape))\n",
    "        # usage_vectors = np.sum(hidden_states, 0)  # (B, |s|, 768)\n",
    "        # usage_vectors = hidden_states.view(hidden_states.shape[1],\n",
    "        #                                    hidden_states.shape[2],\n",
    "        #                                    -1)\n",
    "        usage_vectors = np.sum(hidden_states[1:, :, :, :], axis=0)\n",
    "        \n",
    "        # usage_vectors = hidden_states.reshape((hidden_states.shape[1], hidden_states.shape[2], -1))\n",
    "        #print(usage_vectors.shape)\n",
    "        return usage_vectors\n",
    "\n",
    "\n",
    "def collect_from_coha(target_words,\n",
    "                      decades,\n",
    "                      sequence_length,\n",
    "                      pretrained_weights='models/bert-base-uncased',\n",
    "                      coha_dir='data/coha',\n",
    "                      output_path=None,\n",
    "                      buffer_size=1024):\n",
    "    \"\"\"\n",
    "    Collect usages of target words from the COHA dataset.\n",
    "\n",
    "    :param target_words: list of words whose usages are to be collected\n",
    "    :param decades: list of year integers (e.g. list(np.arange(1910, 2001, 10)))\n",
    "    :param sequence_length: the number of tokens in the context of a word occurrence\n",
    "    :param pretrained_weights: path to model folder with weights and config file\n",
    "    :param coha_dir: path to COHA directory (containing `all_1810.txt`, ..., `all_2000.txt`)\n",
    "    :param output_path: path to output file for `usages` dictionary. If provided, data is stored\n",
    "                        in this file incrementally (use e.g. to avoid out of memory errors)\n",
    "    :param buffer_size: (max) number of usages to process in a single model run\n",
    "    :return: usages: a dictionary from target words to lists of usage tuples\n",
    "                     lemma -> [(vector, sentence, word_position, decade), (v, s, p, d), ...]\n",
    "    \"\"\"\n",
    "\n",
    "    # load model and tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained(pretrained_weights)\n",
    "    model = BertModel.from_pretrained(pretrained_weights)\n",
    "    if torch.cuda.is_available():\n",
    "        model.to('cuda')\n",
    "\n",
    "    # build word-index vocabulary for target words\n",
    "    i2w = {}\n",
    "    for t, t_id in zip(target_words, tokenizer.encode(' '.join(target_words))[1:-1]): #changed for new hugginface api\n",
    "        i2w[t_id] = t\n",
    "\n",
    "    # buffers for batch processing\n",
    "    batch_input_ids = []\n",
    "    batch_tokens = []\n",
    "    batch_pos = []\n",
    "    batch_snippets = []\n",
    "    batch_decades = []\n",
    "\n",
    "    usages = defaultdict(list)  # w -> (vector, sentence, word_position, decade)\n",
    "    \n",
    "    # do collection\n",
    "    \n",
    "    print(len(decades))\n",
    "    for T, decade in enumerate(decades):\n",
    "        # one time interval at a time\n",
    "        print('Decade {}...'.format(decade))\n",
    "\n",
    "        \n",
    "        ### gabriella changes\n",
    "        ### my coha is organized differently. \n",
    "        ### the decades have random numbers for the alphabet index places , so i have to use regex\n",
    "        ### to ignore that. \n",
    "        print(coha_dir)\n",
    "        print(decade)\n",
    "        my_regex = r'text_' + re.escape(str(decade)) + 's.*'\n",
    "\n",
    "        #print(\"running through decade \", decade)\n",
    "\n",
    "        # iterate through directories\n",
    "        for decade_dir in os.listdir(coha_dir):\n",
    "\n",
    "            if re.match(my_regex, decade_dir):\n",
    "                # get all the text files for that decade\n",
    "                # iterate through text files for this decade\n",
    "                this_decade_files = os.listdir(os.path.join(coha_dir, decade_dir))\n",
    "                for F, filename in enumerate(tqdm(this_decade_files)):\n",
    "                    #print(filename)\n",
    "                    \n",
    "                    with open(os.path.join(coha_dir, decade_dir, filename), 'r') as f:\n",
    "                        lines = f.readlines()\n",
    "                        #print(\"gets here\")\n",
    "\n",
    "                        # get the usages from this file\n",
    "                        for L, line in enumerate(lines):\n",
    "                            #print(\"gets to line: \", L)\n",
    "\n",
    "\n",
    "\n",
    "                            # tokenize line and convert to token ids\n",
    "                            tokens = tokenizer.encode(line)\n",
    "\n",
    "                            for pos, token in enumerate(tokens):\n",
    "                                #print(token)\n",
    "                                # store usage info of target words only\n",
    "                                if token in i2w:\n",
    "                                    context_ids, pos_in_context = get_context(tokens, pos, sequence_length)\n",
    "\n",
    "                                    input_ids = [101] + context_ids + [102]\n",
    "\n",
    "\n",
    "                                    # convert later to save storage space\n",
    "                                    snippet = tokenizer.convert_ids_to_tokens(context_ids)\n",
    "                                    #print(i2w[token])\n",
    "                                    #print(' '.join(snippet))\n",
    "\n",
    "                                    # add usage info to buffers\n",
    "                                    batch_input_ids.append(input_ids)\n",
    "                                    batch_tokens.append(i2w[token])\n",
    "                                    batch_pos.append(pos_in_context)\n",
    "                                    batch_snippets.append(snippet)\n",
    "                                    batch_decades.append(decade)\n",
    "\n",
    "#                                 print(\"batch size \", len(batch_input_ids))\n",
    "#                                 print(\"lines left \", len(lines) - L)\n",
    "#                                 print(\"files left in this decade\" , len(this_decade_files) - F)\n",
    "#                                 print(\"decades left\", len(decades) - T)\n",
    "                                    \n",
    "                                # if the buffers are full...             or if we're at the end of the dataset\n",
    "                                if (len(batch_input_ids) >= buffer_size) or (L == len(lines) - 1 and T == len(decades) - 1 and F==len(this_decade_files)):\n",
    "                                    \n",
    "                                    usage_vectors = get_usage_vectors(\n",
    "                                        model,\n",
    "                                        batch_input_ids,\n",
    "                                        batch_tokens,\n",
    "                                        batch_snippets, \n",
    "                                        batch_pos, \n",
    "                                        batch_decades )\n",
    "                                    \n",
    "#                                     with torch.no_grad():\n",
    "#                                         # collect list of input ids into a single batch tensor\n",
    "#                                         input_ids_tensor = torch.tensor(batch_input_ids)\n",
    "#                                         if torch.cuda.is_available():\n",
    "#                                             input_ids_tensor = input_ids_tensor.to('cuda')\n",
    "\n",
    "#                                         # run usages through language model\n",
    "#                                         outputs = model(input_ids_tensor,  output_hidden_states=True )\n",
    "#                                         #print(len(outputs.hidden_states)) # items in the tuple = 1 + num layers\n",
    "#                                         if torch.cuda.is_available():\n",
    "#                                             hidden_states = [l.detach().cpu().clone().numpy() for l in outputs[2]]\n",
    "#                                         else:\n",
    "#                                             #print(\"fjekl\")\n",
    "#                                             hidden_states = [l.clone().numpy() for l in outputs.hidden_states]\n",
    "\n",
    "#                                         # get usage vectors from hidden states\n",
    "#                                         hidden_states = np.stack(hidden_states)  # (13, B, |s|, 768)\n",
    "#                                         print('Expected hidden states size: (13, B, |s|, 768). Got {}'.format(hidden_states.shape))\n",
    "#                                         # usage_vectors = np.sum(hidden_states, 0)  # (B, |s|, 768)\n",
    "#                                         # usage_vectors = hidden_states.view(hidden_states.shape[1],\n",
    "#                                         #                                    hidden_states.shape[2],\n",
    "#                                         #                                    -1)\n",
    "#                                         usage_vectors = np.sum(hidden_states[1:, :, :, :], axis=0)\n",
    "#                                         # usage_vectors = hidden_states.reshape((hidden_states.shape[1], hidden_states.shape[2], -1))\n",
    "#                                         #print(\"makes usage vectors\")\n",
    "#                                         print(usage_vectors.shape)\n",
    "\n",
    "\n",
    "\n",
    "                                    # store usage tuples in a dictionary: lemma -> (vector, snippet, position, decade)\n",
    "                                    #print(len(batch_input_ids))\n",
    "                                    for b in np.arange(len(batch_input_ids)):\n",
    "                                        usage_vector = usage_vectors[b, batch_pos[b]+1, :] # get the right position\n",
    "                                        usages[batch_tokens[b]].append(\n",
    "                                             (usage_vector, batch_snippets[b], batch_pos[b], batch_decades[b]))\n",
    "                                    \n",
    "#                                     print(usages)\n",
    "                        \n",
    "                                    # finally, empty the batch buffers\n",
    "                                    batch_input_ids, batch_tokens, batch_pos, batch_snippets, batch_decades = [], [], [], [], []\n",
    "                        \n",
    "        print(\"saving usages for decade\")\n",
    "        if os.path.exists(output_path):\n",
    "            append_write = 'ab' # append if already exists\n",
    "        else:\n",
    "            append_write = 'wb' # make a new file if not\n",
    "\n",
    "        # and store data incrementally\n",
    "        if output_path:\n",
    "            #print(append_write)\n",
    "            with open(output_path, append_write) as f:\n",
    "                pickle.dump(usages, file=f)\n",
    "    \n",
    "\n",
    "    return usages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "815298a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "# Target words: we want to collect tokens of each of these words from COHA\n",
    "\n",
    "targets = ['net', 'virtual', 'disk', 'card', 'optical', 'virus',\n",
    "           'signal', 'mirror', 'energy', 'compact', 'leaf',\n",
    "           'brick', 'federal', 'sphere', 'coach', 'spine', 'parent', 'sleep']\n",
    "\n",
    "decades = [decade for decade in np.arange(1910, 2009, 10)]\n",
    "print(len(decades))\n",
    "\n",
    "buffer_size=1024\n",
    "sequence_length=128\n",
    "\n",
    "\n",
    "coha_dir = '/home/shared/corpora/Corpus of Historical American English/TEXTS'\n",
    "bert_dir = 'bert-base-uncased'\n",
    "output_dir = '../data/cwr4lsc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0889b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Decade 1910...\n",
      "/home/shared/corpora/Corpus of Historical American English/TEXTS\n",
      "1910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                  | 0/3355 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (7482 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3355/3355 [05:33<00:00, 10.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving usages for decade\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Decade 1920...\n",
      "/home/shared/corpora/Corpus of Historical American English/TEXTS\n",
      "1920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                 | 0/11557 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (5408 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11557/11557 [06:31<00:00, 29.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving usages for decade\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Decade 1930...\n",
      "/home/shared/corpora/Corpus of Historical American English/TEXTS\n",
      "1930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                 | 0/10352 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (516 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10352/10352 [06:43<00:00, 25.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving usages for decade\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Decade 1940...\n",
      "/home/shared/corpora/Corpus of Historical American English/TEXTS\n",
      "1940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                 | 0/11343 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (768 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11343/11343 [06:27<00:00, 29.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving usages for decade\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Decade 1950...\n",
      "/home/shared/corpora/Corpus of Historical American English/TEXTS\n",
      "1950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                 | 0/11935 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (603 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11935/11935 [06:46<00:00, 29.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving usages for decade\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Decade 1960...\n",
      "/home/shared/corpora/Corpus of Historical American English/TEXTS\n",
      "1960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                 | 0/10113 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (693 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10113/10113 [06:33<00:00, 25.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving usages for decade\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Decade 1970...\n",
      "/home/shared/corpora/Corpus of Historical American English/TEXTS\n",
      "1970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                  | 0/9419 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1233 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9419/9419 [06:54<00:00, 22.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving usages for decade\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Decade 1980...\n",
      "/home/shared/corpora/Corpus of Historical American English/TEXTS\n",
      "1980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                 | 0/11106 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (4048 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11106/11106 [07:21<00:00, 25.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving usages for decade\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Decade 1990...\n",
      "/home/shared/corpora/Corpus of Historical American English/TEXTS\n",
      "1990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                  | 0/9778 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (634 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9778/9778 [08:30<00:00, 19.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving usages for decade\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Decade 2000...\n",
      "/home/shared/corpora/Corpus of Historical American English/TEXTS\n",
      "2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                 | 0/13795 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (642 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13795/13795 [09:10<00:00, 25.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving usages for decade\n"
     ]
    }
   ],
   "source": [
    "### collect just the usages and not the vectors. \n",
    "\n",
    "for decade in decades:\n",
    "    collect_from_coha(targets,\n",
    "                      [decade],\n",
    "                      sequence_length=sequence_length,\n",
    "                      pretrained_weights=bert_dir,\n",
    "                      coha_dir=coha_dir,\n",
    "                      output_path='{}/usages_with_vectors_16_len{}_{}.dict'.format(output_dir, sequence_length, decade),\n",
    "                      buffer_size=buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82d37886",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "\n",
    "l.append([1,2,3])\n",
    "l.append([4,5,6])\n",
    "l\n",
    "\n",
    "with open('file', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d959de78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# import argparse\n",
    "# import numpy as np\n",
    "# from usage_collector import collect_from_coha\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--seqlen', type=int, default=128)\n",
    "# parser.add_argument('--bertdir', type=str, default='models/bert-base-uncased')\n",
    "# parser.add_argument('--cohadir', type=str, default='data/coha')\n",
    "# parser.add_argument('--outdir', type=str, default='data')\n",
    "# parser.add_argument('--buffer', type=int, default=1024)\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# targets = ['net', 'virtual', 'disk', 'card', 'optical', 'virus',\n",
    "#            'signal', 'mirror', 'energy', 'compact', 'leaf',\n",
    "#            'brick', 'federal', 'sphere', 'coach', 'spine', 'parent', 'sleep']\n",
    "\n",
    "# print('{}\\nSEQUENCE LENGTH: {}\\n{}'.format('-' * 30, args.seqlen, '-' * 30))\n",
    "\n",
    "# # decades = list(np.arange(1910, 2001, 10))\n",
    "# # decades = list(np.arange(1810, 1811, 10))\n",
    "\n",
    "# for decade in np.arange(1910, 2009, 10):\n",
    "#     collect_from_coha(targets,\n",
    "#                       [decade],\n",
    "#                       sequence_length=args.seqlen,\n",
    "#                       pretrained_weights=args.bertdir,\n",
    "#                       coha_dir=args.cohadir,\n",
    "#                       output_path='{}/concat/usages_16_len{}_{}.dict'.format(args.outdir, args.seqlen, decade),\n",
    "#                       buffer_size=args.buffer)\n",
    "\n",
    "#     # # Save usages\n",
    "#     # with open('{}/concat/usages_16_len{}_{}.dict'.format(args.outdir, args.seqlen, decade), 'wb') as f:\n",
    "#     #     pickle.dump(usages, file=f)\n",
    "#     # usages = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d167699",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/gabriellachronis/Box Sync/src/cwr4lsc/test_output', 'rb') as file:\n",
    "\n",
    "    # dump information to that file\n",
    "    data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31073d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gabriella_cwr4lsc",
   "language": "python",
   "name": "gabriella_cwr4lsc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
