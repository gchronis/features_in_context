{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data\n",
    "\n",
    "(we're doing this elsewhere now)\n",
    "\n",
    "First we will create a dictionary of Semcor words, and look at them and their frequencies.\n",
    "\n",
    "\n",
    "Next, we want to create a dataset of a subsample of semcor. We want to remove the most common and least common words\n",
    "\n",
    "\n",
    "We limit this set in several ways:\n",
    "    - only noun senses\n",
    "    - max 30 examples of each sense of a word.\n",
    "    - concrete\n",
    "    - remove nominalizations, which tend to have eventive readings (we are interested in nouns denoting entities)\n",
    "\n",
    "So, we begin iterating through a randomly shuffled semcor. For each word, we throw it out if it does not fit our criteria. Then, we look at the senses.\n",
    "\n",
    "\n",
    "\n",
    "At the end, we store a list of all of the words we've collected. For each item in the dictionary, we should know:\n",
    "- the number of tokens\n",
    "- the wordnet senses\n",
    "- a list of the semcor sentence indices of the tokens of each word. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from nltk.corpus import semcor\n",
    "#from nltk.tree import Tree\n",
    "#import itertools\n",
    "#import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "from bert import *\n",
    "import csv\n",
    "from nltk.corpus.reader.wordnet import Lemma\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import inflect\n",
    "import os\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "import re\n",
    "from src.utils import *\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.modeling:loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /Users/gabriellachronis/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "INFO:pytorch_pretrained_bert.modeling:extracting archive file /Users/gabriellachronis/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /var/folders/9m/vzvx58rs51v_x5nm620fz4xr0000gn/T/tmpybc6znrm\n",
      "INFO:pytorch_pretrained_bert.modeling:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/gabriellachronis/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "bert = BERTBase()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict feature vectors and save pairwise token data;\n",
    "# Run correlation analysis;\n",
    "# Saturate with other information;\n",
    "\n",
    "\n",
    "    \n",
    "All the old code that used to be above this is moot and improved. The eval data are already generated. We simply need to load in this file of eval data and work from there. \n",
    "\n",
    "Because you already have the data from the\n",
    "    \"Collect semcor eval data.ipynb\" script\n",
    "in the right format in the form of \n",
    "    [lemma, sense, word_form, context]\n",
    "in the file\n",
    "    \"data/processed/semcor_eval_data_11_27_2021.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "models and save paths\n",
    "\"\"\"\n",
    "\n",
    "models = [\n",
    "    # commented models have already been run\n",
    "    ## buchanan\n",
    "    #'../trained_models/model.plsr.buchanan.allbuthomonyms.5k.300components.500max_iters',\n",
    "    #'../trained_models/model.plsr.buchanan.allbuthomonyms.1k.300components.500max_iters',\n",
    "    #'../trained_models/model.ffnn.buchanan.allbuthomonyms.5k.50epochs.0.5dropout.lr1e-4.hsize300',\n",
    "    #'../trained_models/model.ffnn.buchanan.allbuthomonyms.1k.50epochs.0.5dropout.lr1e-4.hsize300',\n",
    "    '../trained_models/model.modabs.buchanan.5k.mu1_1.mu2_0.1.mu3_0.001.mu4_5.nnk_4',\n",
    "    '../trained_models/model.modabs.buchanan.1k.mu1_1.mu2_0.1.mu3_1e-07.mu4_10.nnk_4',\n",
    "\n",
    "\n",
    "    ### mcrae\n",
    "    #'../trained_models/model.plsr.mc_rae_real.allbuthomonyms.5k.100components.500max_iters',\n",
    "    #'../trained_models/model.plsr.mc_rae_real.allbuthomonyms.1k.50components.500max_iters',\n",
    "    #'../trained_models/model.ffnn.mc_rae_real.allbuthomonyms.5k.50epochs.0.5dropout.lr1e-4.hsize300',\n",
    "    #'../trained_models/model.ffnn.mc_rae_real.allbuthomonyms.1k.50epochs.0.5dropout.lr1e-4.hsize300',\n",
    "    '../trained_models/model.modabs.mc_rae_real.5k.mu1_1.mu2_0.1.mu3_0.001.mu4_5.nnk_4',\n",
    "    '../trained_models/model.modabs.mc_rae_real.1k.mu1_1.mu2_0.1.mu3_1e-07.mu4_10.nnk_4',\n",
    "    \n",
    "    #### binder\n",
    "    #'../trained_models/model.ffnn.binder.5k.50epochs.0.5dropout.lr1e-4.hsize300',\n",
    "    #'../trained_models/model.ffnn.binder.1k.50epochs.0.5dropout.lr1e-4.hsize300',\n",
    "    #'../trained_models/model.plsr.binder.5k.30components.500max_iters',\n",
    "    #'../trained_models/model.plsr.binder.1k.30components.500max_iters',\n",
    "    '../trained_models/model.modabs.binder.5k.mu1_1.mu2_0.1.mu3_0.001.mu4_5.nnk_4', # modabs binder 5k missing\n",
    "    '../trained_models/model.modabs.binder.1k.mu1_1.mu2_0.1.mu3_0.001.mu4_5.nnk_4' # modabs binder 1k missing\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    torch.load(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now, we have our dataset that we want to analyze. We just need to do:\n",
    "\n",
    "for each model we want to evaluate, run the following script:\n",
    "\n",
    "open the file of data\n",
    "\n",
    "read it in as a dataframe\n",
    "\n",
    "for each of the unique words in that dataset\n",
    "\n",
    "    we calculate pairwise distances between each otoken and every otehr token\n",
    "    and construct a similarities dataset. \n",
    "    \n",
    "    add some extra infor about sense and polysemy bins\n",
    "    \n",
    "    and store into a file\n",
    "    \n",
    "    \n",
    "    (no longer do this-then we run correlations for that word-???)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def make_predictions(df, model, bert):\n",
    "    \"\"\"\n",
    "    df has columns\n",
    "    [lemma, sense, word_form, context]\n",
    "    \n",
    "    for a single semcor lemma\n",
    "    \"\"\"    \n",
    "    predictions = []\n",
    "    for index, row in df.iterrows():\n",
    "\n",
    "        #print(row.word_form)\n",
    "        #print(row.context)\n",
    "\n",
    "        predicted_vector = model.predict_in_context(row.word_form, row.context, bert)\n",
    "\n",
    "        predictions.append(predicted_vector)\n",
    "    return predictions\n",
    "\n",
    "def read_concreteness_values():\n",
    "    \"\"\"\n",
    "    returns df with columns\n",
    "    word\n",
    "    Conc.M\n",
    "    \"\"\"\n",
    "    brysbaert_filename = \"/Users/gabriellachronis/data/Concreteness_ratings_Brysbaert_et_al_BRM.csv\"\n",
    "    concreteness_df = pd.read_csv(brysbaert_filename, sep='\\t')\n",
    "    concreteness_df= concreteness_df[[\"Word\", \"Conc.M\"]]\n",
    "    concreteness_df = concreteness_df.set_index(\"Word\")\n",
    "    return concreteness_df\n",
    "\n",
    "def get_pairwise_wu_palmer_data(model, df, bert):\n",
    "    \n",
    "    \"\"\"\n",
    "    df has columns\n",
    "    [lemma, sense, word_form, context]\n",
    "    \"\"\"\n",
    "    \n",
    "    # add concreteness\n",
    "    concreteness_df = read_concreteness_values()\n",
    "    df = df.join(concreteness_df, how = \"left\", on = \"lemma\")\n",
    "\n",
    "    \n",
    "    # storage\n",
    "    run_stats = []\n",
    "    vals = []\n",
    "    item = 0\n",
    "\n",
    "    # run through lemma by lemma\n",
    "    unique_words = df.lemma.unique()\n",
    "    for l in range(0, len(unique_words)):\n",
    "        if l % 1000 == 0:\n",
    "            print(\"processed %s lemmas\" % l)\n",
    "        \n",
    "        # get a dataframe containing all the tokens of this word\n",
    "        word = unique_words[l]\n",
    "        word_data = df[df.lemma == word].copy()\n",
    "        \n",
    "        # lemma level info\n",
    "        n_senses = len(word_data['sense'].unique())\n",
    "        n_word_forms = len(word_data['word_form'].unique())\n",
    "        \n",
    "        # token level predictions\n",
    "        predictions = make_predictions(word_data, model, bert)\n",
    "        word_data['prediction'] = predictions\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        then we calculate the pairwise distances between all of the vectors, only counting one pair one time\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # pop the first token off the list\n",
    "        num_toks = len(word_data)\n",
    "        for i in range(0,num_toks):\n",
    "            item +=1\n",
    "            if item % 1000 == 0:\n",
    "                print(\"processed %s of %s tokens\" % (item,num_toks))            \n",
    "            # compare it with each of the other tokens\n",
    "            # dont have to compare to any earlier\n",
    "            for j in range(i+1,num_toks):\n",
    "\n",
    "                #print(df.iloc[i])\n",
    "                #print(df.iloc[j])\n",
    "\n",
    "                # calculate cosine similarity between the two vectors\n",
    "                cos_sim = 1 - cosine(word_data.iloc[i].prediction, word_data.iloc[j].prediction)\n",
    "\n",
    "                # and wu palmer similarity between the two wn lemmas\n",
    "                lemma1 = lemma_from_string(word_data.iloc[i].sense)\n",
    "                lemma1_str = lemma_name_from_string(word_data.iloc[i].sense)\n",
    "                lemma2 = lemma_from_string(word_data.iloc[j].sense)\n",
    "                lemma2_str = lemma_name_from_string(word_data.iloc[j].sense)\n",
    "                synset1 = lemma1.synset()\n",
    "                synset2 = lemma2.synset()\n",
    "                wup_sim = synset1.wup_similarity(synset2)\n",
    "\n",
    "                # get other token level data\n",
    "                pos1 = re.findall(r\"\\.(.*?)\\.\", lemma1_str)[0]\n",
    "                pos2 = re.findall(r\"\\.(.*?)\\.\", lemma2_str)[0]\n",
    "                concreteness = word_data.iloc[i][\"Conc.M\"]\n",
    "                \n",
    "                # if we can't compute a distance for these senses / recognize them, discard\n",
    "                if type(wup_sim) == float:\n",
    "                    # store this data point into a list\n",
    "                    vals.append((word, lemma1_str, lemma2_str, pos1, pos2, cos_sim, wup_sim, n_senses, n_word_forms, concreteness))\n",
    "        \n",
    "    # turn results into dataframe\n",
    "    columns = [\n",
    "        \"lemma\",\n",
    "        \"token_sense_1\",\n",
    "        \"token_sense_2\",\n",
    "        \"sense1_pos\",\n",
    "        \"sense2_pos\",\n",
    "        \"cos_sim\",\n",
    "        \"wup_sim\", \n",
    "        \"n_senses\",\n",
    "        \"n_word_forms\",\n",
    "        \"concreteness\"]\n",
    "    token_similarities = pd.DataFrame.from_records(vals, columns = columns)\n",
    "        \n",
    "    \n",
    "    # add in bins\n",
    "    token_similarities['wn_bin'] = pd.cut(token_similarities.n_senses, \n",
    "                        bins = [0, 2.1, 4.1, 6.1, 8.1, 10.1, 20.1, 50.1, 200], labels = False)\n",
    "    token_similarities['conc_bin'] = pd.cut(token_similarities.concreteness, \n",
    "                        bins = [0, 1.5, 2.5, 3.5, 4.5, 10], labels = False)\n",
    "   \n",
    "    return token_similarities\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "def plot_sims():\n",
    "    cos_sims = sense_similarities['cos_sim']\n",
    "    wup_sims = sense_similarities['wup_sim']\n",
    "    plt.scatter(wup_sims, cos_sims)\n",
    "    plt.title(\"Wordnet similarity of homonymous senses plotted against cosine similarity of predicted vectors of two tokens in semantic feature space\")\n",
    "    plt.xlabel(\"Wu and Palmer Similarity\")\n",
    "    plt.ylabel(\"Cosine Similarity\")\n",
    "    plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************\n",
      "*** Evaluating ../trained_models/model.modabs.buchanan.5k.mu1_1.mu2_0.1.mu3_0.001.mu4_5.nnk_4 model ***\n",
      "****************************************\n",
      "processed 0 lemmas\n",
      "processed 1000 of 118 tokens\n",
      "processed 2000 of 47 tokens\n",
      "processed 3000 of 85 tokens\n",
      "processed 4000 of 86 tokens\n",
      "processed 5000 of 77 tokens\n",
      "processed 6000 of 69 tokens\n",
      "processed 7000 of 24 tokens\n",
      "processed 8000 of 34 tokens\n",
      "processed 9000 of 17 tokens\n",
      "processed 10000 of 58 tokens\n",
      "processed 11000 of 31 tokens\n",
      "processed 12000 of 65 tokens\n",
      "processed 13000 of 14 tokens\n",
      "processed 14000 of 58 tokens\n",
      "processed 15000 of 37 tokens\n",
      "processed 16000 of 29 tokens\n",
      "processed 17000 of 20 tokens\n",
      "processed 18000 of 28 tokens\n",
      "processed 19000 of 51 tokens\n",
      "processed 20000 of 24 tokens\n",
      "processed 21000 of 56 tokens\n",
      "processed 22000 of 18 tokens\n",
      "processed 23000 of 97 tokens\n",
      "processed 24000 of 28 tokens\n",
      "processed 1000 lemmas\n",
      "processed 25000 of 8 tokens\n",
      "processed 26000 of 34 tokens\n",
      "processed 27000 of 4 tokens\n",
      "processed 28000 of 21 tokens\n",
      "processed 29000 of 105 tokens\n",
      "processed 30000 of 86 tokens\n",
      "processed 31000 of 16 tokens\n",
      "processed 32000 of 17 tokens\n",
      "processed 33000 of 46 tokens\n",
      "processed 34000 of 31 tokens\n",
      "processed 35000 of 27 tokens\n",
      "processed 36000 of 49 tokens\n",
      "processed 37000 of 58 tokens\n",
      "processed 38000 of 18 tokens\n",
      "processed 2000 lemmas\n",
      "processed 39000 of 25 tokens\n",
      "processed 40000 of 25 tokens\n",
      "processed 41000 of 6 tokens\n",
      "processed 42000 of 44 tokens\n",
      "processed 43000 of 8 tokens\n",
      "processed 44000 of 3 tokens\n",
      "processed 45000 of 7 tokens\n",
      "processed 46000 of 9 tokens\n",
      "processed 47000 of 2 tokens\n",
      "processed 3000 lemmas\n",
      "processed 48000 of 4 tokens\n",
      "processed 49000 of 13 tokens\n",
      "processed 50000 of 15 tokens\n",
      "processed 51000 of 4 tokens\n",
      "processed 52000 of 10 tokens\n",
      "processed 4000 lemmas\n",
      "processed 53000 of 4 tokens\n",
      "processed 54000 of 8 tokens\n",
      "processed 55000 of 3 tokens\n",
      "processed 56000 of 2 tokens\n",
      "processed 5000 lemmas\n",
      "processed 57000 of 15 tokens\n",
      "processed 58000 of 2 tokens\n",
      "processed 59000 of 6 tokens\n",
      "processed 60000 of 4 tokens\n",
      "processed 6000 lemmas\n",
      "processed 61000 of 3 tokens\n",
      "processed 62000 of 2 tokens\n",
      "processed 63000 of 4 tokens\n",
      "processed 7000 lemmas\n",
      "processed 64000 of 1 tokens\n",
      "processed 65000 of 1 tokens\n",
      "processed 8000 lemmas\n",
      "processed 66000 of 6 tokens\n",
      "processed 67000 of 2 tokens\n",
      "processed 9000 lemmas\n",
      "processed 68000 of 2 tokens\n",
      "processed 10000 lemmas\n",
      "processed 69000 of 1 tokens\n",
      "processed 70000 of 2 tokens\n",
      "processed 11000 lemmas\n",
      "processed 71000 of 2 tokens\n",
      "****************************************\n",
      "*** Evaluating ../trained_models/model.modabs.buchanan.1k.mu1_1.mu2_0.1.mu3_1e-07.mu4_10.nnk_4 model ***\n",
      "****************************************\n",
      "processed 0 lemmas\n",
      "processed 1000 of 118 tokens\n",
      "processed 2000 of 47 tokens\n",
      "processed 3000 of 85 tokens\n",
      "processed 4000 of 86 tokens\n",
      "processed 5000 of 77 tokens\n",
      "processed 6000 of 69 tokens\n",
      "processed 7000 of 24 tokens\n",
      "processed 8000 of 34 tokens\n",
      "processed 9000 of 17 tokens\n",
      "processed 10000 of 58 tokens\n",
      "processed 11000 of 31 tokens\n",
      "processed 12000 of 65 tokens\n",
      "processed 13000 of 14 tokens\n",
      "processed 14000 of 58 tokens\n",
      "processed 15000 of 37 tokens\n",
      "processed 16000 of 29 tokens\n",
      "processed 17000 of 20 tokens\n",
      "processed 18000 of 28 tokens\n",
      "processed 19000 of 51 tokens\n",
      "processed 20000 of 24 tokens\n",
      "processed 21000 of 56 tokens\n",
      "processed 22000 of 18 tokens\n",
      "processed 23000 of 97 tokens\n",
      "processed 24000 of 28 tokens\n",
      "processed 1000 lemmas\n",
      "processed 25000 of 8 tokens\n",
      "processed 26000 of 34 tokens\n",
      "processed 27000 of 4 tokens\n",
      "processed 28000 of 21 tokens\n",
      "processed 29000 of 105 tokens\n",
      "processed 30000 of 86 tokens\n",
      "processed 31000 of 16 tokens\n",
      "processed 32000 of 17 tokens\n",
      "processed 33000 of 46 tokens\n",
      "processed 34000 of 31 tokens\n",
      "processed 35000 of 27 tokens\n",
      "processed 36000 of 49 tokens\n",
      "processed 37000 of 58 tokens\n",
      "processed 38000 of 18 tokens\n",
      "processed 2000 lemmas\n",
      "processed 39000 of 25 tokens\n",
      "processed 40000 of 25 tokens\n",
      "processed 41000 of 6 tokens\n",
      "processed 42000 of 44 tokens\n",
      "processed 43000 of 8 tokens\n",
      "processed 44000 of 3 tokens\n",
      "processed 45000 of 7 tokens\n",
      "processed 46000 of 9 tokens\n",
      "processed 47000 of 2 tokens\n",
      "processed 3000 lemmas\n",
      "processed 48000 of 4 tokens\n",
      "processed 49000 of 13 tokens\n",
      "processed 50000 of 15 tokens\n",
      "processed 51000 of 4 tokens\n",
      "processed 52000 of 10 tokens\n",
      "processed 4000 lemmas\n",
      "processed 53000 of 4 tokens\n",
      "processed 54000 of 8 tokens\n",
      "processed 55000 of 3 tokens\n",
      "processed 56000 of 2 tokens\n",
      "processed 5000 lemmas\n",
      "processed 57000 of 15 tokens\n",
      "processed 58000 of 2 tokens\n",
      "processed 59000 of 6 tokens\n",
      "processed 60000 of 4 tokens\n",
      "processed 6000 lemmas\n",
      "processed 61000 of 3 tokens\n",
      "processed 62000 of 2 tokens\n",
      "processed 63000 of 4 tokens\n",
      "processed 7000 lemmas\n",
      "processed 64000 of 1 tokens\n",
      "processed 65000 of 1 tokens\n",
      "processed 8000 lemmas\n",
      "processed 66000 of 6 tokens\n",
      "processed 67000 of 2 tokens\n",
      "processed 9000 lemmas\n",
      "processed 68000 of 2 tokens\n",
      "processed 10000 lemmas\n",
      "processed 69000 of 1 tokens\n",
      "processed 70000 of 2 tokens\n",
      "processed 11000 lemmas\n",
      "processed 71000 of 2 tokens\n",
      "****************************************\n",
      "*** Evaluating ../trained_models/model.modabs.mc_rae_real.5k.mu1_1.mu2_0.1.mu3_0.001.mu4_5.nnk_4 model ***\n",
      "****************************************\n",
      "processed 0 lemmas\n",
      "processed 1000 of 118 tokens\n",
      "processed 2000 of 47 tokens\n",
      "processed 3000 of 85 tokens\n",
      "processed 4000 of 86 tokens\n",
      "processed 5000 of 77 tokens\n",
      "processed 6000 of 69 tokens\n",
      "processed 7000 of 24 tokens\n",
      "processed 8000 of 34 tokens\n",
      "processed 9000 of 17 tokens\n",
      "processed 10000 of 58 tokens\n",
      "processed 11000 of 31 tokens\n",
      "processed 12000 of 65 tokens\n",
      "processed 13000 of 14 tokens\n",
      "processed 14000 of 58 tokens\n",
      "processed 15000 of 37 tokens\n",
      "processed 16000 of 29 tokens\n",
      "processed 17000 of 20 tokens\n",
      "processed 18000 of 28 tokens\n",
      "processed 19000 of 51 tokens\n",
      "processed 20000 of 24 tokens\n",
      "processed 21000 of 56 tokens\n",
      "processed 22000 of 18 tokens\n",
      "processed 23000 of 97 tokens\n",
      "processed 24000 of 28 tokens\n",
      "processed 1000 lemmas\n",
      "processed 25000 of 8 tokens\n",
      "processed 26000 of 34 tokens\n",
      "processed 27000 of 4 tokens\n",
      "processed 28000 of 21 tokens\n",
      "processed 29000 of 105 tokens\n",
      "processed 30000 of 86 tokens\n",
      "processed 31000 of 16 tokens\n",
      "processed 32000 of 17 tokens\n",
      "processed 33000 of 46 tokens\n",
      "processed 34000 of 31 tokens\n",
      "processed 35000 of 27 tokens\n",
      "processed 36000 of 49 tokens\n",
      "processed 37000 of 58 tokens\n",
      "processed 38000 of 18 tokens\n",
      "processed 2000 lemmas\n",
      "processed 39000 of 25 tokens\n",
      "processed 40000 of 25 tokens\n",
      "processed 41000 of 6 tokens\n",
      "processed 42000 of 44 tokens\n",
      "processed 43000 of 8 tokens\n",
      "processed 44000 of 3 tokens\n",
      "processed 45000 of 7 tokens\n",
      "processed 46000 of 9 tokens\n",
      "processed 47000 of 2 tokens\n",
      "processed 3000 lemmas\n",
      "processed 48000 of 4 tokens\n",
      "processed 49000 of 13 tokens\n",
      "processed 50000 of 15 tokens\n",
      "processed 51000 of 4 tokens\n",
      "processed 52000 of 10 tokens\n",
      "processed 4000 lemmas\n",
      "processed 53000 of 4 tokens\n",
      "processed 54000 of 8 tokens\n",
      "processed 55000 of 3 tokens\n",
      "processed 56000 of 2 tokens\n",
      "processed 5000 lemmas\n",
      "processed 57000 of 15 tokens\n",
      "processed 58000 of 2 tokens\n",
      "processed 59000 of 6 tokens\n",
      "processed 60000 of 4 tokens\n",
      "processed 6000 lemmas\n",
      "processed 61000 of 3 tokens\n",
      "processed 62000 of 2 tokens\n",
      "processed 63000 of 4 tokens\n",
      "processed 7000 lemmas\n",
      "processed 64000 of 1 tokens\n",
      "processed 65000 of 1 tokens\n",
      "processed 8000 lemmas\n",
      "processed 66000 of 6 tokens\n",
      "processed 67000 of 2 tokens\n",
      "processed 9000 lemmas\n",
      "processed 68000 of 2 tokens\n",
      "processed 10000 lemmas\n",
      "processed 69000 of 1 tokens\n",
      "processed 70000 of 2 tokens\n",
      "processed 11000 lemmas\n",
      "processed 71000 of 2 tokens\n",
      "****************************************\n",
      "*** Evaluating ../trained_models/model.modabs.mc_rae_real.1k.mu1_1.mu2_0.1.mu3_1e-07.mu4_10.nnk_4 model ***\n",
      "****************************************\n",
      "processed 0 lemmas\n",
      "processed 1000 of 118 tokens\n",
      "processed 2000 of 47 tokens\n",
      "processed 3000 of 85 tokens\n",
      "processed 4000 of 86 tokens\n",
      "processed 5000 of 77 tokens\n",
      "processed 6000 of 69 tokens\n",
      "processed 7000 of 24 tokens\n",
      "processed 8000 of 34 tokens\n",
      "processed 9000 of 17 tokens\n",
      "processed 10000 of 58 tokens\n",
      "processed 11000 of 31 tokens\n",
      "processed 12000 of 65 tokens\n",
      "processed 13000 of 14 tokens\n",
      "processed 14000 of 58 tokens\n",
      "processed 15000 of 37 tokens\n",
      "processed 16000 of 29 tokens\n",
      "processed 17000 of 20 tokens\n",
      "processed 18000 of 28 tokens\n",
      "processed 19000 of 51 tokens\n",
      "processed 20000 of 24 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 21000 of 56 tokens\n",
      "processed 22000 of 18 tokens\n",
      "processed 23000 of 97 tokens\n",
      "processed 24000 of 28 tokens\n",
      "processed 1000 lemmas\n",
      "processed 25000 of 8 tokens\n",
      "processed 26000 of 34 tokens\n",
      "processed 27000 of 4 tokens\n",
      "processed 28000 of 21 tokens\n",
      "processed 29000 of 105 tokens\n",
      "processed 30000 of 86 tokens\n",
      "processed 31000 of 16 tokens\n",
      "processed 32000 of 17 tokens\n",
      "processed 33000 of 46 tokens\n",
      "processed 34000 of 31 tokens\n",
      "processed 35000 of 27 tokens\n",
      "processed 36000 of 49 tokens\n",
      "processed 37000 of 58 tokens\n",
      "processed 38000 of 18 tokens\n",
      "processed 2000 lemmas\n",
      "processed 39000 of 25 tokens\n",
      "processed 40000 of 25 tokens\n",
      "processed 41000 of 6 tokens\n",
      "processed 42000 of 44 tokens\n",
      "processed 43000 of 8 tokens\n",
      "processed 44000 of 3 tokens\n",
      "processed 45000 of 7 tokens\n",
      "processed 46000 of 9 tokens\n",
      "processed 47000 of 2 tokens\n",
      "processed 3000 lemmas\n",
      "processed 48000 of 4 tokens\n",
      "processed 49000 of 13 tokens\n",
      "processed 50000 of 15 tokens\n",
      "processed 51000 of 4 tokens\n",
      "processed 52000 of 10 tokens\n",
      "processed 4000 lemmas\n",
      "processed 53000 of 4 tokens\n",
      "processed 54000 of 8 tokens\n",
      "processed 55000 of 3 tokens\n",
      "processed 56000 of 2 tokens\n",
      "processed 5000 lemmas\n",
      "processed 57000 of 15 tokens\n",
      "processed 58000 of 2 tokens\n",
      "processed 59000 of 6 tokens\n",
      "processed 60000 of 4 tokens\n",
      "processed 6000 lemmas\n",
      "processed 61000 of 3 tokens\n",
      "processed 62000 of 2 tokens\n",
      "processed 63000 of 4 tokens\n",
      "processed 7000 lemmas\n",
      "processed 64000 of 1 tokens\n",
      "processed 65000 of 1 tokens\n",
      "processed 8000 lemmas\n",
      "processed 66000 of 6 tokens\n",
      "processed 67000 of 2 tokens\n",
      "processed 9000 lemmas\n",
      "processed 68000 of 2 tokens\n",
      "processed 10000 lemmas\n",
      "processed 69000 of 1 tokens\n",
      "processed 70000 of 2 tokens\n",
      "processed 11000 lemmas\n",
      "processed 71000 of 2 tokens\n",
      "****************************************\n",
      "*** Evaluating ../trained_models/model.modabs.binder.5k.mu1_1.mu2_0.1.mu3_0.001.mu4_5.nnk_4 model ***\n",
      "****************************************\n",
      "processed 0 lemmas\n",
      "processed 1000 of 118 tokens\n",
      "processed 2000 of 47 tokens\n",
      "processed 3000 of 85 tokens\n",
      "processed 4000 of 86 tokens\n",
      "processed 5000 of 77 tokens\n",
      "processed 6000 of 69 tokens\n",
      "processed 7000 of 24 tokens\n",
      "processed 8000 of 34 tokens\n",
      "processed 9000 of 17 tokens\n",
      "processed 10000 of 58 tokens\n",
      "processed 11000 of 31 tokens\n",
      "processed 12000 of 65 tokens\n",
      "processed 13000 of 14 tokens\n",
      "processed 14000 of 58 tokens\n",
      "processed 15000 of 37 tokens\n",
      "processed 16000 of 29 tokens\n",
      "processed 17000 of 20 tokens\n",
      "processed 18000 of 28 tokens\n",
      "processed 19000 of 51 tokens\n",
      "processed 20000 of 24 tokens\n",
      "processed 21000 of 56 tokens\n",
      "processed 22000 of 18 tokens\n",
      "processed 23000 of 97 tokens\n",
      "processed 24000 of 28 tokens\n",
      "processed 1000 lemmas\n",
      "processed 25000 of 8 tokens\n",
      "processed 26000 of 34 tokens\n",
      "processed 27000 of 4 tokens\n",
      "processed 28000 of 21 tokens\n",
      "processed 29000 of 105 tokens\n",
      "processed 30000 of 86 tokens\n",
      "processed 31000 of 16 tokens\n",
      "processed 32000 of 17 tokens\n",
      "processed 33000 of 46 tokens\n",
      "processed 34000 of 31 tokens\n",
      "processed 35000 of 27 tokens\n",
      "processed 36000 of 49 tokens\n",
      "processed 37000 of 58 tokens\n",
      "processed 38000 of 18 tokens\n",
      "processed 2000 lemmas\n",
      "processed 39000 of 25 tokens\n",
      "processed 40000 of 25 tokens\n",
      "processed 41000 of 6 tokens\n",
      "processed 42000 of 44 tokens\n",
      "processed 43000 of 8 tokens\n",
      "processed 44000 of 3 tokens\n",
      "processed 45000 of 7 tokens\n",
      "processed 46000 of 9 tokens\n",
      "processed 47000 of 2 tokens\n",
      "processed 3000 lemmas\n",
      "processed 48000 of 4 tokens\n",
      "processed 49000 of 13 tokens\n",
      "processed 50000 of 15 tokens\n",
      "processed 51000 of 4 tokens\n",
      "processed 52000 of 10 tokens\n",
      "processed 4000 lemmas\n",
      "processed 53000 of 4 tokens\n",
      "processed 54000 of 8 tokens\n",
      "processed 55000 of 3 tokens\n",
      "processed 56000 of 2 tokens\n",
      "processed 5000 lemmas\n",
      "processed 57000 of 15 tokens\n",
      "processed 58000 of 2 tokens\n",
      "processed 59000 of 6 tokens\n",
      "processed 60000 of 4 tokens\n",
      "processed 6000 lemmas\n",
      "processed 61000 of 3 tokens\n",
      "processed 62000 of 2 tokens\n",
      "processed 63000 of 4 tokens\n",
      "processed 7000 lemmas\n",
      "processed 64000 of 1 tokens\n",
      "processed 65000 of 1 tokens\n",
      "processed 8000 lemmas\n",
      "processed 66000 of 6 tokens\n",
      "processed 67000 of 2 tokens\n",
      "processed 9000 lemmas\n",
      "processed 68000 of 2 tokens\n",
      "processed 10000 lemmas\n",
      "processed 69000 of 1 tokens\n",
      "processed 70000 of 2 tokens\n",
      "processed 11000 lemmas\n",
      "processed 71000 of 2 tokens\n",
      "****************************************\n",
      "*** Evaluating ../trained_models/model.modabs.binder.1k.mu1_1.mu2_0.1.mu3_0.001.mu4_5.nnk_4 model ***\n",
      "****************************************\n",
      "processed 0 lemmas\n",
      "processed 1000 of 118 tokens\n",
      "processed 2000 of 47 tokens\n",
      "processed 3000 of 85 tokens\n",
      "processed 4000 of 86 tokens\n",
      "processed 5000 of 77 tokens\n",
      "processed 6000 of 69 tokens\n",
      "processed 7000 of 24 tokens\n",
      "processed 8000 of 34 tokens\n",
      "processed 9000 of 17 tokens\n",
      "processed 10000 of 58 tokens\n",
      "processed 11000 of 31 tokens\n",
      "processed 12000 of 65 tokens\n",
      "processed 13000 of 14 tokens\n",
      "processed 14000 of 58 tokens\n",
      "processed 15000 of 37 tokens\n",
      "processed 16000 of 29 tokens\n",
      "processed 17000 of 20 tokens\n",
      "processed 18000 of 28 tokens\n",
      "processed 19000 of 51 tokens\n",
      "processed 20000 of 24 tokens\n",
      "processed 21000 of 56 tokens\n",
      "processed 22000 of 18 tokens\n",
      "processed 23000 of 97 tokens\n",
      "processed 24000 of 28 tokens\n",
      "processed 1000 lemmas\n",
      "processed 25000 of 8 tokens\n",
      "processed 26000 of 34 tokens\n",
      "processed 27000 of 4 tokens\n",
      "processed 28000 of 21 tokens\n",
      "processed 29000 of 105 tokens\n",
      "processed 30000 of 86 tokens\n",
      "processed 31000 of 16 tokens\n",
      "processed 32000 of 17 tokens\n",
      "processed 33000 of 46 tokens\n",
      "processed 34000 of 31 tokens\n",
      "processed 35000 of 27 tokens\n",
      "processed 36000 of 49 tokens\n",
      "processed 37000 of 58 tokens\n",
      "processed 38000 of 18 tokens\n",
      "processed 2000 lemmas\n",
      "processed 39000 of 25 tokens\n",
      "processed 40000 of 25 tokens\n",
      "processed 41000 of 6 tokens\n",
      "processed 42000 of 44 tokens\n",
      "processed 43000 of 8 tokens\n",
      "processed 44000 of 3 tokens\n",
      "processed 45000 of 7 tokens\n",
      "processed 46000 of 9 tokens\n",
      "processed 47000 of 2 tokens\n",
      "processed 3000 lemmas\n",
      "processed 48000 of 4 tokens\n",
      "processed 49000 of 13 tokens\n",
      "processed 50000 of 15 tokens\n",
      "processed 51000 of 4 tokens\n",
      "processed 52000 of 10 tokens\n",
      "processed 4000 lemmas\n",
      "processed 53000 of 4 tokens\n",
      "processed 54000 of 8 tokens\n",
      "processed 55000 of 3 tokens\n",
      "processed 56000 of 2 tokens\n",
      "processed 5000 lemmas\n",
      "processed 57000 of 15 tokens\n",
      "processed 58000 of 2 tokens\n",
      "processed 59000 of 6 tokens\n",
      "processed 60000 of 4 tokens\n",
      "processed 6000 lemmas\n",
      "processed 61000 of 3 tokens\n",
      "processed 62000 of 2 tokens\n",
      "processed 63000 of 4 tokens\n",
      "processed 7000 lemmas\n",
      "processed 64000 of 1 tokens\n",
      "processed 65000 of 1 tokens\n",
      "processed 8000 lemmas\n",
      "processed 66000 of 6 tokens\n",
      "processed 67000 of 2 tokens\n",
      "processed 9000 lemmas\n",
      "processed 68000 of 2 tokens\n",
      "processed 10000 lemmas\n",
      "processed 69000 of 1 tokens\n",
      "processed 70000 of 2 tokens\n",
      "processed 11000 lemmas\n",
      "processed 71000 of 2 tokens\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# we need the default na option turned off bc the word/lemma 'null' was being interpreted as 'NaN'\n",
    "names = [\"lemma\", \"sense\", \"word_form\", \"context\"]\n",
    "df = pd.read_csv('../data/processed/semcor_eval_data_11_27_2021.csv', names = names , keep_default_na=False)\n",
    "\n",
    "for save_path in models:\n",
    "    print(\"****************************************\")\n",
    "    print(\"*** Evaluating %s model ***\" % save_path)\n",
    "    print(\"****************************************\")\n",
    "    model = torch.load(save_path)\n",
    "    out_path = '../results/semcor_pairwise_data_' + os.path.split(save_path)[1] + '.csv'    \n",
    "\n",
    "    # remove results file if exists\n",
    "    if os.path.exists(out_path):\n",
    "        os.remove(out_path)\n",
    "    pairwise_data = get_pairwise_wu_palmer_data(model, df, bert)\n",
    "    pairwise_data.to_csv(out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Conc.M</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>roadsweeper</th>\n",
       "      <td>4.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>traindriver</th>\n",
       "      <td>4.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tush</th>\n",
       "      <td>4.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hairdress</th>\n",
       "      <td>3.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pharmaceutics</th>\n",
       "      <td>3.77</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Conc.M\n",
       "Word                 \n",
       "roadsweeper      4.85\n",
       "traindriver      4.54\n",
       "tush             4.45\n",
       "hairdress        3.93\n",
       "pharmaceutics    3.77"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brysbaert_filename = \"/Users/gabriellachronis/data/Concreteness_ratings_Brysbaert_et_al_BRM.csv\"\n",
    "concreteness_df = pd.read_csv(brysbaert_filename, sep='\\t')\n",
    "concreteness_df= concreteness_df[[\"Word\", \"Conc.M\"]]\n",
    "concreteness_df = concreteness_df.set_index(\"Word\")\n",
    "concreteness_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a little test\n",
    "# lemma_from_string(csv_input.iloc[0].token_sense_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now we need to go in and add abstractness value and \n",
    "bin number of senses into polysemy band\n",
    "\n",
    "(already done since we fixed above)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# #cols = [\"lemma\", \"token_sense_1\", \"token_sense_2\", \"cos_sim\", \"wup_sim\", \"n_senses\"]\n",
    "\n",
    "\n",
    "# for save_path in models:\n",
    "#     print(\"****************************************\")\n",
    "#     print(\"*** Saturating pairwise data for model: %s ***\" % save_path)\n",
    "#     print(\"****************************************\")\n",
    "#     infile = '../results/semcor_pairwise_data_' + os.path.split(save_path)[1] + '.csv'\n",
    "\n",
    "#     outfile = '../results/saturated_semcor_pairwise_data_' + os.path.split(save_path)[1] + '.csv'\n",
    "\n",
    "    \n",
    "#     dtype = {\n",
    "#         'lemma':             str,\n",
    "#         'token_sense_1':     object,\n",
    "#         'token_sense_2':     object,\n",
    "#         'cos_sim':           float,\n",
    "#         'wup_sim':           float,\n",
    "#         'n_senses':          float   \n",
    "#     }\n",
    "#     csv_input = pd.read_csv(infile, encoding_errors='ignore') #, names=cols) # requires pandas 1.3\n",
    "#     #csv_input = pd.read_csv(infile)\n",
    "    \n",
    "#     print(len(csv_input))\n",
    "    \n",
    "#      # filtering out the rows with `POSITION_T` value in corresponding column\n",
    "#     csv_input = csv_input[csv_input.token_sense_1.str.contains('token_sense_1') == False]\n",
    "    \n",
    "#     print(len(csv_input))\n",
    "# #     print(csv_input.dtypes)\n",
    "\n",
    "    \n",
    "#     csv_input = csv_input.convert_dtypes(convert_floating=True)\n",
    "    \n",
    "# #     print(csv_input.dtypes)\n",
    "    \n",
    "#     print(csv_input.head())\n",
    "\n",
    "    \n",
    "#     # add polysemy bin\n",
    "#     csv_input['wn_bin'] = pd.cut(csv_input.n_senses, \n",
    "#                         bins = [0, 2.1, 4.1, 6.1, 8.1, 10.1, 20.1, 50.1, 200], labels = False)\n",
    "\n",
    "\n",
    "#     # add POS rows\n",
    "#     pos1s = []\n",
    "#     pos2s = []\n",
    "#     sense1s = []\n",
    "#     sense2s = []\n",
    "#     for index, row in csv_input.iterrows():\n",
    "#         pos1 = re.findall(r\"\\.(.*?)\\.\", row.token_sense_1)[0]\n",
    "#         pos2 = re.findall(r\"\\.(.*?)\\.\", row.token_sense_2)[0]\n",
    "#         pos1s.append(pos1)\n",
    "#         pos2s.append(pos2)\n",
    "#         sense1 = lemma_name_from_string(row.token_sense_1)\n",
    "#         sense2 = lemma_name_from_string(row.token_sense_2)\n",
    "#         sense1s.append(sense1)\n",
    "#         sense2s.append(sense2)\n",
    "        \n",
    "#     csv_input['sense1_pos'] = pos1s\n",
    "#     csv_input['sense2_pos'] = pos2s\n",
    "#     csv_input['token_sense_1'] = sense_1s\n",
    "#     csv_input['token_sense_2'] = sense_2s\n",
    "    \n",
    "    \n",
    "#     # add concreteness\n",
    "#     csv_input = csv_input.join(concreteness_df, how = \"left\", on = \"lemma\")\n",
    "    \n",
    "#     csv_input['conc_bin'] = pd.cut(csv_input['Conc.M'], \n",
    "#                         bins = [0, 2.3, 4.5, 10], labels = False)\n",
    "    \n",
    "#     # remove token sense columns\n",
    "# #     csv_input.drop(['token_sense_1'], axis=1)\n",
    "# #     csv_input.drop(['token_sense_2'], axis=1)\n",
    "    \n",
    "#     #print(csv_input.where(csv_input['Conc.M'].notnull()))\n",
    "#     print(csv_input.head(20))\n",
    "    \n",
    "#     raise Exception(\"dewfieow\")\n",
    "    \n",
    "#     csv_input.to_csv(outfile, index=False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "and now we make another dataset for each one with the correlations.\n",
    "first we define some functions\n",
    "\"\"\"\n",
    "\n",
    "def run_correlation(sense_similarities):\n",
    "    \"\"\"\n",
    "    input\n",
    "    :sense_similarities: dataframe with columns\n",
    "        cosine_sims \n",
    "        wup_sims\n",
    "    output\n",
    "    4-tuple with\n",
    "        pearson\n",
    "        pearson_p\n",
    "        spearman\n",
    "        spearman_p\n",
    "    \"\"\"\n",
    "    #print(sense_similarities.head())\n",
    "    \n",
    "#     if sense_similarities is None:\n",
    "#         # not really sure why we're getting none values here it should be impossible\n",
    "#         return (float(\"nan\"), float(\"nan\"), float(\"nan\"), float(\"nan\"))\n",
    "#     elif len(sense_similarities['wup_sim'].unique()) == 1:\n",
    "\n",
    "    # if we only have 1 sense represented in semcor\n",
    "    # the correlation will be garbage with a constant y value; skip to avoid warnings\n",
    "    if sense_similarities.iloc[0]['n_senses'] == 1:\n",
    "        return (float(\"nan\"), float(\"nan\"), float(\"nan\"), float(\"nan\"))\n",
    "\n",
    "    if len(sense_similarities) > 1 :\n",
    "        #print(word)\n",
    "\n",
    "        cos_sims = sense_similarities['cos_sim']\n",
    "        wup_sims = sense_similarities['wup_sim']\n",
    "\n",
    "        pearson, pearson_p = pearsonr(cos_sims, wup_sims )\n",
    "        #print('Pearsons correlation: %.3f, p-value: %s'  % (pearson, pearson_p))\n",
    "\n",
    "        spearman, spearman_p = spearmanr(cos_sims, wup_sims )\n",
    "        #print('Spearmans correlation: %.3f, p-value: %s'  % (spearman, spearman_p))\n",
    "\n",
    "        return (pearson, pearson_p, spearman, spearman_p)\n",
    "    \n",
    "    # if we don't have enough, return dummies\n",
    "    print(\"not enough examples of lemma: \", sense_similarities.iloc[0].lemma)\n",
    "    return (float(\"nan\"), float(\"nan\"), float(\"nan\"), float(\"nan\"))\n",
    "\n",
    "def plot_sims():\n",
    "    cos_sims = sense_similarities['cos_sim']\n",
    "    wup_sims = sense_similarities['wup_sim']\n",
    "    plt.scatter(wup_sims, cos_sims)\n",
    "    plt.title(\"Wordnet similarity of homonymous senses plotted against cosine similarity of predicted vectors of two tokens in semantic feature space\")\n",
    "    plt.xlabel(\"Wu and Palmer Similarity\")\n",
    "    plt.ylabel(\"Cosine Similarity\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#debug_df = pd.read_csv('../results/debug_semcor_pairwise_data_model.plsr.buchanan.allbuthomoyms.5k.300components.500max_iters.csv')\n",
    "#debug_df[debug_df.lemma=='tan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma</th>\n",
       "      <th>sense</th>\n",
       "      <th>wordform</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53086</th>\n",
       "      <td>herb</td>\n",
       "      <td>Lemma('herb.n.01.herb')</td>\n",
       "      <td>herbs</td>\n",
       "      <td>Gazing at her husband 's drugged body , his ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53087</th>\n",
       "      <td>herb</td>\n",
       "      <td>Lemma('herb.n.02.herb')</td>\n",
       "      <td>herbs</td>\n",
       "      <td>She stood still over the leg of lamb , rubbing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      lemma                    sense wordform  \\\n",
       "53086  herb  Lemma('herb.n.01.herb')    herbs   \n",
       "53087  herb  Lemma('herb.n.02.herb')    herbs   \n",
       "\n",
       "                                                 context  \n",
       "53086  Gazing at her husband 's drugged body , his ch...  \n",
       "53087  She stood still over the leg of lamb , rubbing...  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#debug_df = pd.read_csv('../data/semcor_eval_data_11_27_2021.csv', names = [\"lemma\", \"sense\", \"wordform\", \"context\"])\n",
    "#debug_df[debug_df.lemma=='tan']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "okay so the problem we are running into is that sometimes theres just one value per lemma, when we have two senses and only one token per sense in the dataset. we can fix this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************\n",
      "*** Running correlation on  ../trained_models/model.modabs.buchanan.5k.mu1_1.mu2_0.1.mu3_0.001.mu4_5.nnk_4 model ***\n",
      "****************************************\n",
      "reading from  ../results/semcor_pairwise_data_model.modabs.buchanan.5k.mu1_1.mu2_0.1.mu3_0.001.mu4_5.nnk_4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/scipy/stats/stats.py:4023: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n",
      "/usr/local/lib/python3.9/site-packages/scipy/stats/stats.py:4484: SpearmanRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(SpearmanRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not enough examples of lemma:  tan\n",
      "not enough examples of lemma:  velour\n",
      "not enough examples of lemma:  hillbilly\n",
      "not enough examples of lemma:  scouring\n",
      "not enough examples of lemma:  inferiority\n",
      "not enough examples of lemma:  shibboleth\n",
      "not enough examples of lemma:  traditionalism\n",
      "not enough examples of lemma:  bawh\n",
      "not enough examples of lemma:  carelessness\n",
      "not enough examples of lemma:  scraping\n",
      "not enough examples of lemma:  emanation\n",
      "not enough examples of lemma:  wattle\n",
      "not enough examples of lemma:  privy\n",
      "not enough examples of lemma:  penance\n",
      "not enough examples of lemma:  reverse\n",
      "not enough examples of lemma:  surrender\n",
      "not enough examples of lemma:  mask\n",
      "not enough examples of lemma:  pulse\n",
      "not enough examples of lemma:  noun\n",
      "not enough examples of lemma:  disregard\n",
      "not enough examples of lemma:  subscription\n",
      "not enough examples of lemma:  brotherhood\n",
      "not enough examples of lemma:  bankruptcy\n",
      "not enough examples of lemma:  booth\n",
      "not enough examples of lemma:  small\n",
      "not enough examples of lemma:  provincialism\n",
      "not enough examples of lemma:  pinpoint\n",
      "not enough examples of lemma:  inflection\n",
      "not enough examples of lemma:  believer\n",
      "not enough examples of lemma:  contributor\n",
      "not enough examples of lemma:  instability\n",
      "not enough examples of lemma:  rhyme\n",
      "not enough examples of lemma:  confederacy\n",
      "not enough examples of lemma:  zeal\n",
      "not enough examples of lemma:  florist\n",
      "not enough examples of lemma:  streak\n",
      "not enough examples of lemma:  landslide\n",
      "not enough examples of lemma:  lane\n",
      "not enough examples of lemma:  humanism\n",
      "not enough examples of lemma:  hulk\n",
      "not enough examples of lemma:  accessory\n",
      "not enough examples of lemma:  firmness\n",
      "not enough examples of lemma:  honour\n",
      "not enough examples of lemma:  italian\n",
      "not enough examples of lemma:  japanese\n",
      "not enough examples of lemma:  slant\n",
      "not enough examples of lemma:  urging\n",
      "not enough examples of lemma:  helper\n",
      "not enough examples of lemma:  merriment\n",
      "not enough examples of lemma:  heaven\n",
      "not enough examples of lemma:  anachronism\n",
      "not enough examples of lemma:  expansiveness\n",
      "not enough examples of lemma:  pine\n",
      "not enough examples of lemma:  violet\n",
      "not enough examples of lemma:  dictate\n",
      "not enough examples of lemma:  helm\n",
      "not enough examples of lemma:  refinement\n",
      "not enough examples of lemma:  sail\n",
      "not enough examples of lemma:  captivity\n",
      "not enough examples of lemma:  reliance\n",
      "not enough examples of lemma:  cultivation\n",
      "not enough examples of lemma:  random\n",
      "not enough examples of lemma:  binomial\n",
      "not enough examples of lemma:  dash\n",
      "not enough examples of lemma:  primate\n",
      "not enough examples of lemma:  roundhouse\n",
      "not enough examples of lemma:  renewal\n",
      "not enough examples of lemma:  dislocation\n",
      "not enough examples of lemma:  herb\n",
      "not enough examples of lemma:  incarnation\n",
      "not enough examples of lemma:  loophole\n",
      "not enough examples of lemma:  jab\n",
      "not enough examples of lemma:  assist\n",
      "not enough examples of lemma:  flare\n",
      "not enough examples of lemma:  trademark\n",
      "not enough examples of lemma:  drafting\n",
      "not enough examples of lemma:  burn\n",
      "not enough examples of lemma:  layout\n",
      "not enough examples of lemma:  nod\n",
      "not enough examples of lemma:  pore\n",
      "not enough examples of lemma:  dereliction\n",
      "not enough examples of lemma:  backbone\n",
      "not enough examples of lemma:  orgy\n",
      "not enough examples of lemma:  mentality\n",
      "not enough examples of lemma:  down\n",
      "not enough examples of lemma:  marking\n",
      "not enough examples of lemma:  dissent\n",
      "not enough examples of lemma:  allocation\n",
      "not enough examples of lemma:  hound\n",
      "not enough examples of lemma:  detachment\n",
      "not enough examples of lemma:  mouthpiece\n",
      "not enough examples of lemma:  infliction\n",
      "not enough examples of lemma:  allegation\n",
      "not enough examples of lemma:  falsity\n",
      "not enough examples of lemma:  lad\n",
      "not enough examples of lemma:  hysteria\n",
      "not enough examples of lemma:  circus\n",
      "not enough examples of lemma:  syndicate\n",
      "not enough examples of lemma:  gage\n",
      "not enough examples of lemma:  hardship\n",
      "not enough examples of lemma:  bound\n",
      "not enough examples of lemma:  alligator\n",
      "not enough examples of lemma:  tangle\n",
      "not enough examples of lemma:  hoop\n",
      "not enough examples of lemma:  southeast\n",
      "not enough examples of lemma:  keynote\n",
      "not enough examples of lemma:  preoccupation\n",
      "not enough examples of lemma:  signature\n",
      "not enough examples of lemma:  premise\n",
      "not enough examples of lemma:  duplication\n",
      "not enough examples of lemma:  knob\n",
      "not enough examples of lemma:  chute\n",
      "not enough examples of lemma:  brilliance\n",
      "not enough examples of lemma:  frost\n",
      "not enough examples of lemma:  subtlety\n",
      "not enough examples of lemma:  rigger\n",
      "not enough examples of lemma:  showing\n",
      "not enough examples of lemma:  upset\n",
      "not enough examples of lemma:  percussion\n",
      "not enough examples of lemma:  genus\n",
      "not enough examples of lemma:  ringing\n",
      "not enough examples of lemma:  miniature\n",
      "not enough examples of lemma:  viewer\n",
      "not enough examples of lemma:  setup\n",
      "not enough examples of lemma:  rut\n",
      "not enough examples of lemma:  outrage\n",
      "not enough examples of lemma:  rearing\n",
      "not enough examples of lemma:  squatting\n",
      "not enough examples of lemma:  sash\n",
      "not enough examples of lemma:  marketplace\n",
      "not enough examples of lemma:  fancy\n",
      "not enough examples of lemma:  prophecy\n",
      "not enough examples of lemma:  configuration\n",
      "not enough examples of lemma:  congestion\n",
      "not enough examples of lemma:  brevity\n",
      "not enough examples of lemma:  conveyor\n",
      "not enough examples of lemma:  liaison\n",
      "not enough examples of lemma:  reunion\n",
      "not enough examples of lemma:  pearl\n",
      "not enough examples of lemma:  sage\n",
      "not enough examples of lemma:  cadre\n",
      "not enough examples of lemma:  profundity\n",
      "not enough examples of lemma:  handicap\n",
      "not enough examples of lemma:  coupon\n",
      "not enough examples of lemma:  rod\n",
      "not enough examples of lemma:  ransom\n",
      "not enough examples of lemma:  flush\n",
      "not enough examples of lemma:  bass\n",
      "not enough examples of lemma:  open\n",
      "not enough examples of lemma:  coordination\n",
      "not enough examples of lemma:  peculiarity\n",
      "not enough examples of lemma:  video\n",
      "not enough examples of lemma:  pickup\n",
      "not enough examples of lemma:  stubbornness\n",
      "not enough examples of lemma:  trim\n",
      "not enough examples of lemma:  illumination\n",
      "not enough examples of lemma:  shit\n",
      "not enough examples of lemma:  abode\n",
      "not enough examples of lemma:  accompaniment\n",
      "not enough examples of lemma:  jelly\n",
      "not enough examples of lemma:  feast\n",
      "not enough examples of lemma:  rot\n",
      "not enough examples of lemma:  thaw\n",
      "not enough examples of lemma:  humming\n",
      "not enough examples of lemma:  ministry\n",
      "not enough examples of lemma:  contemplation\n",
      "not enough examples of lemma:  shrubbery\n",
      "not enough examples of lemma:  pretence\n",
      "not enough examples of lemma:  pharmacy\n",
      "not enough examples of lemma:  calendar\n",
      "not enough examples of lemma:  antecedent\n",
      "not enough examples of lemma:  landmark\n",
      "not enough examples of lemma:  gassing\n",
      "not enough examples of lemma:  knot\n",
      "not enough examples of lemma:  crudity\n",
      "not enough examples of lemma:  leave\n",
      "not enough examples of lemma:  hurt\n",
      "not enough examples of lemma:  murmur\n",
      "not enough examples of lemma:  oscillation\n",
      "not enough examples of lemma:  sardine\n",
      "not enough examples of lemma:  sanitation\n",
      "not enough examples of lemma:  monster\n",
      "not enough examples of lemma:  birmingham\n",
      "not enough examples of lemma:  humanity\n",
      "not enough examples of lemma:  creed\n",
      "not enough examples of lemma:  pioneer\n",
      "not enough examples of lemma:  groove\n",
      "not enough examples of lemma:  overture\n",
      "not enough examples of lemma:  lust\n",
      "not enough examples of lemma:  salvo\n",
      "not enough examples of lemma:  loft\n",
      "not enough examples of lemma:  confinement\n",
      "not enough examples of lemma:  index\n",
      "not enough examples of lemma:  malady\n",
      "not enough examples of lemma:  attachment\n",
      "not enough examples of lemma:  self-will\n",
      "not enough examples of lemma:  corduroy\n",
      "not enough examples of lemma:  diplomacy\n",
      "not enough examples of lemma:  arch\n",
      "not enough examples of lemma:  instrumentality\n",
      "not enough examples of lemma:  fullness\n",
      "not enough examples of lemma:  nurse\n",
      "not enough examples of lemma:  cartwheel\n",
      "not enough examples of lemma:  founder\n",
      "not enough examples of lemma:  misconstruction\n",
      "not enough examples of lemma:  flag\n",
      "not enough examples of lemma:  currency\n",
      "not enough examples of lemma:  deposit\n",
      "not enough examples of lemma:  dealing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not enough examples of lemma:  northeast\n",
      "not enough examples of lemma:  hollyhock\n",
      "not enough examples of lemma:  brooding\n",
      "not enough examples of lemma:  aggressor\n",
      "not enough examples of lemma:  tumbler\n",
      "not enough examples of lemma:  portrayal\n",
      "not enough examples of lemma:  inspiration\n",
      "not enough examples of lemma:  pavement\n",
      "not enough examples of lemma:  hypocrisy\n",
      "not enough examples of lemma:  chorus\n",
      "not enough examples of lemma:  toll\n",
      "not enough examples of lemma:  claw\n",
      "not enough examples of lemma:  palette\n",
      "not enough examples of lemma:  hint\n",
      "not enough examples of lemma:  consolidation\n",
      "not enough examples of lemma:  coverage\n",
      "not enough examples of lemma:  attainment\n",
      "not enough examples of lemma:  southwest\n",
      "not enough examples of lemma:  bathing\n",
      "not enough examples of lemma:  assyrian\n",
      "not enough examples of lemma:  nymph\n",
      "not enough examples of lemma:  equality\n",
      "not enough examples of lemma:  exterior\n",
      "not enough examples of lemma:  symbolism\n",
      "not enough examples of lemma:  fermentation\n",
      "not enough examples of lemma:  porter\n",
      "not enough examples of lemma:  chord\n",
      "not enough examples of lemma:  mold\n",
      "not enough examples of lemma:  clown\n",
      "not enough examples of lemma:  keeping\n",
      "not enough examples of lemma:  redundancy\n",
      "not enough examples of lemma:  descendant\n",
      "not enough examples of lemma:  tidewater\n",
      "not enough examples of lemma:  forerunner\n",
      "not enough examples of lemma:  villain\n",
      "not enough examples of lemma:  lease\n",
      "not enough examples of lemma:  grievance\n",
      "not enough examples of lemma:  concurrence\n",
      "not enough examples of lemma:  gum\n",
      "not enough examples of lemma:  emergence\n",
      "not enough examples of lemma:  stimulant\n",
      "not enough examples of lemma:  trio\n",
      "not enough examples of lemma:  blackout\n",
      "not enough examples of lemma:  propagation\n",
      "not enough examples of lemma:  inconvenience\n",
      "not enough examples of lemma:  nobility\n",
      "not enough examples of lemma:  wait\n",
      "not enough examples of lemma:  sitting\n",
      "not enough examples of lemma:  expanse\n",
      "not enough examples of lemma:  tiger\n",
      "not enough examples of lemma:  buzz\n",
      "not enough examples of lemma:  spacing\n",
      "writing to  ../results/semcor_analysis_model.modabs.buchanan.5k.mu1_1.mu2_0.1.mu3_0.001.mu4_5.nnk_4.csv\n",
      "****************************************\n",
      "*** Running correlation on  ../trained_models/model.modabs.buchanan.1k.mu1_1.mu2_0.1.mu3_1e-07.mu4_10.nnk_4 model ***\n",
      "****************************************\n",
      "reading from  ../results/semcor_pairwise_data_model.modabs.buchanan.1k.mu1_1.mu2_0.1.mu3_1e-07.mu4_10.nnk_4.csv\n",
      "not enough examples of lemma:  tan\n",
      "not enough examples of lemma:  velour\n",
      "not enough examples of lemma:  hillbilly\n",
      "not enough examples of lemma:  scouring\n",
      "not enough examples of lemma:  inferiority\n",
      "not enough examples of lemma:  shibboleth\n",
      "not enough examples of lemma:  traditionalism\n",
      "not enough examples of lemma:  bawh\n",
      "not enough examples of lemma:  carelessness\n",
      "not enough examples of lemma:  scraping\n",
      "not enough examples of lemma:  emanation\n",
      "not enough examples of lemma:  wattle\n",
      "not enough examples of lemma:  privy\n",
      "not enough examples of lemma:  penance\n",
      "not enough examples of lemma:  reverse\n",
      "not enough examples of lemma:  surrender\n",
      "not enough examples of lemma:  mask\n",
      "not enough examples of lemma:  pulse\n",
      "not enough examples of lemma:  noun\n",
      "not enough examples of lemma:  disregard\n",
      "not enough examples of lemma:  subscription\n",
      "not enough examples of lemma:  brotherhood\n",
      "not enough examples of lemma:  bankruptcy\n",
      "not enough examples of lemma:  booth\n",
      "not enough examples of lemma:  small\n",
      "not enough examples of lemma:  provincialism\n",
      "not enough examples of lemma:  pinpoint\n",
      "not enough examples of lemma:  inflection\n",
      "not enough examples of lemma:  believer\n",
      "not enough examples of lemma:  contributor\n",
      "not enough examples of lemma:  instability\n",
      "not enough examples of lemma:  rhyme\n",
      "not enough examples of lemma:  confederacy\n",
      "not enough examples of lemma:  zeal\n",
      "not enough examples of lemma:  florist\n",
      "not enough examples of lemma:  streak\n",
      "not enough examples of lemma:  landslide\n",
      "not enough examples of lemma:  lane\n",
      "not enough examples of lemma:  humanism\n",
      "not enough examples of lemma:  hulk\n",
      "not enough examples of lemma:  accessory\n",
      "not enough examples of lemma:  firmness\n",
      "not enough examples of lemma:  honour\n",
      "not enough examples of lemma:  italian\n",
      "not enough examples of lemma:  japanese\n",
      "not enough examples of lemma:  slant\n",
      "not enough examples of lemma:  urging\n",
      "not enough examples of lemma:  helper\n",
      "not enough examples of lemma:  merriment\n",
      "not enough examples of lemma:  heaven\n",
      "not enough examples of lemma:  anachronism\n",
      "not enough examples of lemma:  expansiveness\n",
      "not enough examples of lemma:  pine\n",
      "not enough examples of lemma:  violet\n",
      "not enough examples of lemma:  dictate\n",
      "not enough examples of lemma:  helm\n",
      "not enough examples of lemma:  refinement\n",
      "not enough examples of lemma:  sail\n",
      "not enough examples of lemma:  captivity\n",
      "not enough examples of lemma:  reliance\n",
      "not enough examples of lemma:  cultivation\n",
      "not enough examples of lemma:  random\n",
      "not enough examples of lemma:  binomial\n",
      "not enough examples of lemma:  dash\n",
      "not enough examples of lemma:  primate\n",
      "not enough examples of lemma:  roundhouse\n",
      "not enough examples of lemma:  renewal\n",
      "not enough examples of lemma:  dislocation\n",
      "not enough examples of lemma:  herb\n",
      "not enough examples of lemma:  incarnation\n",
      "not enough examples of lemma:  loophole\n",
      "not enough examples of lemma:  jab\n",
      "not enough examples of lemma:  assist\n",
      "not enough examples of lemma:  flare\n",
      "not enough examples of lemma:  trademark\n",
      "not enough examples of lemma:  drafting\n",
      "not enough examples of lemma:  burn\n",
      "not enough examples of lemma:  layout\n",
      "not enough examples of lemma:  nod\n",
      "not enough examples of lemma:  pore\n",
      "not enough examples of lemma:  dereliction\n",
      "not enough examples of lemma:  backbone\n",
      "not enough examples of lemma:  orgy\n",
      "not enough examples of lemma:  mentality\n",
      "not enough examples of lemma:  down\n",
      "not enough examples of lemma:  marking\n",
      "not enough examples of lemma:  dissent\n",
      "not enough examples of lemma:  allocation\n",
      "not enough examples of lemma:  hound\n",
      "not enough examples of lemma:  detachment\n",
      "not enough examples of lemma:  mouthpiece\n",
      "not enough examples of lemma:  infliction\n",
      "not enough examples of lemma:  allegation\n",
      "not enough examples of lemma:  falsity\n",
      "not enough examples of lemma:  lad\n",
      "not enough examples of lemma:  hysteria\n",
      "not enough examples of lemma:  circus\n",
      "not enough examples of lemma:  syndicate\n",
      "not enough examples of lemma:  gage\n",
      "not enough examples of lemma:  hardship\n",
      "not enough examples of lemma:  bound\n",
      "not enough examples of lemma:  alligator\n",
      "not enough examples of lemma:  tangle\n",
      "not enough examples of lemma:  hoop\n",
      "not enough examples of lemma:  southeast\n",
      "not enough examples of lemma:  keynote\n",
      "not enough examples of lemma:  preoccupation\n",
      "not enough examples of lemma:  signature\n",
      "not enough examples of lemma:  premise\n",
      "not enough examples of lemma:  duplication\n",
      "not enough examples of lemma:  knob\n",
      "not enough examples of lemma:  chute\n",
      "not enough examples of lemma:  brilliance\n",
      "not enough examples of lemma:  frost\n",
      "not enough examples of lemma:  subtlety\n",
      "not enough examples of lemma:  rigger\n",
      "not enough examples of lemma:  showing\n",
      "not enough examples of lemma:  upset\n",
      "not enough examples of lemma:  percussion\n",
      "not enough examples of lemma:  genus\n",
      "not enough examples of lemma:  ringing\n",
      "not enough examples of lemma:  miniature\n",
      "not enough examples of lemma:  viewer\n",
      "not enough examples of lemma:  setup\n",
      "not enough examples of lemma:  rut\n",
      "not enough examples of lemma:  outrage\n",
      "not enough examples of lemma:  rearing\n",
      "not enough examples of lemma:  squatting\n",
      "not enough examples of lemma:  sash\n",
      "not enough examples of lemma:  marketplace\n",
      "not enough examples of lemma:  fancy\n",
      "not enough examples of lemma:  prophecy\n",
      "not enough examples of lemma:  configuration\n",
      "not enough examples of lemma:  congestion\n",
      "not enough examples of lemma:  brevity\n",
      "not enough examples of lemma:  conveyor\n",
      "not enough examples of lemma:  liaison\n",
      "not enough examples of lemma:  reunion\n",
      "not enough examples of lemma:  pearl\n",
      "not enough examples of lemma:  sage\n",
      "not enough examples of lemma:  cadre\n",
      "not enough examples of lemma:  profundity\n",
      "not enough examples of lemma:  handicap\n",
      "not enough examples of lemma:  coupon\n",
      "not enough examples of lemma:  rod\n",
      "not enough examples of lemma:  ransom\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not enough examples of lemma:  flush\n",
      "not enough examples of lemma:  bass\n",
      "not enough examples of lemma:  open\n",
      "not enough examples of lemma:  coordination\n",
      "not enough examples of lemma:  peculiarity\n",
      "not enough examples of lemma:  video\n",
      "not enough examples of lemma:  pickup\n",
      "not enough examples of lemma:  stubbornness\n",
      "not enough examples of lemma:  trim\n",
      "not enough examples of lemma:  illumination\n",
      "not enough examples of lemma:  shit\n",
      "not enough examples of lemma:  abode\n",
      "not enough examples of lemma:  accompaniment\n",
      "not enough examples of lemma:  jelly\n",
      "not enough examples of lemma:  feast\n",
      "not enough examples of lemma:  rot\n",
      "not enough examples of lemma:  thaw\n",
      "not enough examples of lemma:  humming\n",
      "not enough examples of lemma:  ministry\n",
      "not enough examples of lemma:  contemplation\n",
      "not enough examples of lemma:  shrubbery\n",
      "not enough examples of lemma:  pretence\n",
      "not enough examples of lemma:  pharmacy\n",
      "not enough examples of lemma:  calendar\n",
      "not enough examples of lemma:  antecedent\n",
      "not enough examples of lemma:  landmark\n",
      "not enough examples of lemma:  gassing\n",
      "not enough examples of lemma:  knot\n",
      "not enough examples of lemma:  crudity\n",
      "not enough examples of lemma:  leave\n",
      "not enough examples of lemma:  hurt\n",
      "not enough examples of lemma:  murmur\n",
      "not enough examples of lemma:  oscillation\n",
      "not enough examples of lemma:  sardine\n",
      "not enough examples of lemma:  sanitation\n",
      "not enough examples of lemma:  monster\n",
      "not enough examples of lemma:  birmingham\n",
      "not enough examples of lemma:  humanity\n",
      "not enough examples of lemma:  creed\n",
      "not enough examples of lemma:  pioneer\n",
      "not enough examples of lemma:  groove\n",
      "not enough examples of lemma:  overture\n",
      "not enough examples of lemma:  lust\n",
      "not enough examples of lemma:  salvo\n",
      "not enough examples of lemma:  loft\n",
      "not enough examples of lemma:  confinement\n",
      "not enough examples of lemma:  index\n",
      "not enough examples of lemma:  malady\n",
      "not enough examples of lemma:  attachment\n",
      "not enough examples of lemma:  self-will\n",
      "not enough examples of lemma:  corduroy\n",
      "not enough examples of lemma:  diplomacy\n",
      "not enough examples of lemma:  arch\n",
      "not enough examples of lemma:  instrumentality\n",
      "not enough examples of lemma:  fullness\n",
      "not enough examples of lemma:  nurse\n",
      "not enough examples of lemma:  cartwheel\n",
      "not enough examples of lemma:  founder\n",
      "not enough examples of lemma:  misconstruction\n",
      "not enough examples of lemma:  flag\n",
      "not enough examples of lemma:  currency\n",
      "not enough examples of lemma:  deposit\n",
      "not enough examples of lemma:  dealing\n",
      "not enough examples of lemma:  northeast\n",
      "not enough examples of lemma:  hollyhock\n",
      "not enough examples of lemma:  brooding\n",
      "not enough examples of lemma:  aggressor\n",
      "not enough examples of lemma:  tumbler\n",
      "not enough examples of lemma:  portrayal\n",
      "not enough examples of lemma:  inspiration\n",
      "not enough examples of lemma:  pavement\n",
      "not enough examples of lemma:  hypocrisy\n",
      "not enough examples of lemma:  chorus\n",
      "not enough examples of lemma:  toll\n",
      "not enough examples of lemma:  claw\n",
      "not enough examples of lemma:  palette\n",
      "not enough examples of lemma:  hint\n",
      "not enough examples of lemma:  consolidation\n",
      "not enough examples of lemma:  coverage\n",
      "not enough examples of lemma:  attainment\n",
      "not enough examples of lemma:  southwest\n",
      "not enough examples of lemma:  bathing\n",
      "not enough examples of lemma:  assyrian\n",
      "not enough examples of lemma:  nymph\n",
      "not enough examples of lemma:  equality\n",
      "not enough examples of lemma:  exterior\n",
      "not enough examples of lemma:  symbolism\n",
      "not enough examples of lemma:  fermentation\n",
      "not enough examples of lemma:  porter\n",
      "not enough examples of lemma:  chord\n",
      "not enough examples of lemma:  mold\n",
      "not enough examples of lemma:  clown\n",
      "not enough examples of lemma:  keeping\n",
      "not enough examples of lemma:  redundancy\n",
      "not enough examples of lemma:  descendant\n",
      "not enough examples of lemma:  tidewater\n",
      "not enough examples of lemma:  forerunner\n",
      "not enough examples of lemma:  villain\n",
      "not enough examples of lemma:  lease\n",
      "not enough examples of lemma:  grievance\n",
      "not enough examples of lemma:  concurrence\n",
      "not enough examples of lemma:  gum\n",
      "not enough examples of lemma:  emergence\n",
      "not enough examples of lemma:  stimulant\n",
      "not enough examples of lemma:  trio\n",
      "not enough examples of lemma:  blackout\n",
      "not enough examples of lemma:  propagation\n",
      "not enough examples of lemma:  inconvenience\n",
      "not enough examples of lemma:  nobility\n",
      "not enough examples of lemma:  wait\n",
      "not enough examples of lemma:  sitting\n",
      "not enough examples of lemma:  expanse\n",
      "not enough examples of lemma:  tiger\n",
      "not enough examples of lemma:  buzz\n",
      "not enough examples of lemma:  spacing\n",
      "writing to  ../results/semcor_analysis_model.modabs.buchanan.1k.mu1_1.mu2_0.1.mu3_1e-07.mu4_10.nnk_4.csv\n",
      "****************************************\n",
      "*** Running correlation on  ../trained_models/model.modabs.mc_rae_real.5k.mu1_1.mu2_0.1.mu3_0.001.mu4_5.nnk_4 model ***\n",
      "****************************************\n",
      "reading from  ../results/semcor_pairwise_data_model.modabs.mc_rae_real.5k.mu1_1.mu2_0.1.mu3_0.001.mu4_5.nnk_4.csv\n",
      "not enough examples of lemma:  tan\n",
      "not enough examples of lemma:  velour\n",
      "not enough examples of lemma:  hillbilly\n",
      "not enough examples of lemma:  scouring\n",
      "not enough examples of lemma:  inferiority\n",
      "not enough examples of lemma:  shibboleth\n",
      "not enough examples of lemma:  traditionalism\n",
      "not enough examples of lemma:  bawh\n",
      "not enough examples of lemma:  carelessness\n",
      "not enough examples of lemma:  scraping\n",
      "not enough examples of lemma:  emanation\n",
      "not enough examples of lemma:  wattle\n",
      "not enough examples of lemma:  privy\n",
      "not enough examples of lemma:  penance\n",
      "not enough examples of lemma:  reverse\n",
      "not enough examples of lemma:  surrender\n",
      "not enough examples of lemma:  mask\n",
      "not enough examples of lemma:  pulse\n",
      "not enough examples of lemma:  noun\n",
      "not enough examples of lemma:  disregard\n",
      "not enough examples of lemma:  subscription\n",
      "not enough examples of lemma:  brotherhood\n",
      "not enough examples of lemma:  bankruptcy\n",
      "not enough examples of lemma:  booth\n",
      "not enough examples of lemma:  small\n",
      "not enough examples of lemma:  provincialism\n",
      "not enough examples of lemma:  pinpoint\n",
      "not enough examples of lemma:  inflection\n",
      "not enough examples of lemma:  believer\n",
      "not enough examples of lemma:  contributor\n",
      "not enough examples of lemma:  instability\n",
      "not enough examples of lemma:  rhyme\n",
      "not enough examples of lemma:  confederacy\n",
      "not enough examples of lemma:  zeal\n",
      "not enough examples of lemma:  florist\n",
      "not enough examples of lemma:  streak\n",
      "not enough examples of lemma:  landslide\n",
      "not enough examples of lemma:  lane\n",
      "not enough examples of lemma:  humanism\n",
      "not enough examples of lemma:  hulk\n",
      "not enough examples of lemma:  accessory\n",
      "not enough examples of lemma:  firmness\n",
      "not enough examples of lemma:  honour\n",
      "not enough examples of lemma:  italian\n",
      "not enough examples of lemma:  japanese\n",
      "not enough examples of lemma:  slant\n",
      "not enough examples of lemma:  urging\n",
      "not enough examples of lemma:  helper\n",
      "not enough examples of lemma:  merriment\n",
      "not enough examples of lemma:  heaven\n",
      "not enough examples of lemma:  anachronism\n",
      "not enough examples of lemma:  expansiveness\n",
      "not enough examples of lemma:  pine\n",
      "not enough examples of lemma:  violet\n",
      "not enough examples of lemma:  dictate\n",
      "not enough examples of lemma:  helm\n",
      "not enough examples of lemma:  refinement\n",
      "not enough examples of lemma:  sail\n",
      "not enough examples of lemma:  captivity\n",
      "not enough examples of lemma:  reliance\n",
      "not enough examples of lemma:  cultivation\n",
      "not enough examples of lemma:  random\n",
      "not enough examples of lemma:  binomial\n",
      "not enough examples of lemma:  dash\n",
      "not enough examples of lemma:  primate\n",
      "not enough examples of lemma:  roundhouse\n",
      "not enough examples of lemma:  renewal\n",
      "not enough examples of lemma:  dislocation\n",
      "not enough examples of lemma:  herb\n",
      "not enough examples of lemma:  incarnation\n",
      "not enough examples of lemma:  loophole\n",
      "not enough examples of lemma:  jab\n",
      "not enough examples of lemma:  assist\n",
      "not enough examples of lemma:  flare\n",
      "not enough examples of lemma:  trademark\n",
      "not enough examples of lemma:  drafting\n",
      "not enough examples of lemma:  burn\n",
      "not enough examples of lemma:  layout\n",
      "not enough examples of lemma:  nod\n",
      "not enough examples of lemma:  pore\n",
      "not enough examples of lemma:  dereliction\n",
      "not enough examples of lemma:  backbone\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not enough examples of lemma:  orgy\n",
      "not enough examples of lemma:  mentality\n",
      "not enough examples of lemma:  down\n",
      "not enough examples of lemma:  marking\n",
      "not enough examples of lemma:  dissent\n",
      "not enough examples of lemma:  allocation\n",
      "not enough examples of lemma:  hound\n",
      "not enough examples of lemma:  detachment\n",
      "not enough examples of lemma:  mouthpiece\n",
      "not enough examples of lemma:  infliction\n",
      "not enough examples of lemma:  allegation\n",
      "not enough examples of lemma:  falsity\n",
      "not enough examples of lemma:  lad\n",
      "not enough examples of lemma:  hysteria\n",
      "not enough examples of lemma:  circus\n",
      "not enough examples of lemma:  syndicate\n",
      "not enough examples of lemma:  gage\n",
      "not enough examples of lemma:  hardship\n",
      "not enough examples of lemma:  bound\n",
      "not enough examples of lemma:  alligator\n",
      "not enough examples of lemma:  tangle\n",
      "not enough examples of lemma:  hoop\n",
      "not enough examples of lemma:  southeast\n",
      "not enough examples of lemma:  keynote\n",
      "not enough examples of lemma:  preoccupation\n",
      "not enough examples of lemma:  signature\n",
      "not enough examples of lemma:  premise\n",
      "not enough examples of lemma:  duplication\n",
      "not enough examples of lemma:  knob\n",
      "not enough examples of lemma:  chute\n",
      "not enough examples of lemma:  brilliance\n",
      "not enough examples of lemma:  frost\n",
      "not enough examples of lemma:  subtlety\n",
      "not enough examples of lemma:  rigger\n",
      "not enough examples of lemma:  showing\n",
      "not enough examples of lemma:  upset\n",
      "not enough examples of lemma:  percussion\n",
      "not enough examples of lemma:  genus\n",
      "not enough examples of lemma:  ringing\n",
      "not enough examples of lemma:  miniature\n",
      "not enough examples of lemma:  viewer\n",
      "not enough examples of lemma:  setup\n",
      "not enough examples of lemma:  rut\n",
      "not enough examples of lemma:  outrage\n",
      "not enough examples of lemma:  rearing\n",
      "not enough examples of lemma:  squatting\n",
      "not enough examples of lemma:  sash\n",
      "not enough examples of lemma:  marketplace\n",
      "not enough examples of lemma:  fancy\n",
      "not enough examples of lemma:  prophecy\n",
      "not enough examples of lemma:  configuration\n",
      "not enough examples of lemma:  congestion\n",
      "not enough examples of lemma:  brevity\n",
      "not enough examples of lemma:  conveyor\n",
      "not enough examples of lemma:  liaison\n",
      "not enough examples of lemma:  reunion\n",
      "not enough examples of lemma:  pearl\n",
      "not enough examples of lemma:  sage\n",
      "not enough examples of lemma:  cadre\n",
      "not enough examples of lemma:  profundity\n",
      "not enough examples of lemma:  handicap\n",
      "not enough examples of lemma:  coupon\n",
      "not enough examples of lemma:  rod\n",
      "not enough examples of lemma:  ransom\n",
      "not enough examples of lemma:  flush\n",
      "not enough examples of lemma:  bass\n",
      "not enough examples of lemma:  open\n",
      "not enough examples of lemma:  coordination\n",
      "not enough examples of lemma:  peculiarity\n",
      "not enough examples of lemma:  video\n",
      "not enough examples of lemma:  pickup\n",
      "not enough examples of lemma:  stubbornness\n",
      "not enough examples of lemma:  trim\n",
      "not enough examples of lemma:  illumination\n",
      "not enough examples of lemma:  shit\n",
      "not enough examples of lemma:  abode\n",
      "not enough examples of lemma:  accompaniment\n",
      "not enough examples of lemma:  jelly\n",
      "not enough examples of lemma:  feast\n",
      "not enough examples of lemma:  rot\n",
      "not enough examples of lemma:  thaw\n",
      "not enough examples of lemma:  humming\n",
      "not enough examples of lemma:  ministry\n",
      "not enough examples of lemma:  contemplation\n",
      "not enough examples of lemma:  shrubbery\n",
      "not enough examples of lemma:  pretence\n",
      "not enough examples of lemma:  pharmacy\n",
      "not enough examples of lemma:  calendar\n",
      "not enough examples of lemma:  antecedent\n",
      "not enough examples of lemma:  landmark\n",
      "not enough examples of lemma:  gassing\n",
      "not enough examples of lemma:  knot\n",
      "not enough examples of lemma:  crudity\n",
      "not enough examples of lemma:  leave\n",
      "not enough examples of lemma:  hurt\n",
      "not enough examples of lemma:  murmur\n",
      "not enough examples of lemma:  oscillation\n",
      "not enough examples of lemma:  sardine\n",
      "not enough examples of lemma:  sanitation\n",
      "not enough examples of lemma:  monster\n",
      "not enough examples of lemma:  birmingham\n",
      "not enough examples of lemma:  humanity\n",
      "not enough examples of lemma:  creed\n",
      "not enough examples of lemma:  pioneer\n",
      "not enough examples of lemma:  groove\n",
      "not enough examples of lemma:  overture\n",
      "not enough examples of lemma:  lust\n",
      "not enough examples of lemma:  salvo\n",
      "not enough examples of lemma:  loft\n",
      "not enough examples of lemma:  confinement\n",
      "not enough examples of lemma:  index\n",
      "not enough examples of lemma:  malady\n",
      "not enough examples of lemma:  attachment\n",
      "not enough examples of lemma:  self-will\n",
      "not enough examples of lemma:  corduroy\n",
      "not enough examples of lemma:  diplomacy\n",
      "not enough examples of lemma:  arch\n",
      "not enough examples of lemma:  instrumentality\n",
      "not enough examples of lemma:  fullness\n",
      "not enough examples of lemma:  nurse\n",
      "not enough examples of lemma:  cartwheel\n",
      "not enough examples of lemma:  founder\n",
      "not enough examples of lemma:  misconstruction\n",
      "not enough examples of lemma:  flag\n",
      "not enough examples of lemma:  currency\n",
      "not enough examples of lemma:  deposit\n",
      "not enough examples of lemma:  dealing\n",
      "not enough examples of lemma:  northeast\n",
      "not enough examples of lemma:  hollyhock\n",
      "not enough examples of lemma:  brooding\n",
      "not enough examples of lemma:  aggressor\n",
      "not enough examples of lemma:  tumbler\n",
      "not enough examples of lemma:  portrayal\n",
      "not enough examples of lemma:  inspiration\n",
      "not enough examples of lemma:  pavement\n",
      "not enough examples of lemma:  hypocrisy\n",
      "not enough examples of lemma:  chorus\n",
      "not enough examples of lemma:  toll\n",
      "not enough examples of lemma:  claw\n",
      "not enough examples of lemma:  palette\n",
      "not enough examples of lemma:  hint\n",
      "not enough examples of lemma:  consolidation\n",
      "not enough examples of lemma:  coverage\n",
      "not enough examples of lemma:  attainment\n",
      "not enough examples of lemma:  southwest\n",
      "not enough examples of lemma:  bathing\n",
      "not enough examples of lemma:  assyrian\n",
      "not enough examples of lemma:  nymph\n",
      "not enough examples of lemma:  equality\n",
      "not enough examples of lemma:  exterior\n",
      "not enough examples of lemma:  symbolism\n",
      "not enough examples of lemma:  fermentation\n",
      "not enough examples of lemma:  porter\n",
      "not enough examples of lemma:  chord\n",
      "not enough examples of lemma:  mold\n",
      "not enough examples of lemma:  clown\n",
      "not enough examples of lemma:  keeping\n",
      "not enough examples of lemma:  redundancy\n",
      "not enough examples of lemma:  descendant\n",
      "not enough examples of lemma:  tidewater\n",
      "not enough examples of lemma:  forerunner\n",
      "not enough examples of lemma:  villain\n",
      "not enough examples of lemma:  lease\n",
      "not enough examples of lemma:  grievance\n",
      "not enough examples of lemma:  concurrence\n",
      "not enough examples of lemma:  gum\n",
      "not enough examples of lemma:  emergence\n",
      "not enough examples of lemma:  stimulant\n",
      "not enough examples of lemma:  trio\n",
      "not enough examples of lemma:  blackout\n",
      "not enough examples of lemma:  propagation\n",
      "not enough examples of lemma:  inconvenience\n",
      "not enough examples of lemma:  nobility\n",
      "not enough examples of lemma:  wait\n",
      "not enough examples of lemma:  sitting\n",
      "not enough examples of lemma:  expanse\n",
      "not enough examples of lemma:  tiger\n",
      "not enough examples of lemma:  buzz\n",
      "not enough examples of lemma:  spacing\n",
      "writing to  ../results/semcor_analysis_model.modabs.mc_rae_real.5k.mu1_1.mu2_0.1.mu3_0.001.mu4_5.nnk_4.csv\n",
      "****************************************\n",
      "*** Running correlation on  ../trained_models/model.modabs.mc_rae_real.1k.mu1_1.mu2_0.1.mu3_1e-07.mu4_10.nnk_4 model ***\n",
      "****************************************\n",
      "reading from  ../results/semcor_pairwise_data_model.modabs.mc_rae_real.1k.mu1_1.mu2_0.1.mu3_1e-07.mu4_10.nnk_4.csv\n",
      "not enough examples of lemma:  tan\n",
      "not enough examples of lemma:  velour\n",
      "not enough examples of lemma:  hillbilly\n",
      "not enough examples of lemma:  scouring\n",
      "not enough examples of lemma:  inferiority\n",
      "not enough examples of lemma:  shibboleth\n",
      "not enough examples of lemma:  traditionalism\n",
      "not enough examples of lemma:  bawh\n",
      "not enough examples of lemma:  carelessness\n",
      "not enough examples of lemma:  scraping\n",
      "not enough examples of lemma:  emanation\n",
      "not enough examples of lemma:  wattle\n",
      "not enough examples of lemma:  privy\n",
      "not enough examples of lemma:  penance\n",
      "not enough examples of lemma:  reverse\n",
      "not enough examples of lemma:  surrender\n",
      "not enough examples of lemma:  mask\n",
      "not enough examples of lemma:  pulse\n",
      "not enough examples of lemma:  noun\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not enough examples of lemma:  disregard\n",
      "not enough examples of lemma:  subscription\n",
      "not enough examples of lemma:  brotherhood\n",
      "not enough examples of lemma:  bankruptcy\n",
      "not enough examples of lemma:  booth\n",
      "not enough examples of lemma:  small\n",
      "not enough examples of lemma:  provincialism\n",
      "not enough examples of lemma:  pinpoint\n",
      "not enough examples of lemma:  inflection\n",
      "not enough examples of lemma:  believer\n",
      "not enough examples of lemma:  contributor\n",
      "not enough examples of lemma:  instability\n",
      "not enough examples of lemma:  rhyme\n",
      "not enough examples of lemma:  confederacy\n",
      "not enough examples of lemma:  zeal\n",
      "not enough examples of lemma:  florist\n",
      "not enough examples of lemma:  streak\n",
      "not enough examples of lemma:  landslide\n",
      "not enough examples of lemma:  lane\n",
      "not enough examples of lemma:  humanism\n",
      "not enough examples of lemma:  hulk\n",
      "not enough examples of lemma:  accessory\n",
      "not enough examples of lemma:  firmness\n",
      "not enough examples of lemma:  honour\n",
      "not enough examples of lemma:  italian\n",
      "not enough examples of lemma:  japanese\n",
      "not enough examples of lemma:  slant\n",
      "not enough examples of lemma:  urging\n",
      "not enough examples of lemma:  helper\n",
      "not enough examples of lemma:  merriment\n",
      "not enough examples of lemma:  heaven\n",
      "not enough examples of lemma:  anachronism\n",
      "not enough examples of lemma:  expansiveness\n",
      "not enough examples of lemma:  pine\n",
      "not enough examples of lemma:  violet\n",
      "not enough examples of lemma:  dictate\n",
      "not enough examples of lemma:  helm\n",
      "not enough examples of lemma:  refinement\n",
      "not enough examples of lemma:  sail\n",
      "not enough examples of lemma:  captivity\n",
      "not enough examples of lemma:  reliance\n",
      "not enough examples of lemma:  cultivation\n",
      "not enough examples of lemma:  random\n",
      "not enough examples of lemma:  binomial\n",
      "not enough examples of lemma:  dash\n",
      "not enough examples of lemma:  primate\n",
      "not enough examples of lemma:  roundhouse\n",
      "not enough examples of lemma:  renewal\n",
      "not enough examples of lemma:  dislocation\n",
      "not enough examples of lemma:  herb\n",
      "not enough examples of lemma:  incarnation\n",
      "not enough examples of lemma:  loophole\n",
      "not enough examples of lemma:  jab\n",
      "not enough examples of lemma:  assist\n",
      "not enough examples of lemma:  flare\n",
      "not enough examples of lemma:  trademark\n",
      "not enough examples of lemma:  drafting\n",
      "not enough examples of lemma:  burn\n",
      "not enough examples of lemma:  layout\n",
      "not enough examples of lemma:  nod\n",
      "not enough examples of lemma:  pore\n",
      "not enough examples of lemma:  dereliction\n",
      "not enough examples of lemma:  backbone\n",
      "not enough examples of lemma:  orgy\n",
      "not enough examples of lemma:  mentality\n",
      "not enough examples of lemma:  down\n",
      "not enough examples of lemma:  marking\n",
      "not enough examples of lemma:  dissent\n",
      "not enough examples of lemma:  allocation\n",
      "not enough examples of lemma:  hound\n",
      "not enough examples of lemma:  detachment\n",
      "not enough examples of lemma:  mouthpiece\n",
      "not enough examples of lemma:  infliction\n",
      "not enough examples of lemma:  allegation\n",
      "not enough examples of lemma:  falsity\n",
      "not enough examples of lemma:  lad\n",
      "not enough examples of lemma:  hysteria\n",
      "not enough examples of lemma:  circus\n",
      "not enough examples of lemma:  syndicate\n",
      "not enough examples of lemma:  gage\n",
      "not enough examples of lemma:  hardship\n",
      "not enough examples of lemma:  bound\n",
      "not enough examples of lemma:  alligator\n",
      "not enough examples of lemma:  tangle\n",
      "not enough examples of lemma:  hoop\n",
      "not enough examples of lemma:  southeast\n",
      "not enough examples of lemma:  keynote\n",
      "not enough examples of lemma:  preoccupation\n",
      "not enough examples of lemma:  signature\n",
      "not enough examples of lemma:  premise\n",
      "not enough examples of lemma:  duplication\n",
      "not enough examples of lemma:  knob\n",
      "not enough examples of lemma:  chute\n",
      "not enough examples of lemma:  brilliance\n",
      "not enough examples of lemma:  frost\n",
      "not enough examples of lemma:  subtlety\n",
      "not enough examples of lemma:  rigger\n",
      "not enough examples of lemma:  showing\n",
      "not enough examples of lemma:  upset\n",
      "not enough examples of lemma:  percussion\n",
      "not enough examples of lemma:  genus\n",
      "not enough examples of lemma:  ringing\n",
      "not enough examples of lemma:  miniature\n",
      "not enough examples of lemma:  viewer\n",
      "not enough examples of lemma:  setup\n",
      "not enough examples of lemma:  rut\n",
      "not enough examples of lemma:  outrage\n",
      "not enough examples of lemma:  rearing\n",
      "not enough examples of lemma:  squatting\n",
      "not enough examples of lemma:  sash\n",
      "not enough examples of lemma:  marketplace\n",
      "not enough examples of lemma:  fancy\n",
      "not enough examples of lemma:  prophecy\n",
      "not enough examples of lemma:  configuration\n",
      "not enough examples of lemma:  congestion\n",
      "not enough examples of lemma:  brevity\n",
      "not enough examples of lemma:  conveyor\n",
      "not enough examples of lemma:  liaison\n",
      "not enough examples of lemma:  reunion\n",
      "not enough examples of lemma:  pearl\n",
      "not enough examples of lemma:  sage\n",
      "not enough examples of lemma:  cadre\n",
      "not enough examples of lemma:  profundity\n",
      "not enough examples of lemma:  handicap\n",
      "not enough examples of lemma:  coupon\n",
      "not enough examples of lemma:  rod\n",
      "not enough examples of lemma:  ransom\n",
      "not enough examples of lemma:  flush\n",
      "not enough examples of lemma:  bass\n",
      "not enough examples of lemma:  open\n",
      "not enough examples of lemma:  coordination\n",
      "not enough examples of lemma:  peculiarity\n",
      "not enough examples of lemma:  video\n",
      "not enough examples of lemma:  pickup\n",
      "not enough examples of lemma:  stubbornness\n",
      "not enough examples of lemma:  trim\n",
      "not enough examples of lemma:  illumination\n",
      "not enough examples of lemma:  shit\n",
      "not enough examples of lemma:  abode\n",
      "not enough examples of lemma:  accompaniment\n",
      "not enough examples of lemma:  jelly\n",
      "not enough examples of lemma:  feast\n",
      "not enough examples of lemma:  rot\n",
      "not enough examples of lemma:  thaw\n",
      "not enough examples of lemma:  humming\n",
      "not enough examples of lemma:  ministry\n",
      "not enough examples of lemma:  contemplation\n",
      "not enough examples of lemma:  shrubbery\n",
      "not enough examples of lemma:  pretence\n",
      "not enough examples of lemma:  pharmacy\n",
      "not enough examples of lemma:  calendar\n",
      "not enough examples of lemma:  antecedent\n",
      "not enough examples of lemma:  landmark\n",
      "not enough examples of lemma:  gassing\n",
      "not enough examples of lemma:  knot\n",
      "not enough examples of lemma:  crudity\n",
      "not enough examples of lemma:  leave\n",
      "not enough examples of lemma:  hurt\n",
      "not enough examples of lemma:  murmur\n",
      "not enough examples of lemma:  oscillation\n",
      "not enough examples of lemma:  sardine\n",
      "not enough examples of lemma:  sanitation\n",
      "not enough examples of lemma:  monster\n",
      "not enough examples of lemma:  birmingham\n",
      "not enough examples of lemma:  humanity\n",
      "not enough examples of lemma:  creed\n",
      "not enough examples of lemma:  pioneer\n",
      "not enough examples of lemma:  groove\n",
      "not enough examples of lemma:  overture\n",
      "not enough examples of lemma:  lust\n",
      "not enough examples of lemma:  salvo\n",
      "not enough examples of lemma:  loft\n",
      "not enough examples of lemma:  confinement\n",
      "not enough examples of lemma:  index\n",
      "not enough examples of lemma:  malady\n",
      "not enough examples of lemma:  attachment\n",
      "not enough examples of lemma:  self-will\n",
      "not enough examples of lemma:  corduroy\n",
      "not enough examples of lemma:  diplomacy\n",
      "not enough examples of lemma:  arch\n",
      "not enough examples of lemma:  instrumentality\n",
      "not enough examples of lemma:  fullness\n",
      "not enough examples of lemma:  nurse\n",
      "not enough examples of lemma:  cartwheel\n",
      "not enough examples of lemma:  founder\n",
      "not enough examples of lemma:  misconstruction\n",
      "not enough examples of lemma:  flag\n",
      "not enough examples of lemma:  currency\n",
      "not enough examples of lemma:  deposit\n",
      "not enough examples of lemma:  dealing\n",
      "not enough examples of lemma:  northeast\n",
      "not enough examples of lemma:  hollyhock\n",
      "not enough examples of lemma:  brooding\n",
      "not enough examples of lemma:  aggressor\n",
      "not enough examples of lemma:  tumbler\n",
      "not enough examples of lemma:  portrayal\n",
      "not enough examples of lemma:  inspiration\n",
      "not enough examples of lemma:  pavement\n",
      "not enough examples of lemma:  hypocrisy\n",
      "not enough examples of lemma:  chorus\n",
      "not enough examples of lemma:  toll\n",
      "not enough examples of lemma:  claw\n",
      "not enough examples of lemma:  palette\n",
      "not enough examples of lemma:  hint\n",
      "not enough examples of lemma:  consolidation\n",
      "not enough examples of lemma:  coverage\n",
      "not enough examples of lemma:  attainment\n",
      "not enough examples of lemma:  southwest\n",
      "not enough examples of lemma:  bathing\n",
      "not enough examples of lemma:  assyrian\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not enough examples of lemma:  nymph\n",
      "not enough examples of lemma:  equality\n",
      "not enough examples of lemma:  exterior\n",
      "not enough examples of lemma:  symbolism\n",
      "not enough examples of lemma:  fermentation\n",
      "not enough examples of lemma:  porter\n",
      "not enough examples of lemma:  chord\n",
      "not enough examples of lemma:  mold\n",
      "not enough examples of lemma:  clown\n",
      "not enough examples of lemma:  keeping\n",
      "not enough examples of lemma:  redundancy\n",
      "not enough examples of lemma:  descendant\n",
      "not enough examples of lemma:  tidewater\n",
      "not enough examples of lemma:  forerunner\n",
      "not enough examples of lemma:  villain\n",
      "not enough examples of lemma:  lease\n",
      "not enough examples of lemma:  grievance\n",
      "not enough examples of lemma:  concurrence\n",
      "not enough examples of lemma:  gum\n",
      "not enough examples of lemma:  emergence\n",
      "not enough examples of lemma:  stimulant\n",
      "not enough examples of lemma:  trio\n",
      "not enough examples of lemma:  blackout\n",
      "not enough examples of lemma:  propagation\n",
      "not enough examples of lemma:  inconvenience\n",
      "not enough examples of lemma:  nobility\n",
      "not enough examples of lemma:  wait\n",
      "not enough examples of lemma:  sitting\n",
      "not enough examples of lemma:  expanse\n",
      "not enough examples of lemma:  tiger\n",
      "not enough examples of lemma:  buzz\n",
      "not enough examples of lemma:  spacing\n",
      "writing to  ../results/semcor_analysis_model.modabs.mc_rae_real.1k.mu1_1.mu2_0.1.mu3_1e-07.mu4_10.nnk_4.csv\n",
      "****************************************\n",
      "*** Running correlation on  ../trained_models/model.modabs.binder.5k.mu1_1.mu2_0.1.mu3_0.001.mu4_5.nnk_4 model ***\n",
      "****************************************\n",
      "reading from  ../results/semcor_pairwise_data_model.modabs.binder.5k.mu1_1.mu2_0.1.mu3_0.001.mu4_5.nnk_4.csv\n",
      "not enough examples of lemma:  tan\n",
      "not enough examples of lemma:  velour\n",
      "not enough examples of lemma:  hillbilly\n",
      "not enough examples of lemma:  scouring\n",
      "not enough examples of lemma:  inferiority\n",
      "not enough examples of lemma:  shibboleth\n",
      "not enough examples of lemma:  traditionalism\n",
      "not enough examples of lemma:  bawh\n",
      "not enough examples of lemma:  carelessness\n",
      "not enough examples of lemma:  scraping\n",
      "not enough examples of lemma:  emanation\n",
      "not enough examples of lemma:  wattle\n",
      "not enough examples of lemma:  privy\n",
      "not enough examples of lemma:  penance\n",
      "not enough examples of lemma:  reverse\n",
      "not enough examples of lemma:  surrender\n",
      "not enough examples of lemma:  mask\n",
      "not enough examples of lemma:  pulse\n",
      "not enough examples of lemma:  noun\n",
      "not enough examples of lemma:  disregard\n",
      "not enough examples of lemma:  subscription\n",
      "not enough examples of lemma:  brotherhood\n",
      "not enough examples of lemma:  bankruptcy\n",
      "not enough examples of lemma:  booth\n",
      "not enough examples of lemma:  small\n",
      "not enough examples of lemma:  provincialism\n",
      "not enough examples of lemma:  pinpoint\n",
      "not enough examples of lemma:  inflection\n",
      "not enough examples of lemma:  believer\n",
      "not enough examples of lemma:  contributor\n",
      "not enough examples of lemma:  instability\n",
      "not enough examples of lemma:  rhyme\n",
      "not enough examples of lemma:  confederacy\n",
      "not enough examples of lemma:  zeal\n",
      "not enough examples of lemma:  florist\n",
      "not enough examples of lemma:  streak\n",
      "not enough examples of lemma:  landslide\n",
      "not enough examples of lemma:  lane\n",
      "not enough examples of lemma:  humanism\n",
      "not enough examples of lemma:  hulk\n",
      "not enough examples of lemma:  accessory\n",
      "not enough examples of lemma:  firmness\n",
      "not enough examples of lemma:  honour\n",
      "not enough examples of lemma:  italian\n",
      "not enough examples of lemma:  japanese\n",
      "not enough examples of lemma:  slant\n",
      "not enough examples of lemma:  urging\n",
      "not enough examples of lemma:  helper\n",
      "not enough examples of lemma:  merriment\n",
      "not enough examples of lemma:  heaven\n",
      "not enough examples of lemma:  anachronism\n",
      "not enough examples of lemma:  expansiveness\n",
      "not enough examples of lemma:  pine\n",
      "not enough examples of lemma:  violet\n",
      "not enough examples of lemma:  dictate\n",
      "not enough examples of lemma:  helm\n",
      "not enough examples of lemma:  refinement\n",
      "not enough examples of lemma:  sail\n",
      "not enough examples of lemma:  captivity\n",
      "not enough examples of lemma:  reliance\n",
      "not enough examples of lemma:  cultivation\n",
      "not enough examples of lemma:  random\n",
      "not enough examples of lemma:  binomial\n",
      "not enough examples of lemma:  dash\n",
      "not enough examples of lemma:  primate\n",
      "not enough examples of lemma:  roundhouse\n",
      "not enough examples of lemma:  renewal\n",
      "not enough examples of lemma:  dislocation\n",
      "not enough examples of lemma:  herb\n",
      "not enough examples of lemma:  incarnation\n",
      "not enough examples of lemma:  loophole\n",
      "not enough examples of lemma:  jab\n",
      "not enough examples of lemma:  assist\n",
      "not enough examples of lemma:  flare\n",
      "not enough examples of lemma:  trademark\n",
      "not enough examples of lemma:  drafting\n",
      "not enough examples of lemma:  burn\n",
      "not enough examples of lemma:  layout\n",
      "not enough examples of lemma:  nod\n",
      "not enough examples of lemma:  pore\n",
      "not enough examples of lemma:  dereliction\n",
      "not enough examples of lemma:  backbone\n",
      "not enough examples of lemma:  orgy\n",
      "not enough examples of lemma:  mentality\n",
      "not enough examples of lemma:  down\n",
      "not enough examples of lemma:  marking\n",
      "not enough examples of lemma:  dissent\n",
      "not enough examples of lemma:  allocation\n",
      "not enough examples of lemma:  hound\n",
      "not enough examples of lemma:  detachment\n",
      "not enough examples of lemma:  mouthpiece\n",
      "not enough examples of lemma:  infliction\n",
      "not enough examples of lemma:  allegation\n",
      "not enough examples of lemma:  falsity\n",
      "not enough examples of lemma:  lad\n",
      "not enough examples of lemma:  hysteria\n",
      "not enough examples of lemma:  circus\n",
      "not enough examples of lemma:  syndicate\n",
      "not enough examples of lemma:  gage\n",
      "not enough examples of lemma:  hardship\n",
      "not enough examples of lemma:  bound\n",
      "not enough examples of lemma:  alligator\n",
      "not enough examples of lemma:  tangle\n",
      "not enough examples of lemma:  hoop\n",
      "not enough examples of lemma:  southeast\n",
      "not enough examples of lemma:  keynote\n",
      "not enough examples of lemma:  preoccupation\n",
      "not enough examples of lemma:  signature\n",
      "not enough examples of lemma:  premise\n",
      "not enough examples of lemma:  duplication\n",
      "not enough examples of lemma:  knob\n",
      "not enough examples of lemma:  chute\n",
      "not enough examples of lemma:  brilliance\n",
      "not enough examples of lemma:  frost\n",
      "not enough examples of lemma:  subtlety\n",
      "not enough examples of lemma:  rigger\n",
      "not enough examples of lemma:  showing\n",
      "not enough examples of lemma:  upset\n",
      "not enough examples of lemma:  percussion\n",
      "not enough examples of lemma:  genus\n",
      "not enough examples of lemma:  ringing\n",
      "not enough examples of lemma:  miniature\n",
      "not enough examples of lemma:  viewer\n",
      "not enough examples of lemma:  setup\n",
      "not enough examples of lemma:  rut\n",
      "not enough examples of lemma:  outrage\n",
      "not enough examples of lemma:  rearing\n",
      "not enough examples of lemma:  squatting\n",
      "not enough examples of lemma:  sash\n",
      "not enough examples of lemma:  marketplace\n",
      "not enough examples of lemma:  fancy\n",
      "not enough examples of lemma:  prophecy\n",
      "not enough examples of lemma:  configuration\n",
      "not enough examples of lemma:  congestion\n",
      "not enough examples of lemma:  brevity\n",
      "not enough examples of lemma:  conveyor\n",
      "not enough examples of lemma:  liaison\n",
      "not enough examples of lemma:  reunion\n",
      "not enough examples of lemma:  pearl\n",
      "not enough examples of lemma:  sage\n",
      "not enough examples of lemma:  cadre\n",
      "not enough examples of lemma:  profundity\n",
      "not enough examples of lemma:  handicap\n",
      "not enough examples of lemma:  coupon\n",
      "not enough examples of lemma:  rod\n",
      "not enough examples of lemma:  ransom\n",
      "not enough examples of lemma:  flush\n",
      "not enough examples of lemma:  bass\n",
      "not enough examples of lemma:  open\n",
      "not enough examples of lemma:  coordination\n",
      "not enough examples of lemma:  peculiarity\n",
      "not enough examples of lemma:  video\n",
      "not enough examples of lemma:  pickup\n",
      "not enough examples of lemma:  stubbornness\n",
      "not enough examples of lemma:  trim\n",
      "not enough examples of lemma:  illumination\n",
      "not enough examples of lemma:  shit\n",
      "not enough examples of lemma:  abode\n",
      "not enough examples of lemma:  accompaniment\n",
      "not enough examples of lemma:  jelly\n",
      "not enough examples of lemma:  feast\n",
      "not enough examples of lemma:  rot\n",
      "not enough examples of lemma:  thaw\n",
      "not enough examples of lemma:  humming\n",
      "not enough examples of lemma:  ministry\n",
      "not enough examples of lemma:  contemplation\n",
      "not enough examples of lemma:  shrubbery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not enough examples of lemma:  pretence\n",
      "not enough examples of lemma:  pharmacy\n",
      "not enough examples of lemma:  calendar\n",
      "not enough examples of lemma:  antecedent\n",
      "not enough examples of lemma:  landmark\n",
      "not enough examples of lemma:  gassing\n",
      "not enough examples of lemma:  knot\n",
      "not enough examples of lemma:  crudity\n",
      "not enough examples of lemma:  leave\n",
      "not enough examples of lemma:  hurt\n",
      "not enough examples of lemma:  murmur\n",
      "not enough examples of lemma:  oscillation\n",
      "not enough examples of lemma:  sardine\n",
      "not enough examples of lemma:  sanitation\n",
      "not enough examples of lemma:  monster\n",
      "not enough examples of lemma:  birmingham\n",
      "not enough examples of lemma:  humanity\n",
      "not enough examples of lemma:  creed\n",
      "not enough examples of lemma:  pioneer\n",
      "not enough examples of lemma:  groove\n",
      "not enough examples of lemma:  overture\n",
      "not enough examples of lemma:  lust\n",
      "not enough examples of lemma:  salvo\n",
      "not enough examples of lemma:  loft\n",
      "not enough examples of lemma:  confinement\n",
      "not enough examples of lemma:  index\n",
      "not enough examples of lemma:  malady\n",
      "not enough examples of lemma:  attachment\n",
      "not enough examples of lemma:  self-will\n",
      "not enough examples of lemma:  corduroy\n",
      "not enough examples of lemma:  diplomacy\n",
      "not enough examples of lemma:  arch\n",
      "not enough examples of lemma:  instrumentality\n",
      "not enough examples of lemma:  fullness\n",
      "not enough examples of lemma:  nurse\n",
      "not enough examples of lemma:  cartwheel\n",
      "not enough examples of lemma:  founder\n",
      "not enough examples of lemma:  misconstruction\n",
      "not enough examples of lemma:  flag\n",
      "not enough examples of lemma:  currency\n",
      "not enough examples of lemma:  deposit\n",
      "not enough examples of lemma:  dealing\n",
      "not enough examples of lemma:  northeast\n",
      "not enough examples of lemma:  hollyhock\n",
      "not enough examples of lemma:  brooding\n",
      "not enough examples of lemma:  aggressor\n",
      "not enough examples of lemma:  tumbler\n",
      "not enough examples of lemma:  portrayal\n",
      "not enough examples of lemma:  inspiration\n",
      "not enough examples of lemma:  pavement\n",
      "not enough examples of lemma:  hypocrisy\n",
      "not enough examples of lemma:  chorus\n",
      "not enough examples of lemma:  toll\n",
      "not enough examples of lemma:  claw\n",
      "not enough examples of lemma:  palette\n",
      "not enough examples of lemma:  hint\n",
      "not enough examples of lemma:  consolidation\n",
      "not enough examples of lemma:  coverage\n",
      "not enough examples of lemma:  attainment\n",
      "not enough examples of lemma:  southwest\n",
      "not enough examples of lemma:  bathing\n",
      "not enough examples of lemma:  assyrian\n",
      "not enough examples of lemma:  nymph\n",
      "not enough examples of lemma:  equality\n",
      "not enough examples of lemma:  exterior\n",
      "not enough examples of lemma:  symbolism\n",
      "not enough examples of lemma:  fermentation\n",
      "not enough examples of lemma:  porter\n",
      "not enough examples of lemma:  chord\n",
      "not enough examples of lemma:  mold\n",
      "not enough examples of lemma:  clown\n",
      "not enough examples of lemma:  keeping\n",
      "not enough examples of lemma:  redundancy\n",
      "not enough examples of lemma:  descendant\n",
      "not enough examples of lemma:  tidewater\n",
      "not enough examples of lemma:  forerunner\n",
      "not enough examples of lemma:  villain\n",
      "not enough examples of lemma:  lease\n",
      "not enough examples of lemma:  grievance\n",
      "not enough examples of lemma:  concurrence\n",
      "not enough examples of lemma:  gum\n",
      "not enough examples of lemma:  emergence\n",
      "not enough examples of lemma:  stimulant\n",
      "not enough examples of lemma:  trio\n",
      "not enough examples of lemma:  blackout\n",
      "not enough examples of lemma:  propagation\n",
      "not enough examples of lemma:  inconvenience\n",
      "not enough examples of lemma:  nobility\n",
      "not enough examples of lemma:  wait\n",
      "not enough examples of lemma:  sitting\n",
      "not enough examples of lemma:  expanse\n",
      "not enough examples of lemma:  tiger\n",
      "not enough examples of lemma:  buzz\n",
      "not enough examples of lemma:  spacing\n",
      "writing to  ../results/semcor_analysis_model.modabs.binder.5k.mu1_1.mu2_0.1.mu3_0.001.mu4_5.nnk_4.csv\n",
      "****************************************\n",
      "*** Running correlation on  ../trained_models/model.modabs.binder.1k.mu1_1.mu2_0.1.mu3_0.001.mu4_5.nnk_4 model ***\n",
      "****************************************\n",
      "reading from  ../results/semcor_pairwise_data_model.modabs.binder.1k.mu1_1.mu2_0.1.mu3_0.001.mu4_5.nnk_4.csv\n",
      "not enough examples of lemma:  tan\n",
      "not enough examples of lemma:  velour\n",
      "not enough examples of lemma:  hillbilly\n",
      "not enough examples of lemma:  scouring\n",
      "not enough examples of lemma:  inferiority\n",
      "not enough examples of lemma:  shibboleth\n",
      "not enough examples of lemma:  traditionalism\n",
      "not enough examples of lemma:  bawh\n",
      "not enough examples of lemma:  carelessness\n",
      "not enough examples of lemma:  scraping\n",
      "not enough examples of lemma:  emanation\n",
      "not enough examples of lemma:  wattle\n",
      "not enough examples of lemma:  privy\n",
      "not enough examples of lemma:  penance\n",
      "not enough examples of lemma:  reverse\n",
      "not enough examples of lemma:  surrender\n",
      "not enough examples of lemma:  mask\n",
      "not enough examples of lemma:  pulse\n",
      "not enough examples of lemma:  noun\n",
      "not enough examples of lemma:  disregard\n",
      "not enough examples of lemma:  subscription\n",
      "not enough examples of lemma:  brotherhood\n",
      "not enough examples of lemma:  bankruptcy\n",
      "not enough examples of lemma:  booth\n",
      "not enough examples of lemma:  small\n",
      "not enough examples of lemma:  provincialism\n",
      "not enough examples of lemma:  pinpoint\n",
      "not enough examples of lemma:  inflection\n",
      "not enough examples of lemma:  believer\n",
      "not enough examples of lemma:  contributor\n",
      "not enough examples of lemma:  instability\n",
      "not enough examples of lemma:  rhyme\n",
      "not enough examples of lemma:  confederacy\n",
      "not enough examples of lemma:  zeal\n",
      "not enough examples of lemma:  florist\n",
      "not enough examples of lemma:  streak\n",
      "not enough examples of lemma:  landslide\n",
      "not enough examples of lemma:  lane\n",
      "not enough examples of lemma:  humanism\n",
      "not enough examples of lemma:  hulk\n",
      "not enough examples of lemma:  accessory\n",
      "not enough examples of lemma:  firmness\n",
      "not enough examples of lemma:  honour\n",
      "not enough examples of lemma:  italian\n",
      "not enough examples of lemma:  japanese\n",
      "not enough examples of lemma:  slant\n",
      "not enough examples of lemma:  urging\n",
      "not enough examples of lemma:  helper\n",
      "not enough examples of lemma:  merriment\n",
      "not enough examples of lemma:  heaven\n",
      "not enough examples of lemma:  anachronism\n",
      "not enough examples of lemma:  expansiveness\n",
      "not enough examples of lemma:  pine\n",
      "not enough examples of lemma:  violet\n",
      "not enough examples of lemma:  dictate\n",
      "not enough examples of lemma:  helm\n",
      "not enough examples of lemma:  refinement\n",
      "not enough examples of lemma:  sail\n",
      "not enough examples of lemma:  captivity\n",
      "not enough examples of lemma:  reliance\n",
      "not enough examples of lemma:  cultivation\n",
      "not enough examples of lemma:  random\n",
      "not enough examples of lemma:  binomial\n",
      "not enough examples of lemma:  dash\n",
      "not enough examples of lemma:  primate\n",
      "not enough examples of lemma:  roundhouse\n",
      "not enough examples of lemma:  renewal\n",
      "not enough examples of lemma:  dislocation\n",
      "not enough examples of lemma:  herb\n",
      "not enough examples of lemma:  incarnation\n",
      "not enough examples of lemma:  loophole\n",
      "not enough examples of lemma:  jab\n",
      "not enough examples of lemma:  assist\n",
      "not enough examples of lemma:  flare\n",
      "not enough examples of lemma:  trademark\n",
      "not enough examples of lemma:  drafting\n",
      "not enough examples of lemma:  burn\n",
      "not enough examples of lemma:  layout\n",
      "not enough examples of lemma:  nod\n",
      "not enough examples of lemma:  pore\n",
      "not enough examples of lemma:  dereliction\n",
      "not enough examples of lemma:  backbone\n",
      "not enough examples of lemma:  orgy\n",
      "not enough examples of lemma:  mentality\n",
      "not enough examples of lemma:  down\n",
      "not enough examples of lemma:  marking\n",
      "not enough examples of lemma:  dissent\n",
      "not enough examples of lemma:  allocation\n",
      "not enough examples of lemma:  hound\n",
      "not enough examples of lemma:  detachment\n",
      "not enough examples of lemma:  mouthpiece\n",
      "not enough examples of lemma:  infliction\n",
      "not enough examples of lemma:  allegation\n",
      "not enough examples of lemma:  falsity\n",
      "not enough examples of lemma:  lad\n",
      "not enough examples of lemma:  hysteria\n",
      "not enough examples of lemma:  circus\n",
      "not enough examples of lemma:  syndicate\n",
      "not enough examples of lemma:  gage\n",
      "not enough examples of lemma:  hardship\n",
      "not enough examples of lemma:  bound\n",
      "not enough examples of lemma:  alligator\n",
      "not enough examples of lemma:  tangle\n",
      "not enough examples of lemma:  hoop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not enough examples of lemma:  southeast\n",
      "not enough examples of lemma:  keynote\n",
      "not enough examples of lemma:  preoccupation\n",
      "not enough examples of lemma:  signature\n",
      "not enough examples of lemma:  premise\n",
      "not enough examples of lemma:  duplication\n",
      "not enough examples of lemma:  knob\n",
      "not enough examples of lemma:  chute\n",
      "not enough examples of lemma:  brilliance\n",
      "not enough examples of lemma:  frost\n",
      "not enough examples of lemma:  subtlety\n",
      "not enough examples of lemma:  rigger\n",
      "not enough examples of lemma:  showing\n",
      "not enough examples of lemma:  upset\n",
      "not enough examples of lemma:  percussion\n",
      "not enough examples of lemma:  genus\n",
      "not enough examples of lemma:  ringing\n",
      "not enough examples of lemma:  miniature\n",
      "not enough examples of lemma:  viewer\n",
      "not enough examples of lemma:  setup\n",
      "not enough examples of lemma:  rut\n",
      "not enough examples of lemma:  outrage\n",
      "not enough examples of lemma:  rearing\n",
      "not enough examples of lemma:  squatting\n",
      "not enough examples of lemma:  sash\n",
      "not enough examples of lemma:  marketplace\n",
      "not enough examples of lemma:  fancy\n",
      "not enough examples of lemma:  prophecy\n",
      "not enough examples of lemma:  configuration\n",
      "not enough examples of lemma:  congestion\n",
      "not enough examples of lemma:  brevity\n",
      "not enough examples of lemma:  conveyor\n",
      "not enough examples of lemma:  liaison\n",
      "not enough examples of lemma:  reunion\n",
      "not enough examples of lemma:  pearl\n",
      "not enough examples of lemma:  sage\n",
      "not enough examples of lemma:  cadre\n",
      "not enough examples of lemma:  profundity\n",
      "not enough examples of lemma:  handicap\n",
      "not enough examples of lemma:  coupon\n",
      "not enough examples of lemma:  rod\n",
      "not enough examples of lemma:  ransom\n",
      "not enough examples of lemma:  flush\n",
      "not enough examples of lemma:  bass\n",
      "not enough examples of lemma:  open\n",
      "not enough examples of lemma:  coordination\n",
      "not enough examples of lemma:  peculiarity\n",
      "not enough examples of lemma:  video\n",
      "not enough examples of lemma:  pickup\n",
      "not enough examples of lemma:  stubbornness\n",
      "not enough examples of lemma:  trim\n",
      "not enough examples of lemma:  illumination\n",
      "not enough examples of lemma:  shit\n",
      "not enough examples of lemma:  abode\n",
      "not enough examples of lemma:  accompaniment\n",
      "not enough examples of lemma:  jelly\n",
      "not enough examples of lemma:  feast\n",
      "not enough examples of lemma:  rot\n",
      "not enough examples of lemma:  thaw\n",
      "not enough examples of lemma:  humming\n",
      "not enough examples of lemma:  ministry\n",
      "not enough examples of lemma:  contemplation\n",
      "not enough examples of lemma:  shrubbery\n",
      "not enough examples of lemma:  pretence\n",
      "not enough examples of lemma:  pharmacy\n",
      "not enough examples of lemma:  calendar\n",
      "not enough examples of lemma:  antecedent\n",
      "not enough examples of lemma:  landmark\n",
      "not enough examples of lemma:  gassing\n",
      "not enough examples of lemma:  knot\n",
      "not enough examples of lemma:  crudity\n",
      "not enough examples of lemma:  leave\n",
      "not enough examples of lemma:  hurt\n",
      "not enough examples of lemma:  murmur\n",
      "not enough examples of lemma:  oscillation\n",
      "not enough examples of lemma:  sardine\n",
      "not enough examples of lemma:  sanitation\n",
      "not enough examples of lemma:  monster\n",
      "not enough examples of lemma:  birmingham\n",
      "not enough examples of lemma:  humanity\n",
      "not enough examples of lemma:  creed\n",
      "not enough examples of lemma:  pioneer\n",
      "not enough examples of lemma:  groove\n",
      "not enough examples of lemma:  overture\n",
      "not enough examples of lemma:  lust\n",
      "not enough examples of lemma:  salvo\n",
      "not enough examples of lemma:  loft\n",
      "not enough examples of lemma:  confinement\n",
      "not enough examples of lemma:  index\n",
      "not enough examples of lemma:  malady\n",
      "not enough examples of lemma:  attachment\n",
      "not enough examples of lemma:  self-will\n",
      "not enough examples of lemma:  corduroy\n",
      "not enough examples of lemma:  diplomacy\n",
      "not enough examples of lemma:  arch\n",
      "not enough examples of lemma:  instrumentality\n",
      "not enough examples of lemma:  fullness\n",
      "not enough examples of lemma:  nurse\n",
      "not enough examples of lemma:  cartwheel\n",
      "not enough examples of lemma:  founder\n",
      "not enough examples of lemma:  misconstruction\n",
      "not enough examples of lemma:  flag\n",
      "not enough examples of lemma:  currency\n",
      "not enough examples of lemma:  deposit\n",
      "not enough examples of lemma:  dealing\n",
      "not enough examples of lemma:  northeast\n",
      "not enough examples of lemma:  hollyhock\n",
      "not enough examples of lemma:  brooding\n",
      "not enough examples of lemma:  aggressor\n",
      "not enough examples of lemma:  tumbler\n",
      "not enough examples of lemma:  portrayal\n",
      "not enough examples of lemma:  inspiration\n",
      "not enough examples of lemma:  pavement\n",
      "not enough examples of lemma:  hypocrisy\n",
      "not enough examples of lemma:  chorus\n",
      "not enough examples of lemma:  toll\n",
      "not enough examples of lemma:  claw\n",
      "not enough examples of lemma:  palette\n",
      "not enough examples of lemma:  hint\n",
      "not enough examples of lemma:  consolidation\n",
      "not enough examples of lemma:  coverage\n",
      "not enough examples of lemma:  attainment\n",
      "not enough examples of lemma:  southwest\n",
      "not enough examples of lemma:  bathing\n",
      "not enough examples of lemma:  assyrian\n",
      "not enough examples of lemma:  nymph\n",
      "not enough examples of lemma:  equality\n",
      "not enough examples of lemma:  exterior\n",
      "not enough examples of lemma:  symbolism\n",
      "not enough examples of lemma:  fermentation\n",
      "not enough examples of lemma:  porter\n",
      "not enough examples of lemma:  chord\n",
      "not enough examples of lemma:  mold\n",
      "not enough examples of lemma:  clown\n",
      "not enough examples of lemma:  keeping\n",
      "not enough examples of lemma:  redundancy\n",
      "not enough examples of lemma:  descendant\n",
      "not enough examples of lemma:  tidewater\n",
      "not enough examples of lemma:  forerunner\n",
      "not enough examples of lemma:  villain\n",
      "not enough examples of lemma:  lease\n",
      "not enough examples of lemma:  grievance\n",
      "not enough examples of lemma:  concurrence\n",
      "not enough examples of lemma:  gum\n",
      "not enough examples of lemma:  emergence\n",
      "not enough examples of lemma:  stimulant\n",
      "not enough examples of lemma:  trio\n",
      "not enough examples of lemma:  blackout\n",
      "not enough examples of lemma:  propagation\n",
      "not enough examples of lemma:  inconvenience\n",
      "not enough examples of lemma:  nobility\n",
      "not enough examples of lemma:  wait\n",
      "not enough examples of lemma:  sitting\n",
      "not enough examples of lemma:  expanse\n",
      "not enough examples of lemma:  tiger\n",
      "not enough examples of lemma:  buzz\n",
      "not enough examples of lemma:  spacing\n",
      "writing to  ../results/semcor_analysis_model.modabs.binder.1k.mu1_1.mu2_0.1.mu3_0.001.mu4_5.nnk_4.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "then we run it\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "for save_path in models:\n",
    "\n",
    "    print(\"****************************************\")\n",
    "    print(\"*** Running correlation on  %s model ***\" % save_path)\n",
    "    print(\"****************************************\")\n",
    "    infile = '../results/semcor_pairwise_data_' + os.path.split(save_path)[1] + '.csv'\n",
    "    outfile = '../results/semcor_analysis_' + os.path.split(save_path)[1] + '.csv'\n",
    "    print(\"reading from \", infile)\n",
    "    out_data = []\n",
    "    \n",
    "    \"\"\"\n",
    "    infile pairwise data columns\n",
    "        \"lemma\",\n",
    "        \"token_sense_1\",\n",
    "        \"token_sense_2\",\n",
    "        \"sense1_pos\",\n",
    "        \"sense2_pos\",\n",
    "        \"cos_sim\",\n",
    "        \"wup_sim\", \n",
    "        \"n_senses\",\n",
    "        \"n_word_forms\",\n",
    "        \"concreteness\"\n",
    "        \"wn_bin\"\n",
    "        \"conc_bin\"\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(infile)\n",
    "    \n",
    "    lemmas = df.lemma.unique()\n",
    "    #print(lemmas[:10])\n",
    "\n",
    "    for word in lemmas:\n",
    "        word_data = df[df.lemma == word]\n",
    "        \n",
    "        n_senses = word_data.iloc[0].n_senses\n",
    "        polysemy_bin = word_data.iloc[0].wn_bin\n",
    "        concreteness_bin = word_data.iloc[0].conc_bin\n",
    "        \n",
    "        pearson, pearson_p, spearman, spearman_p = run_correlation(word_data)\n",
    "        row = (word, len(word_data), n_senses, polysemy_bin, concreteness_bin, pearson, pearson_p, spearman, spearman_p)\n",
    "        #print(corr)\n",
    "        out_data.append(row)\n",
    "        \n",
    "        #raise Exception(\"hfjesh\")\n",
    "    \n",
    "    print(\"writing to \", outfile)\n",
    "    out_df = pd.DataFrame.from_records(out_data, columns = ['word', 'n', 'n_senses', 'polysemy_bin', 'concreteness_bin', 'pearson', 'pearson_p', 'spearman', 'spearman_p'] )\n",
    "    out_df.to_csv(outfile)\n",
    "    \n",
    "    \n",
    "    #for word in \n",
    "    #run_correlation()\n",
    "    #cols = [\"lemma\", \"token_sense_1\", \"token_sense_2\", \"cos_sim\", \"wup_sim\", \"n_senses\"]\n",
    "    #df = csv_input = pd.read_csv(infile, names=cols)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************\n",
      "*** Running correlation on  ../trained_models/model.modabs.buchanan.5k.mu1_1.mu2_0.1.mu3_0.001.mu4_5.nnk_4 model ***\n",
      "****************************************\n",
      "pearson:  0.09968364889710171\n",
      "p-value:  0.0\n",
      "n lemmas:  8021\n",
      "n tokens:  1045966\n",
      "****************************************\n",
      "*** Running correlation on  ../trained_models/model.modabs.buchanan.1k.mu1_1.mu2_0.1.mu3_1e-07.mu4_10.nnk_4 model ***\n",
      "****************************************\n",
      "pearson:  0.0993133955923324\n",
      "p-value:  0.0\n",
      "n lemmas:  8021\n",
      "n tokens:  1045966\n",
      "****************************************\n",
      "*** Running correlation on  ../trained_models/model.modabs.mc_rae_real.5k.mu1_1.mu2_0.1.mu3_0.001.mu4_5.nnk_4 model ***\n",
      "****************************************\n",
      "pearson:  -0.033206692941499465\n",
      "p-value:  6.048711961725376e-253\n",
      "n lemmas:  8021\n",
      "n tokens:  1045966\n",
      "****************************************\n",
      "*** Running correlation on  ../trained_models/model.modabs.mc_rae_real.1k.mu1_1.mu2_0.1.mu3_1e-07.mu4_10.nnk_4 model ***\n",
      "****************************************\n",
      "pearson:  -0.03320459018694799\n",
      "p-value:  6.507932743078112e-253\n",
      "n lemmas:  8021\n",
      "n tokens:  1045966\n",
      "****************************************\n",
      "*** Running correlation on  ../trained_models/model.modabs.binder.5k.mu1_1.mu2_0.1.mu3_0.001.mu4_5.nnk_4 model ***\n",
      "****************************************\n",
      "pearson:  -0.029027945690009473\n",
      "p-value:  9.227108990556368e-194\n",
      "n lemmas:  8021\n",
      "n tokens:  1045966\n",
      "****************************************\n",
      "*** Running correlation on  ../trained_models/model.modabs.binder.1k.mu1_1.mu2_0.1.mu3_0.001.mu4_5.nnk_4 model ***\n",
      "****************************************\n",
      "pearson:  -0.029102225524674487\n",
      "p-value:  9.603119406162975e-195\n",
      "n lemmas:  8021\n",
      "n tokens:  1045966\n"
     ]
    }
   ],
   "source": [
    "# lets do the correlation on the whole pairwise dataset\n",
    "\n",
    "for save_path in models:\n",
    "    print(\"****************************************\")\n",
    "    print(\"*** Running correlation on  %s model ***\" % save_path)\n",
    "    print(\"****************************************\")\n",
    "    infile = '../results/semcor_pairwise_data_' + os.path.split(save_path)[1] + '.csv'\n",
    "    df = pd.read_csv(infile)\n",
    "    pearson, pearson_p = pearsonr(df.cos_sim, df.wup_sim)\n",
    "    print(\"pearson: \", pearson)\n",
    "    print(\"p-value: \", pearson_p)\n",
    "    print(\"n lemmas: \", len(df['lemma'].unique()))\n",
    "    print(\"n tokens: \", len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets do it on the whole dataset but just for the most concrete nouns\n",
    "# or rather broken down by concreteness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.,  1.,  3.,  0.,  4., nan])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.conc_bin.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************\n",
      "*** Running correlation on  ../trained_models/model.modabs.buchanan.5k.mu1_1.mu2_0.1.mu3_0.001.mu4_5.nnk_4 model ***\n",
      "****************************************\n",
      "   bin   pearson      pearson_p  n_lemmas  n_tokens\n",
      "3  0.0 -0.078292   2.450221e-09        45      5789\n",
      "1  1.0  0.071282  1.125213e-190      1062    170306\n",
      "0  2.0  0.110042   0.000000e+00      1244    361334\n",
      "2  3.0  0.091524   0.000000e+00      1505    265386\n",
      "4  4.0  0.138588   0.000000e+00      1247    192408\n",
      "****************************************\n",
      "*** Running correlation on  ../trained_models/model.modabs.buchanan.1k.mu1_1.mu2_0.1.mu3_1e-07.mu4_10.nnk_4 model ***\n",
      "****************************************\n",
      "   bin   pearson      pearson_p  n_lemmas  n_tokens\n",
      "3  0.0 -0.018794   1.527927e-01        45      5789\n",
      "1  1.0  0.060846  2.162418e-139      1062    170306\n",
      "0  2.0  0.124655   0.000000e+00      1244    361334\n",
      "2  3.0  0.109911   0.000000e+00      1505    265386\n",
      "4  4.0  0.105070   0.000000e+00      1247    192408\n",
      "****************************************\n",
      "*** Running correlation on  ../trained_models/model.modabs.mc_rae_real.5k.mu1_1.mu2_0.1.mu3_0.001.mu4_5.nnk_4 model ***\n",
      "****************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/scipy/stats/stats.py:4023: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   bin   pearson     pearson_p  n_lemmas  n_tokens\n",
      "3  0.0       NaN           NaN        45      5789\n",
      "1  1.0  0.005503  2.314713e-02      1062    170306\n",
      "0  2.0 -0.009052  5.289346e-08      1244    361334\n",
      "2  3.0 -0.005804  2.789498e-03      1505    265386\n",
      "4  4.0 -0.016512  4.373231e-13      1247    192408\n",
      "****************************************\n",
      "*** Running correlation on  ../trained_models/model.modabs.mc_rae_real.1k.mu1_1.mu2_0.1.mu3_1e-07.mu4_10.nnk_4 model ***\n",
      "****************************************\n",
      "   bin   pearson     pearson_p  n_lemmas  n_tokens\n",
      "3  0.0       NaN           NaN        45      5789\n",
      "1  1.0  0.005444  2.466447e-02      1062    170306\n",
      "0  2.0 -0.009094  4.587882e-08      1244    361334\n",
      "2  3.0 -0.005805  2.784282e-03      1505    265386\n",
      "4  4.0 -0.015756  4.785256e-12      1247    192408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/scipy/stats/stats.py:4023: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for save_path in models:\n",
    "    print(\"****************************************\")\n",
    "    print(\"*** Running correlation on  %s model ***\" % save_path)\n",
    "    print(\"****************************************\")\n",
    "    \n",
    "    infile = '../results/semcor_pairwise_data_' + os.path.split(save_path)[1] + '.csv'\n",
    "    df = pd.read_csv(infile)\n",
    "    #drop rows that contain specific 'value' in 'column_name'\n",
    "    #df = df[df.conc_bin != \"nan\"]\n",
    "    df = df.dropna()\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    for binn in df.conc_bin.unique():\n",
    "\n",
    "        conc_df = df[df.conc_bin == binn]\n",
    "        \n",
    "\n",
    "        pearson, pearson_p = pearsonr(conc_df.cos_sim, conc_df.wup_sim)\n",
    "\n",
    "        \n",
    "        res = {\n",
    "        'bin' : binn,\n",
    "        'pearson': pearson,\n",
    "        'pearson_p' : pearson_p,\n",
    "        'n_lemmas' : len(conc_df['lemma'].unique()),\n",
    "        'n_tokens' : len(conc_df)\n",
    "        }\n",
    "        #print(\"pearson: \", pearson)\n",
    "        #print(\"p-value: \", pearson_p)\n",
    "        #print(\"n lemmas: \", n_lemmas)\n",
    "        #print(\"n tokens: \", n_tokens)\n",
    "        \n",
    "        results.append(res)\n",
    "        \n",
    "    model_results = pd.DataFrame.from_records(results).sort_values(by='bin')\n",
    "    print(model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for reference, bins are\n",
    "\n",
    "    # add in bins\n",
    "    token_similarities['wn_bin'] = pd.cut(token_similarities.n_senses, \n",
    "                        bins = [0, 2.1, 4.1, 6.1, 8.1, 10.1, 20.1, 50.1, 200], labels = False)\n",
    "    token_similarities['conc_bin'] = pd.cut(token_similarities.concreteness, \n",
    "                        bins = [0, 1.5, 2.5, 3.5, 4.5, 10], labels = False)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 5, 2, 0, 3, 4, 6])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "do it for polysemy\n",
    "\"\"\"\n",
    "df.head(5)\n",
    "df.wn_bin.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************\n",
      "*** Running correlation on  ../trained_models/model.modabs.buchanan.5k.mu1_1.mu2_0.1.mu3_0.001.mu4_5.nnk_4 model ***\n",
      "****************************************\n",
      "   bin   pearson      pearson_p  n_lemmas  n_tokens\n",
      "3    0  0.017791   1.576201e-10      5587    129304\n",
      "5    1  0.050597   9.580476e-79      1366    137699\n",
      "0    2  0.050950  5.613667e-166       794    290053\n",
      "2    3  0.048454   1.638527e-95       182    182929\n",
      "4    4  0.123787   0.000000e+00        62    107591\n",
      "6    5  0.097805  5.085332e-106        16     49761\n",
      "1    6  0.062717  5.640582e-123        13    141126\n",
      "7    7  0.057415   6.472627e-07         1      7503\n",
      "****************************************\n",
      "*** Running correlation on  ../trained_models/model.modabs.buchanan.1k.mu1_1.mu2_0.1.mu3_1e-07.mu4_10.nnk_4 model ***\n",
      "****************************************\n",
      "   bin   pearson      pearson_p  n_lemmas  n_tokens\n",
      "3    0  0.027437   5.748339e-23      5587    129304\n",
      "5    1  0.005545   3.961007e-02      1366    137699\n",
      "0    2  0.061918  2.782795e-244       794    290053\n",
      "2    3  0.081184  4.921832e-265       182    182929\n",
      "4    4  0.056072   1.167674e-75        62    107591\n",
      "6    5  0.141184  7.129280e-220        16     49761\n",
      "1    6  0.048308   1.109770e-73        13    141126\n",
      "7    7  0.002362   8.379452e-01         1      7503\n",
      "****************************************\n",
      "*** Running correlation on  ../trained_models/model.modabs.mc_rae_real.5k.mu1_1.mu2_0.1.mu3_0.001.mu4_5.nnk_4 model ***\n",
      "****************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/scipy/stats/stats.py:4023: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   bin   pearson     pearson_p  n_lemmas  n_tokens\n",
      "3    0 -0.005032  7.040193e-02      5587    129304\n",
      "5    1 -0.023076  1.090836e-17      1366    137699\n",
      "0    2  0.016272  1.883501e-18       794    290053\n",
      "2    3 -0.004632  4.756725e-02       182    182929\n",
      "4    4 -0.057261  7.931490e-79        62    107591\n",
      "6    5       NaN           NaN        16     49761\n",
      "1    6       NaN           NaN        13    141126\n",
      "7    7       NaN           NaN         1      7503\n",
      "****************************************\n",
      "*** Running correlation on  ../trained_models/model.modabs.mc_rae_real.1k.mu1_1.mu2_0.1.mu3_1e-07.mu4_10.nnk_4 model ***\n",
      "****************************************\n",
      "   bin   pearson     pearson_p  n_lemmas  n_tokens\n",
      "3    0 -0.004956  7.471817e-02      5587    129304\n",
      "5    1 -0.022819  2.481891e-17      1366    137699\n",
      "0    2  0.017037  4.465181e-20       794    290053\n",
      "2    3 -0.004741  4.259217e-02       182    182929\n",
      "4    4 -0.057841  2.142861e-80        62    107591\n",
      "6    5       NaN           NaN        16     49761\n",
      "1    6       NaN           NaN        13    141126\n",
      "7    7       NaN           NaN         1      7503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/scipy/stats/stats.py:4023: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for save_path in models:\n",
    "    print(\"****************************************\")\n",
    "    print(\"*** Running correlation on  %s model ***\" % save_path)\n",
    "    print(\"****************************************\")\n",
    "    \n",
    "    infile = '../results/semcor_pairwise_data_' + os.path.split(save_path)[1] + '.csv'\n",
    "    df = pd.read_csv(infile)\n",
    "    \n",
    "    df['wn_bin'] = pd.cut(df.n_senses, \n",
    "                        bins = [0, 1.1, 2.1, 4.1, 6.1, 8.1, 10.1, 20.1,  200], labels = False)\n",
    "    results = []\n",
    "    \n",
    "    for binn in df.wn_bin.unique():\n",
    "\n",
    "        wn_df = df[df.wn_bin == binn]\n",
    "        \n",
    "\n",
    "        pearson, pearson_p = pearsonr(wn_df.cos_sim, wn_df.wup_sim)\n",
    "\n",
    "        \n",
    "        res = {\n",
    "        'bin' : binn,\n",
    "        'pearson': pearson,\n",
    "        'pearson_p' : pearson_p,\n",
    "        'n_lemmas' : len(wn_df['lemma'].unique()),\n",
    "        'n_tokens' : len(wn_df)\n",
    "        }\n",
    "        #print(\"pearson: \", pearson)\n",
    "        #print(\"p-value: \", pearson_p)\n",
    "        #print(\"n lemmas: \", n_lemmas)\n",
    "        #print(\"n tokens: \", n_tokens)\n",
    "        \n",
    "        results.append(res)\n",
    "        \n",
    "    model_results = pd.DataFrame.from_records(results).sort_values(by='bin')\n",
    "    print(model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.         0.75       0.875      0.9        0.91666667 0.9375\n",
      " 0.83333333 0.85714286 0.81818182 0.88888889 0.92857143 0.84615385\n",
      " 0.92307692 0.90909091 0.78571429 0.76470588 0.86666667 0.8125\n",
      " 0.88235294]\n",
      "        Unnamed: 0 lemma token_sense_1 token_sense_2 sense1_pos sense2_pos  \\\n",
      "220389      220389   jew  jew.n.01.Jew  jew.n.01.Jew          n          n   \n",
      "220390      220390   jew  jew.n.01.Jew  jew.n.01.Jew          n          n   \n",
      "220391      220391   jew  jew.n.01.Jew  jew.n.01.Jew          n          n   \n",
      "220392      220392   jew  jew.n.01.Jew  jew.n.01.Jew          n          n   \n",
      "220393      220393   jew  jew.n.01.Jew  jew.n.01.Jew          n          n   \n",
      "\n",
      "         cos_sim  wup_sim  n_senses  n_word_forms  concreteness  wn_bin  \\\n",
      "220389  0.993850     0.75         1             2           NaN       0   \n",
      "220390  0.998647     0.75         1             2           NaN       0   \n",
      "220391  0.994076     0.75         1             2           NaN       0   \n",
      "220392  0.996887     0.75         1             2           NaN       0   \n",
      "220393  0.997168     0.75         1             2           NaN       0   \n",
      "\n",
      "        conc_bin  \n",
      "220389       NaN  \n",
      "220390       NaN  \n",
      "220391       NaN  \n",
      "220392       NaN  \n",
      "220393       NaN  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['jew', 'amateur', 'enzyme', 'compulsive', 'virus', 'follower',\n",
       "       'grownup', 'savannah', 'murderer', 'wakefulness', 'christ',\n",
       "       'napoleon', 'alabaster', 'explorer', 'expert', 'person', 'police',\n",
       "       'thomas huxley', 'choreography', 'christianity', 'lip', 'neighbor',\n",
       "       'chick', 'roleplaying', 'st. louis', 'negro', 'applicant', 'dog',\n",
       "       'dancing', 'carbon tetrachloride', 'gambling', 'assassin',\n",
       "       'individualist', 'bomber', 'red clay', 'alabama', 'coal',\n",
       "       'chicago', 'dexamethasone', 'religious belief', 'avocado',\n",
       "       'houston', 'hand blower', 'high explosives', 'inhabitant',\n",
       "       'scotland yard', 'peer', 'acting', 'berry', 'newspaper critic',\n",
       "       'detroit', 'megaton bombs', 'neutron bombs', 'advocate',\n",
       "       'swelling', 'dweller', 'fingerprint', 'religious beliefs',\n",
       "       'blonde', 'sovereign', 'intellectual', 'national', 'protein',\n",
       "       'vitamin e', 'convert', 'siamese cats', 'patron saints',\n",
       "       'face powder', 'registrant', 'planner', 'jesus christ', 'desert',\n",
       "       'someone', 'baltimore', 'native', 'jesus', 'lefty', 'bel canto',\n",
       "       'antagonist', 'engineer', 'common man', 'pear', 'bartlett pear',\n",
       "       'traveler', 'george washington', 'abraham lincoln', 'bodybuilder',\n",
       "       'killer', 'posse', 'sleeping capsule', 'bath towel', 'monarch',\n",
       "       'nucleic acid', 'atomic bombs', 'sparkle', 'bloodhound', 'newport',\n",
       "       'salt water', 'insulin', 'gunman', 'electric shocks', 'coward',\n",
       "       'megaton bomb', 'working girl', 'singing', 'puppy', 'dead person',\n",
       "       'kidnapper', 'warrior', 'sleeping pill', 'strychnine', 'hound dog',\n",
       "       'scientist', 'butter', 'virgin mary', 'multiple sclerosis',\n",
       "       'heat sink', 'root cellar', 'honolulu', 'acetone', 'police force',\n",
       "       'jowl', 'good person', 'sea water', 'two-by-four', 'germanium',\n",
       "       'hitler', 'alley cat', 'rest rooms', 'gelding', 'fox terrier',\n",
       "       'the virgin', 'inexperienced person', 'mary magdalene',\n",
       "       'tranquilizer', 'game birds', 'julius caesar', 'ground water',\n",
       "       'hurler', 'celebrant', 'modern', 'fortitude', 'decedent',\n",
       "       'heart rate', 'prednisone', 'abe lincoln', 'carbon dioxide',\n",
       "       'kickoff', 'good guys', 'bad guys', 'grape', 'atomic bomb',\n",
       "       'atom bomb', 'distilled water'], dtype=object)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how can we have real correlations for bin 0?\n",
    "\n",
    "test = df[df['wn_bin'] ==0]\n",
    "print(test.wup_sim.unique())\n",
    "\n",
    "print(test[test.wup_sim != 1].head(5))\n",
    "\n",
    "\n",
    "test[test.wup_sim != 1].lemma.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9m/vzvx58rs51v_x5nm620fz4xr0000gn/T/ipykernel_88747/1891269078.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlemma_from_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'christianity.n.01.christianity'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Box Sync/src/features_in_context/notebooks/../lib/utils.py\u001b[0m in \u001b[0;36mlemma_from_string\u001b[0;34m(lemma_string)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;31m# grabs everything in between (' ') in a string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# (needed to update from r\"'(.*?)'\" to deal with cases with quotes in word like o'clock)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0mstring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"\\('(.*?)'\\)\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlemma_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m     \u001b[0;31m#print(string)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mlemma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "lemma_from_string('christianity.n.01.christianity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lemma('christianity.n.01.Christianity')"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('christianity')[0].lemmas()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn = wn.synsets('Christianity')[0].lemmas()[0].synset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8181818181818182"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn.wup_similarity(syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this means sometimes a sense's wup sim with itself is less than 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>lemma</th>\n",
       "      <th>token_sense_1</th>\n",
       "      <th>token_sense_2</th>\n",
       "      <th>sense1_pos</th>\n",
       "      <th>sense2_pos</th>\n",
       "      <th>cos_sim</th>\n",
       "      <th>wup_sim</th>\n",
       "      <th>n_senses</th>\n",
       "      <th>n_word_forms</th>\n",
       "      <th>concreteness</th>\n",
       "      <th>wn_bin</th>\n",
       "      <th>conc_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>149641</th>\n",
       "      <td>149641</td>\n",
       "      <td>line</td>\n",
       "      <td>cable.n.02.line</td>\n",
       "      <td>cable.n.02.line</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>0.490191</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>7</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149642</th>\n",
       "      <td>149642</td>\n",
       "      <td>line</td>\n",
       "      <td>cable.n.02.line</td>\n",
       "      <td>cable.n.02.line</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>0.745497</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>7</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149643</th>\n",
       "      <td>149643</td>\n",
       "      <td>line</td>\n",
       "      <td>cable.n.02.line</td>\n",
       "      <td>cable.n.02.line</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>0.628056</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>7</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149644</th>\n",
       "      <td>149644</td>\n",
       "      <td>line</td>\n",
       "      <td>cable.n.02.line</td>\n",
       "      <td>cable.n.02.line</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>0.709143</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>7</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149645</th>\n",
       "      <td>149645</td>\n",
       "      <td>line</td>\n",
       "      <td>cable.n.02.line</td>\n",
       "      <td>cable.n.02.line</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>0.684494</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>7</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157139</th>\n",
       "      <td>157139</td>\n",
       "      <td>line</td>\n",
       "      <td>telephone_line.n.02.line</td>\n",
       "      <td>channel.n.05.line</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>0.287194</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>7</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157140</th>\n",
       "      <td>157140</td>\n",
       "      <td>line</td>\n",
       "      <td>telephone_line.n.02.line</td>\n",
       "      <td>line.n.22.line</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>0.305778</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>7</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157141</th>\n",
       "      <td>157141</td>\n",
       "      <td>line</td>\n",
       "      <td>wrinkle.n.01.line</td>\n",
       "      <td>channel.n.05.line</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>0.319876</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>7</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157142</th>\n",
       "      <td>157142</td>\n",
       "      <td>line</td>\n",
       "      <td>wrinkle.n.01.line</td>\n",
       "      <td>line.n.22.line</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>0.245727</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>7</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157143</th>\n",
       "      <td>157143</td>\n",
       "      <td>line</td>\n",
       "      <td>channel.n.05.line</td>\n",
       "      <td>line.n.22.line</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>0.404815</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>7</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7503 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0 lemma             token_sense_1      token_sense_2  \\\n",
       "149641      149641  line           cable.n.02.line    cable.n.02.line   \n",
       "149642      149642  line           cable.n.02.line    cable.n.02.line   \n",
       "149643      149643  line           cable.n.02.line    cable.n.02.line   \n",
       "149644      149644  line           cable.n.02.line    cable.n.02.line   \n",
       "149645      149645  line           cable.n.02.line    cable.n.02.line   \n",
       "...            ...   ...                       ...                ...   \n",
       "157139      157139  line  telephone_line.n.02.line  channel.n.05.line   \n",
       "157140      157140  line  telephone_line.n.02.line     line.n.22.line   \n",
       "157141      157141  line         wrinkle.n.01.line  channel.n.05.line   \n",
       "157142      157142  line         wrinkle.n.01.line     line.n.22.line   \n",
       "157143      157143  line         channel.n.05.line     line.n.22.line   \n",
       "\n",
       "       sense1_pos sense2_pos   cos_sim   wup_sim  n_senses  n_word_forms  \\\n",
       "149641          n          n  0.490191  1.000000        22             2   \n",
       "149642          n          n  0.745497  1.000000        22             2   \n",
       "149643          n          n  0.628056  1.000000        22             2   \n",
       "149644          n          n  0.709143  1.000000        22             2   \n",
       "149645          n          n  0.684494  1.000000        22             2   \n",
       "...           ...        ...       ...       ...       ...           ...   \n",
       "157139          n          n  0.287194  0.133333        22             2   \n",
       "157140          n          n  0.305778  0.625000        22             2   \n",
       "157141          n          n  0.319876  0.266667        22             2   \n",
       "157142          n          n  0.245727  0.125000        22             2   \n",
       "157143          n          n  0.404815  0.133333        22             2   \n",
       "\n",
       "        concreteness  wn_bin  conc_bin  \n",
       "149641           4.5       7       3.0  \n",
       "149642           4.5       7       3.0  \n",
       "149643           4.5       7       3.0  \n",
       "149644           4.5       7       3.0  \n",
       "149645           4.5       7       3.0  \n",
       "...              ...     ...       ...  \n",
       "157139           4.5       7       3.0  \n",
       "157140           4.5       7       3.0  \n",
       "157141           4.5       7       3.0  \n",
       "157142           4.5       7       3.0  \n",
       "157143           4.5       7       3.0  \n",
       "\n",
       "[7503 rows x 13 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "lets check out this super polysemous word\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_csv('../results/debug_semcor_pairwise_data_model.plsr.buchanan.allbuthomonyms.5k.300components.500max_iters.csv')\n",
    "\n",
    "\n",
    "df['wn_bin'] = pd.cut(df.n_senses, \n",
    "                        bins = [0, 1.1, 2.1, 4.1, 6.1, 8.1, 10.1, 20.1,  200], labels = False)\n",
    "\n",
    "line = df[df['n_senses'] > 20]\n",
    "line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#dfSample = df.sample(1000, weights='wup_sim') # This is the importante line\n",
    "#xdataSample, ydataSample = dfSample[\"cos_sim\"], dfSample[\"wup_sim\"]\n",
    "\n",
    "lineminus1 = line[line['wup_sim'] != 1]\n",
    "\n",
    "xdataSample, ydataSample = lineminus1[\"cos_sim\"], lineminus1[\"wup_sim\"]\n",
    "\n",
    "sns.regplot(x=ydataSample, y=xdataSample) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['structure', 'need', 'washington', 'idea', 'growth', 'amount',\n",
       "       'earth', 'pain', 'box', 'hour', 'range', 'motif', 'game',\n",
       "       'material', 'water', 'example', 'rank', 'second', 'product',\n",
       "       'share', 'fork', 'film', 'start', 'officer', 'distribution',\n",
       "       'mess', 'help', 'provision', 'administration', 'bloom', 'fly',\n",
       "       'replacement', 'spread', 'conversion', 'hearing', 'trouble',\n",
       "       'contribution', 'bill', 'affair', 'question', 'match',\n",
       "       'production', 'function', 'community', 'literature', 'hope',\n",
       "       'bolt', 'boy', 'living', 'target', 'eye', 'circumstance', 'brain',\n",
       "       'nerve', 'office', 'lesson', 'church', 'gain', 'sphere', 'roll',\n",
       "       'capital', 'room', 'trace', 'post', 'program', 'reception',\n",
       "       'memory', 'wire', 'report', 'watch', 'nature', 'balance', 'item',\n",
       "       'truth', 'interference', 'floor', 'leadership', 'prayer', 'faith',\n",
       "       'act', 'energy', 'quality', 'card', 'limit', 'exercise', 'fact',\n",
       "       'politics', 'distance', 'influence', 'news', 'skin', 'difficulty',\n",
       "       'treatment', 'style', 'residence', 'south', 'discipline', 'role',\n",
       "       'possibility', 'market', 'weight', 'court', 'effect', 'dream',\n",
       "       'conflict', 'element', 'interpretation', 'argument', 'address',\n",
       "       'focus', 'decision', 'manner', 'chain', 'expedition', 'rest',\n",
       "       'formula', 'crack', 'wing', 'difference', 'book', 'grace',\n",
       "       'height', 'population', 'object', 'identity', 'consideration',\n",
       "       'stop', 'trust', 'painting', 'tension', 'variation', 'reason',\n",
       "       'scale', 'reservation', 'guard', 'peace', 'reach', 'contrast',\n",
       "       'portion', 'instruction', 'economy', 'success', 'organization',\n",
       "       'speech', 'color', 'grade', 'origin', 'north', 'mechanic', 'soul',\n",
       "       'understanding', 'conception', 'solution', 'estimate', 'blood',\n",
       "       'agreement', 'wisdom', 'institution', 'comfort', 'employment',\n",
       "       'crossroad', 'mission', 'factor', 'distinction', 'bank', 'frame',\n",
       "       'practice', 'gray', 'condition', 'label', 'abstraction', 'pot',\n",
       "       'occasion', 'race', 'twist', 'violation', 'strip', 'rule',\n",
       "       'sequence', 'suffering', 'passion', 'appreciation', 'canvas',\n",
       "       'couple', 'beam', 'tail', 'agent', 'exposure', 'peak', 'stream',\n",
       "       'culture', 'track', 'port', 'witness', 'middle', 'illustration',\n",
       "       'collection', 'taste', 'association', 'cap', 'drama', 'crash',\n",
       "       'receiver', 'print', 'reflection', 'quarter', 'first', 'shade',\n",
       "       'separation', 'excitement', 'counter', 'complement', 'allowance',\n",
       "       'bow', 'inventory', 'bench', 'regard', 'denial', 'talk', 'reserve',\n",
       "       'attraction', 'acceptance', 'medium', 'runner', 'discharge',\n",
       "       'pull', 'slice', 'stamp', 'disturbance', 'introduction'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foursenses = df[df['n_senses'] == 4]['lemma'].unique()\n",
    "foursenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "discip = df[df['lemma'] == 'crash']\n",
    "\n",
    "\n",
    "xdataSample, ydataSample = discip[\"cos_sim\"], discip[\"wup_sim\"]\n",
    "\n",
    "sns.regplot(x=ydataSample, y=xdataSample) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>lemma</th>\n",
       "      <th>token_sense_1</th>\n",
       "      <th>token_sense_2</th>\n",
       "      <th>sense1_pos</th>\n",
       "      <th>sense2_pos</th>\n",
       "      <th>cos_sim</th>\n",
       "      <th>wup_sim</th>\n",
       "      <th>n_senses</th>\n",
       "      <th>n_word_forms</th>\n",
       "      <th>concreteness</th>\n",
       "      <th>wn_bin</th>\n",
       "      <th>conc_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>communication</td>\n",
       "      <td>communication.n.01.communication</td>\n",
       "      <td>communication.n.01.communication</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>0.828119</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>communication</td>\n",
       "      <td>communication.n.01.communication</td>\n",
       "      <td>communication.n.01.communication</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>0.708595</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>communication</td>\n",
       "      <td>communication.n.01.communication</td>\n",
       "      <td>communication.n.01.communication</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>0.798475</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>communication</td>\n",
       "      <td>communication.n.01.communication</td>\n",
       "      <td>communication.n.01.communication</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>0.726443</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>communication</td>\n",
       "      <td>communication.n.01.communication</td>\n",
       "      <td>communication.n.01.communication</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>0.862673</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0          lemma                     token_sense_1  \\\n",
       "0           0  communication  communication.n.01.communication   \n",
       "1           1  communication  communication.n.01.communication   \n",
       "2           2  communication  communication.n.01.communication   \n",
       "3           3  communication  communication.n.01.communication   \n",
       "4           4  communication  communication.n.01.communication   \n",
       "\n",
       "                      token_sense_2 sense1_pos sense2_pos   cos_sim  wup_sim  \\\n",
       "0  communication.n.01.communication          n          n  0.828119      1.0   \n",
       "1  communication.n.01.communication          n          n  0.708595      1.0   \n",
       "2  communication.n.01.communication          n          n  0.798475      1.0   \n",
       "3  communication.n.01.communication          n          n  0.726443      1.0   \n",
       "4  communication.n.01.communication          n          n  0.862673      1.0   \n",
       "\n",
       "   n_senses  n_word_forms  concreteness  wn_bin  conc_bin  \n",
       "0         3             2           2.9       2       2.0  \n",
       "1         3             2           2.9       2       2.0  \n",
       "2         3             2           2.9       2       2.0  \n",
       "3         3             2           2.9       2       2.0  \n",
       "4         3             2           2.9       2       2.0  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
