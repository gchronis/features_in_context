{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Context evaluation of the best performing models.\n",
    "\n",
    "We now wish to evaluate each of our projection models at the token level. We construct an evaluation dataset from a set of homonyms from the McRae feature set. The dataset consists of tokens of homonymous words collected from semcor, alongside gold feature data (mcrae) for disambiguated senses.\n",
    "\n",
    "We have gold features in mcrae and buchanan format, so we only use these datasets, not binder.\n",
    "\n",
    "\n",
    "We have trained models with best performing hyperparameters on the other words in the datasets.\n",
    "\n",
    "4 layer ffnn (1k)\n",
    "4 layer ffnn (5k)\n",
    "4 layer ffnn (glove)\n",
    "plsr (1k)\n",
    "plsr (5k)\n",
    "plsr(glove)\n",
    "label propagation (1k)\n",
    "label propagation (5k)\n",
    "label propagation (glove)\n",
    "\n",
    "To evaluate a model, we use it to predict features in context for each token in the dataset. Then, we compare to the gold feature data. We also calculate the average cosine distance between gold and predicted vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.modeling:loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /Users/gabriellachronis/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "INFO:pytorch_pretrained_bert.modeling:extracting archive file /Users/gabriellachronis/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /var/folders/9m/vzvx58rs51v_x5nm620fz4xr0000gn/T/tmp_8_eamj0\n",
      "INFO:pytorch_pretrained_bert.modeling:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/gabriellachronis/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import torch\n",
    "from bert import *\n",
    "from feature_data import *\n",
    "from multiprototype import *\n",
    "from models import *\n",
    "from utils import *\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus.reader.wordnet import Lemma\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "bert = BERTBase()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to evaluate the model on its ability to predict features in context. First, we obtain gold vectors for both senses of 10 ambiguous words from the McRae et al. data\n",
    "\n",
    "What are the words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct the Evaluation Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluate_in_context' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9m/vzvx58rs51v_x5nm620fz4xr0000gn/T/ipykernel_33368/1050176146.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mevaluate_in_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_norms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'evaluate_in_context' is not defined"
     ]
    }
   ],
   "source": [
    "save_path = '../trained_models/model.plsr.buchanan.allbuthomonyms.5k.300components.500max_iters'\n",
    "model = torch.load(save_path)\n",
    "\n",
    "df = pd.read_csv('../data/processed/bnc_contexts_for_mcrae_homonyms.csv')\n",
    "feature_norms = BuchananFeatureNorms('../data/external/buchanan/cue_feature_words.csv')\n",
    "\n",
    "\n",
    "\n",
    "evaluate_in_context(model, df, bert, feature_norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambiguous_pairs = [\n",
    "    ('bat_animal', 'bat_baseball'),\n",
    "    ('board_wood', 'board_black'),\n",
    "    ('bow_ribbon', 'bow_weapon'),\n",
    "    ('cap_bottle', 'cap_hat'),\n",
    "    #('crane_machine', 'crane_animal')\n",
    "    ('hose', 'hose_leggings'),\n",
    "    ('mink', 'mink_coat'), # this one is not fully disambiguated\n",
    "    ('mouse', 'mouse_computer'),\n",
    "    ('pipe_smoking', 'pipe_plumbing'),\n",
    "    ('tank_army', 'tank_container')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bat_animal',\n",
       " 'bat_baseball',\n",
       " 'board_wood',\n",
       " 'board_black',\n",
       " 'bow_ribbon',\n",
       " 'bow_weapon',\n",
       " 'cap_bottle',\n",
       " 'cap_hat',\n",
       " 'hose',\n",
       " 'hose_leggings',\n",
       " 'mink',\n",
       " 'mink_coat',\n",
       " 'mouse',\n",
       " 'mouse_computer',\n",
       " 'pipe_smoking',\n",
       " 'pipe_plumbing',\n",
       " 'tank_army',\n",
       " 'tank_container']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[item for t in ambiguous_pairs for item in t]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features for bat: {'animal': 6.779661017, 'cricket': 7.344632768, 'fly': 7.90960452, 'hit': 7.344632768, 'wood': 9.039548023}\n",
      "features for bat_baseball: {'ball': 60.0, 'heave': 16.66666667, 'hit': 96.66666667, 'long': 50.0, 'metal': 53.33333333, 'sport': 23.33333333, 'swing': 20.0, 'wood': 73.33333333}\n",
      "features for bat_animal: {'animal': 33.33333333, 'black': 53.33333333, 'blind': 33.33333333, 'cave': 46.66666667, 'down': 33.33333333, 'fang': 23.33333333, 'fly': 63.33333333, 'fur': 36.66666667, 'mammal': 23.33333333, 'navigate': 30.0, 'nocturnal': 63.33333333, 'radar': 30.0, 'scare': 20.0, 'screech': 26.66666667, 'sleep': 33.33333333, 'small': 20.0, 'vampire': 16.66666667, 'wing': 86.66666667}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "take a look at the gold features for these words.\n",
    "\"\"\"\n",
    "norms = BuchananFeatureNorms('../data/external/buchanan/cue_feature_words.csv')\n",
    "\n",
    "\n",
    "norms.print_features('bat')\n",
    "norms.print_features('bat_baseball')\n",
    "norms.print_features('bat_animal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to find examples of each sense and make predictions using the model for that word. We will score the predictions of each example and then average them. Here are 10 examples of bat_animal. Load these from the file we have created with these words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Models\n",
    "\n",
    "Hopefully this is done already "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "models and save paths\n",
    "\n",
    "\"\"\"\n",
    "buchanan_models = [\n",
    "    '../trained_models/model.plsr.buchanan.allbuthomonyms.5k.300components.500max_iters',\n",
    "    '../trained_models/model.plsr.buchanan.allbuthomonyms.1k.300components.500max_iters',\n",
    "    '../trained_models/model.ffnn.buchanan.allbuthomonyms.5k.50epochs.0.5dropout.lr1e-4.hsize300',\n",
    "    '../trained_models/model.ffnn.buchanan.allbuthomonyms.1k.50epochs.0.5dropout.lr1e-4.hsize300',\n",
    "    #'../trained_models/model.modabs.buchanan.allbuthomoyms.5k',\n",
    "    #'../trained_models/model.modabs.buchanan.allbuthomoyms.1k',\n",
    "]\n",
    "\n",
    "buchanan_glove_models = [\n",
    "    '../trained_models/model.plsr.buchanan.allbuthomonyms.glove.300components.300max_iters',\n",
    "    #'../trained_models/model.ffnn.buchanan.allbuthomonyms.glove.50epochs.0.5dropout.lr1e-4.hsize300',\n",
    "    #'../trained_models/model.modabs.buchanan.allbuthomoyms.glove'   \n",
    "]\n",
    "\n",
    "mcrae_models = [\n",
    "    '../trained_models/model.ffnn.mc_rae_real.allbuthomonyms.5k.50epochs.0.5dropout.lr1e-4.hsize300',\n",
    "    '../trained_models/model.ffnn.mc_rae_real.allbuthomonyms.1k.50epochs.0.5dropout.lr1e-4.hsize300',\n",
    "    '../trained_models/model.plsr.mc_rae_real.allbuthomonyms.5k.100components.500max_iters',\n",
    "    '../trained_models/model.plsr.mc_rae_real.allbuthomonyms.1k.50components.500max_iters',\n",
    "    '../trained_models/model.modabs.mc_rae_real.5k.mu1_1.mu2_0.1.mu3_0.001.mu4_5.nnk_4',\n",
    "    '../trained_models/model.modabs.mc_rae_real.1k.mu1_1.mu2_0.1.mu3_1e-07.mu4_10.nnk_4',\n",
    "]\n",
    "\n",
    "mcrae_glove_models = [\n",
    "    '../trained_models/model.ffnn.mc_rae_real.allbuthomonyms.glove.50epochs.0.5dropout.lr1e-4.hsize300',\n",
    "    '../trained_models/model.modabs.mc_rae_real.glove.mu1_1.mu2_0.1.mu3_1e-07.mu4_5.nnk_5',\n",
    "    #'../trained_models/model.modabs.mc_rae_real.glove.mu1_1.mu2_0.1.mu3_1e-07.mu4_5.nnk_5',\n",
    "]\n",
    "\n",
    "binder_models = [\n",
    "    '../trained_models/model.ffnn.binder.5k.50epochs.0.5dropout.lr1e-4.hsize300',\n",
    "    '../trained_models/model.ffnn.binder.1k.50epochs.0.5dropout.lr1e-4.hsize300',\n",
    "    '../trained_models/model.plsr.binder.5k.300components.500max_iters',\n",
    "    '../trained_models/model.plsr.binder.1k.300components.500max_iters',\n",
    "    '../trained_models/model.modabs.binder.5k.mu1_1.mu2_0.1.mu3_0.001.mu4_5.nnk_4',\n",
    "    '../trained_models/model.modabs.binder.1k.mu1_1.mu2_0.1.mu3_0.001.mu4_5.nnk_4',\n",
    "]\n",
    "\n",
    "binder_glove_models = [\n",
    "    '../trained_models/model.ffnn.binder.glove.50epochs.0.5dropout.lr1e-4.hsize300',\n",
    "    '../trained_models/model.plsr.binder.glove.300components.500max_iters',\n",
    "    '../trained_models/model.modabs.binder.glove.mu1_1.mu2_0.1.mu3_0.001.mu4_5.nnk_4',    \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the models on the data\n",
    "\n",
    "TODO are you sure this is the exact dataset you want? Per Katrins comment:\n",
    "\n",
    "Another small question on the McRae homonym analysis: If there was a word labeled with sense cap.n.02 but it's not \"cap\" but another word, say \"lid\", do you also use that in this analysis, or only the correct lemma?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>word</th>\n",
       "      <th>source</th>\n",
       "      <th>sentence</th>\n",
       "      <th>buchanan_cue_word</th>\n",
       "      <th>mcrae_cue_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>bat</td>\n",
       "      <td>COCA:2010:TV\\nGood Luck Charlie</td>\n",
       "      <td>! - Oh, real mature! - I am more mature than you are! Whatever! Oh! Okay! Okay! All right, drop the bat Or baby booboo becomes baby barbecue. You would n't dare! Try me. - Give me the baby. - Give me the bat. - The baby. - The bat. P.J.! Pull me up! Pull me up! Oh, wait! Are you quitting? - What? - You know, like, If I pull you back in, will you keep doing the sit-in? Since my alternative is</td>\n",
       "      <td>bat_baseball</td>\n",
       "      <td>bat_(baseball)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>bat</td>\n",
       "      <td>COCA:2014:FIC\\nBk:VengeanceIsMine</td>\n",
       "      <td>Best's casino in the wide-open gambling town, cutting in on the Family's profits. # Enrique Sandoval's house was silent. Anthony reached the back door and found it was locked. He expected that, but it never hurt to try the simple way first. He carefully placed the bat on the Mexican tile, and unzipped his coveralls enough to slip the.45 intothe shoulder holster. That done, he dug out the set of picks he'd learned to use as a kid. The cheap General lock surrendered a moment later. The door opened into a dark, spacious</td>\n",
       "      <td>bat_baseball</td>\n",
       "      <td>bat_(baseball)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>bat</td>\n",
       "      <td>COCA:2002:MAG\\nPeople</td>\n",
       "      <td>Alex looking on. In a taped confession Derek maintained that King had pushed Alex around that night, causing the boy to cry. They said they were tired of being abused and wanted to live with Chavis. They also provided a sickeningly detailed account of the assault. The sound of the bat on his father's head, said Alex, was like '' wood cracking,or hitting concrete or something. \" Earlier this year, however, the boys suddenly recanted their confession. They maintained that Chavis had induced them to say they killed King, assuring them they could claim selfdefense.</td>\n",
       "      <td>bat_baseball</td>\n",
       "      <td>bat_(baseball)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>bat</td>\n",
       "      <td>COCA:2015:NEWS\\nUSAToday</td>\n",
       "      <td>happened. \" And sometimes it's better just to be quiet. Three years ago, in Kerry Wood's final season with the Cubs, he and Samardzija bet each other who could hit a baseball 100 feet away into a trash can. '' So I missed, tried to snap the bat over my leg, '' Samardzija said, '' and it didn't break.The middle of my thigh was all black and blue. I had a contusion. But I was smart enough not to tell anybody about it. It was only the second day of spring training, so I</td>\n",
       "      <td>bat_baseball</td>\n",
       "      <td>bat_(baseball)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>bat</td>\n",
       "      <td>COCA:2019:NEWS\\nDallas Morning News</td>\n",
       "      <td>back half that he could n't catch up to or hit the ball on the barrel with any consistency. Odor did change his bat style to a \" cupped \" model, which means the end of the bat has been hollowed out into a cupped form. # The thinking behind the cupped bat is that it creates more balance in the wood and allows manufacturers to use aharder/denser piece of lumber. Also, there is a slight weight drop. Maybe a quarter of an ounce. Maybe a half ounce. Pretty negligible. Or maybe not. Odor did not credit the change in</td>\n",
       "      <td>bat_baseball</td>\n",
       "      <td>bat_(baseball)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949</th>\n",
       "      <td>30</td>\n",
       "      <td>mink</td>\n",
       "      <td>COCA:2009:FIC\\nAntiochRev</td>\n",
       "      <td>blows up. Enid wraps her fur stole tightly around her shoulders. \" Nice, \" Bonita says, stroking it. \" That real fur? \" It looked just like the ratty, old piece her friend Elnora used to wear to church which everyone knew was monkey trying to look like mink. '' I told you. I hate artificial. '' Which, Bonita notes, does n't quite answer her question. Edmund is still struggling with the lid. \" Hurry up, Ed, \" Enid says. \" Before this wind gets any stronger. \" \" What we need is</td>\n",
       "      <td>mink_coat</td>\n",
       "      <td>mink_(coat)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>31</td>\n",
       "      <td>mink</td>\n",
       "      <td>COCA:1996:NEWS\\nDenver</td>\n",
       "      <td>$ 5.5 million to make this project work. # The goal: Weakening Colorado's environ-mental standards. # To do this, the Army has been raising minks in Minnesota and feeding them a chemical polluting the Rocky Mountain Arsenal in Colorado. Since 1991, Army officials have spent $ 8,100 per mink - or 85 times more than any one pelt is worth - to conduct thispollution experiment. # If the Minnesota minks eat enough **28;576;TOOLONG, or DIMP, without developing health problems, the Army plans to ask Colorado regulators to ease detoxification standards for the arsenal's $ 2 billion Superfund cleanup</td>\n",
       "      <td>mink_coat</td>\n",
       "      <td>mink_(coat)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951</th>\n",
       "      <td>32</td>\n",
       "      <td>mink</td>\n",
       "      <td>COCA:1999:MAG\\nTownCountry</td>\n",
       "      <td>s cashmere turtleneck from Celine bv Michael Kors (S1,475) is just the thing to keep out the autumn chill, especially when paired with St. John Sport's brown leather jacket with Mongolian-lamb collar ($650). Opposite, right: Jasmine brings Henrietta Street right up to date in her sueded mink coat ($13,000), cashmere turtleneck ($1,475), luggage-leather jeans ($1,475) and ankle boots ($725), all from Celine by Michael Kors. Cashmere logo bag with floral embroidery by Fendi ($1,150). This page: Jasmine ascends the entrance-hall staircase with grace to spare in</td>\n",
       "      <td>mink_coat</td>\n",
       "      <td>mink_(coat)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>33</td>\n",
       "      <td>mink</td>\n",
       "      <td>COCA:1990:NEWS\\nWashPost</td>\n",
       "      <td>, who teaches business aspects of interior design at the Parsons School of Design in New York. \" People who buy into it quickly fail because they have n't gone through the process of acquiring taste. I tell my students,' If a client doesn't know the difference between skunk and mink, don't buy them mink.' '' # During that decade people whocatered to the rich and famous -- their hairdressers, physical trainers, interior designers -- were also media celebrities. Mario Buatta reveled in the sobriquet Prince of Chintz. \" There is a symbiotic relationship, \" remarked</td>\n",
       "      <td>mink_coat</td>\n",
       "      <td>mink_(coat)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>34</td>\n",
       "      <td>mink</td>\n",
       "      <td>COCA:2006:NEWS\\nWashPost</td>\n",
       "      <td>Stanley R. Zupnik, an area real estate broker. # \" She did n't just have bravado, \" says Austin Kiplinger, a friend and publisher who lived on Willard Avenue in the 1940s. \" She did her homework. '' # He laughs. '' How she got herself a red mink coat I don't know. I guess you dip it in red ink.\" # Once, hoping to lure in a buyer who had previously snubbed her, Edwards hired a chauffeur to pick him up from the airport in the new Rolls. \" Same woman, same brain, nice</td>\n",
       "      <td>mink_coat</td>\n",
       "      <td>mink_(coat)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>954 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  word                               source  \\\n",
       "0     1   bat      COCA:2010:TV\\nGood Luck Charlie   \n",
       "1     2   bat    COCA:2014:FIC\\nBk:VengeanceIsMine   \n",
       "2     3   bat                COCA:2002:MAG\\nPeople   \n",
       "3     4   bat             COCA:2015:NEWS\\nUSAToday   \n",
       "4     5   bat  COCA:2019:NEWS\\nDallas Morning News   \n",
       "..   ..   ...                                  ...   \n",
       "949  30  mink            COCA:2009:FIC\\nAntiochRev   \n",
       "950  31  mink               COCA:1996:NEWS\\nDenver   \n",
       "951  32  mink           COCA:1999:MAG\\nTownCountry   \n",
       "952  33  mink             COCA:1990:NEWS\\nWashPost   \n",
       "953  34  mink             COCA:2006:NEWS\\nWashPost   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   sentence  \\\n",
       "0                                                                                                                                                                                                                ! - Oh, real mature! - I am more mature than you are! Whatever! Oh! Okay! Okay! All right, drop the bat Or baby booboo becomes baby barbecue. You would n't dare! Try me. - Give me the baby. - Give me the bat. - The baby. - The bat. P.J.! Pull me up! Pull me up! Oh, wait! Are you quitting? - What? - You know, like, If I pull you back in, will you keep doing the sit-in? Since my alternative is   \n",
       "1                                                                                Best's casino in the wide-open gambling town, cutting in on the Family's profits. # Enrique Sandoval's house was silent. Anthony reached the back door and found it was locked. He expected that, but it never hurt to try the simple way first. He carefully placed the bat on the Mexican tile, and unzipped his coveralls enough to slip the.45 intothe shoulder holster. That done, he dug out the set of picks he'd learned to use as a kid. The cheap General lock surrendered a moment later. The door opened into a dark, spacious   \n",
       "2                                   Alex looking on. In a taped confession Derek maintained that King had pushed Alex around that night, causing the boy to cry. They said they were tired of being abused and wanted to live with Chavis. They also provided a sickeningly detailed account of the assault. The sound of the bat on his father's head, said Alex, was like '' wood cracking,or hitting concrete or something. \" Earlier this year, however, the boys suddenly recanted their confession. They maintained that Chavis had induced them to say they killed King, assuring them they could claim selfdefense.   \n",
       "3                                                                                                                                   happened. \" And sometimes it's better just to be quiet. Three years ago, in Kerry Wood's final season with the Cubs, he and Samardzija bet each other who could hit a baseball 100 feet away into a trash can. '' So I missed, tried to snap the bat over my leg, '' Samardzija said, '' and it didn't break.The middle of my thigh was all black and blue. I had a contusion. But I was smart enough not to tell anybody about it. It was only the second day of spring training, so I   \n",
       "4                                                                                       back half that he could n't catch up to or hit the ball on the barrel with any consistency. Odor did change his bat style to a \" cupped \" model, which means the end of the bat has been hollowed out into a cupped form. # The thinking behind the cupped bat is that it creates more balance in the wood and allows manufacturers to use aharder/denser piece of lumber. Also, there is a slight weight drop. Maybe a quarter of an ounce. Maybe a half ounce. Pretty negligible. Or maybe not. Odor did not credit the change in   \n",
       "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      ...   \n",
       "949                                                                                                                        blows up. Enid wraps her fur stole tightly around her shoulders. \" Nice, \" Bonita says, stroking it. \" That real fur? \" It looked just like the ratty, old piece her friend Elnora used to wear to church which everyone knew was monkey trying to look like mink. '' I told you. I hate artificial. '' Which, Bonita notes, does n't quite answer her question. Edmund is still struggling with the lid. \" Hurry up, Ed, \" Enid says. \" Before this wind gets any stronger. \" \" What we need is   \n",
       "950  $ 5.5 million to make this project work. # The goal: Weakening Colorado's environ-mental standards. # To do this, the Army has been raising minks in Minnesota and feeding them a chemical polluting the Rocky Mountain Arsenal in Colorado. Since 1991, Army officials have spent $ 8,100 per mink - or 85 times more than any one pelt is worth - to conduct thispollution experiment. # If the Minnesota minks eat enough **28;576;TOOLONG, or DIMP, without developing health problems, the Army plans to ask Colorado regulators to ease detoxification standards for the arsenal's $ 2 billion Superfund cleanup   \n",
       "951                                    s cashmere turtleneck from Celine bv Michael Kors (S1,475) is just the thing to keep out the autumn chill, especially when paired with St. John Sport's brown leather jacket with Mongolian-lamb collar ($650). Opposite, right: Jasmine brings Henrietta Street right up to date in her sueded mink coat ($13,000), cashmere turtleneck ($1,475), luggage-leather jeans ($1,475) and ankle boots ($725), all from Celine by Michael Kors. Cashmere logo bag with floral embroidery by Fendi ($1,150). This page: Jasmine ascends the entrance-hall staircase with grace to spare in   \n",
       "952                                , who teaches business aspects of interior design at the Parsons School of Design in New York. \" People who buy into it quickly fail because they have n't gone through the process of acquiring taste. I tell my students,' If a client doesn't know the difference between skunk and mink, don't buy them mink.' '' # During that decade people whocatered to the rich and famous -- their hairdressers, physical trainers, interior designers -- were also media celebrities. Mario Buatta reveled in the sobriquet Prince of Chintz. \" There is a symbiotic relationship, \" remarked   \n",
       "953                                                                                                                                   Stanley R. Zupnik, an area real estate broker. # \" She did n't just have bravado, \" says Austin Kiplinger, a friend and publisher who lived on Willard Avenue in the 1940s. \" She did her homework. '' # He laughs. '' How she got herself a red mink coat I don't know. I guess you dip it in red ink.\" # Once, hoping to lure in a buyer who had previously snubbed her, Edwards hired a chauffeur to pick him up from the airport in the new Rolls. \" Same woman, same brain, nice   \n",
       "\n",
       "    buchanan_cue_word  mcrae_cue_word  \n",
       "0        bat_baseball  bat_(baseball)  \n",
       "1        bat_baseball  bat_(baseball)  \n",
       "2        bat_baseball  bat_(baseball)  \n",
       "3        bat_baseball  bat_(baseball)  \n",
       "4        bat_baseball  bat_(baseball)  \n",
       "..                ...             ...  \n",
       "949         mink_coat     mink_(coat)  \n",
       "950         mink_coat     mink_(coat)  \n",
       "951         mink_coat     mink_(coat)  \n",
       "952         mink_coat     mink_(coat)  \n",
       "953         mink_coat     mink_(coat)  \n",
       "\n",
       "[954 rows x 6 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "eval_words = pd.read_csv('../data/processed/mcrae_homonyms_from_coca_10_20_2022.csv')\n",
    "eval_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    " eval_words = eval_words.groupby('buchanan_cue_word').apply(lambda x: x.sample(10)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "data is a list of tuples of (cue word, lemma, context)\n",
    "\n",
    "for each model, we want to run an analysis of these\n",
    "\"\"\"\n",
    "\n",
    "    \n",
    "def evaluate_in_context(model, df, bert, glove=False, model_name=None, dataset='mc_rae'):\n",
    "    \"\"\"\n",
    "    df has columns\n",
    "    sentence\n",
    "    buchanan_cue_word\n",
    "    mcrae_cue_word\n",
    "    \"\"\"\n",
    "    n = 0\n",
    "    \n",
    "    cosines = []\n",
    "    top_10_precs = []\n",
    "    top_20_precs = []\n",
    "    top_k_precs = []\n",
    "    correlations = []\n",
    "    rows = []\n",
    "\n",
    "    num_top_10 = 0\n",
    "    num_top_20 = 0\n",
    "    num_total = 0\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if dataset == 'buchanan':\n",
    "            cue_word = row.buchanan_cue_word  \n",
    "        else:\n",
    "            cue_word = row.mcrae_cue_word  \n",
    "\n",
    "        context = row.sentence\n",
    "        word = row.word\n",
    "        \n",
    "        n +=1\n",
    "\n",
    "\n",
    "        prediction = model.predict_in_context(word, context, bert, glove=glove)\n",
    "        #top_k =  model.predict_top_n_features_in_context(singular, context, k, bert)\n",
    "\n",
    "        # get gold feature\n",
    "        gold_vector = model.feature_norms.get_feature_vector(cue_word)\n",
    "        gold_feats = model.feature_norms.get_features(cue_word)\n",
    "        k = len(gold_feats)\n",
    "\n",
    "        # get cosines\n",
    "        cos = 1 - cosine(prediction, gold_vector)\n",
    "        cosines.append(cos)\n",
    "\n",
    "        # get MAP at 10\n",
    "        top_10 = model.predict_top_n_features_in_context(word, context, 10, vec=prediction, bert=bert)\n",
    "        top_10_gold = model.feature_norms.top_n(cue_word, 10)\n",
    "        \n",
    "        num_in_top_10 = len(set(top_10).intersection(set(top_10_gold)))\n",
    "        top_10_prec = num_in_top_10 / len(top_10_gold)\n",
    "        top_10_precs.append(top_10_prec)\n",
    "\n",
    "        # get MAP at 20\n",
    "        top_20 = model.predict_top_n_features_in_context(word, context, 10, vec=prediction, bert=bert)\n",
    "        top_20_gold = model.feature_norms.top_n(cue_word, 20)\n",
    "\n",
    "        num_in_top_20 = len(set(top_20).intersection(set(top_20_gold)))\n",
    "        top_20_prec = num_in_top_20 / len(top_20_gold)\n",
    "        top_20_precs.append(top_20_prec)\n",
    "\n",
    "        # get MAP at k\n",
    "        gold_len = len(gold_feats)\n",
    "        top_k = model.predict_top_n_features_in_context(word, context, gold_len, vec=prediction, bert=bert)\n",
    "        num_in_top_k = len(set(top_k).intersection(set(gold_feats)))\n",
    "        top_k_prec = num_in_top_k / gold_len\n",
    "        top_k_precs.append(top_k_prec)\n",
    "\n",
    "        # get correlation\n",
    "        corr, p = spearmanr(prediction, gold_vector)\n",
    "        correlations.append(corr)\n",
    "\n",
    "        #if ((i % 20 ==0) and debug=='info') or (debug=='true'):\n",
    "#         if False:\n",
    "#             print(word)\n",
    "#             print(\"top ten predicted features: \", top_10)\n",
    "#             print(\"top ten gold features: \", top_10_gold)\n",
    "#             print(\"gold features: \", gold_feats)\n",
    "#             print(\"top k predicted features: \", top_k)\n",
    "\n",
    "#             print(\"cosine: %f\" % cos)\n",
    "#             print(\"correlation: %f\" % corr)\n",
    "#             print(\"top k prec: %f\" % top_k_prec)\n",
    "    \n",
    "    \n",
    "        row = (cue_word, context, word, top_10_prec, top_20_prec, top_k_prec, top_10, top_10_gold, top_k, gold_feats, cos)\n",
    "        rows.append(row)\n",
    "    \n",
    "    cols = ['cue_word', \n",
    "               'context', \n",
    "               'lemma', \n",
    "               'top_10_prec', \n",
    "               'top_20_prec', \n",
    "               'top_k_prec',\n",
    "               'top_10', \n",
    "               'top_10_gold', \n",
    "               'top_k', \n",
    "               'gold_feats', \n",
    "               'cos']\n",
    "    \n",
    "        \n",
    "    avg_top_10_prec = np.average(top_10_precs)\n",
    "    avg_top_20_prec = np.average(top_20_precs)\n",
    "    avg_top_k_prec = np.average(top_k_precs)\n",
    "    average_correlation = np.average(correlations)\n",
    "    average_cosine = np.average(cosines)\n",
    "        \n",
    "        \n",
    "    print(\"Average cosine between gold and predicted feature norms: %s\" % average_cosine)\n",
    "    print(\"average Percentage (%) of gold gold-standard features retrieved in the top 10 features of the predicted vector: \", top_10_prec)\n",
    "    print(\"average Percentage (%) of gold gold-standard features retrieved in the top 20 features of the predicted vector: \", top_20_prec)\n",
    "    print(\"Average % @k (derby metric)\", top_k_prec)\n",
    "    print(\"correlation between gold and predicted vectors: %s \" % average_correlation)        \n",
    "    print(\"total number of predictions: \", n)\n",
    "    \n",
    "    results_df = pd.DataFrame.from_records(rows, columns = cols)\n",
    "    results_df['avg_top_10_prec'] = avg_top_10_prec\n",
    "    results_df['avg_top_20_prec'] = avg_top_20_prec\n",
    "    results_df['avg_top_k_prec'] = avg_top_k_prec\n",
    "    results_df['avg_correlation'] = average_correlation\n",
    "    results_df['avg_cosine'] = average_cosine\n",
    "    \n",
    "    # TODO make this possible!!!!\n",
    "    # results_df['model'] = model.name ()\n",
    "    results_df['model'] = model_name\n",
    "\n",
    "    \n",
    "    return results_df\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate McRae models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************\n",
      "*** Evaluating ../trained_models/model.ffnn.mc_rae_real.allbuthomonyms.5k.50epochs.0.5dropout.lr1e-4.hsize300 model ***\n",
      "****************************************\n",
      "Average cosine between gold and predicted feature norms: 0.24112600584701804\n",
      "average Percentage (%) of gold gold-standard features retrieved in the top 10 features of the predicted vector:  0.0\n",
      "average Percentage (%) of gold gold-standard features retrieved in the top 20 features of the predicted vector:  0.0\n",
      "Average % @k (derby metric) 0.0\n",
      "correlation between gold and predicted vectors: 0.055783235473536016 \n",
      "total number of predictions:  130\n",
      "****************************************\n",
      "*** Evaluating ../trained_models/model.ffnn.mc_rae_real.allbuthomonyms.1k.50epochs.0.5dropout.lr1e-4.hsize300 model ***\n",
      "****************************************\n",
      "Average cosine between gold and predicted feature norms: 0.25036146954868194\n",
      "average Percentage (%) of gold gold-standard features retrieved in the top 10 features of the predicted vector:  0.1\n",
      "average Percentage (%) of gold gold-standard features retrieved in the top 20 features of the predicted vector:  0.09090909090909091\n",
      "Average % @k (derby metric) 0.09090909090909091\n",
      "correlation between gold and predicted vectors: 0.055745966437194754 \n",
      "total number of predictions:  130\n",
      "****************************************\n",
      "*** Evaluating ../trained_models/model.plsr.mc_rae_real.allbuthomonyms.5k.100components.500max_iters model ***\n",
      "****************************************\n",
      "Average cosine between gold and predicted feature norms: 0.24308423764622308\n",
      "average Percentage (%) of gold gold-standard features retrieved in the top 10 features of the predicted vector:  0.1\n",
      "average Percentage (%) of gold gold-standard features retrieved in the top 20 features of the predicted vector:  0.09090909090909091\n",
      "Average % @k (derby metric) 0.09090909090909091\n",
      "correlation between gold and predicted vectors: 0.05398030340700466 \n",
      "total number of predictions:  130\n",
      "****************************************\n",
      "*** Evaluating ../trained_models/model.plsr.mc_rae_real.allbuthomonyms.1k.50components.500max_iters model ***\n",
      "****************************************\n",
      "Average cosine between gold and predicted feature norms: 0.24769186136839783\n",
      "average Percentage (%) of gold gold-standard features retrieved in the top 10 features of the predicted vector:  0.1\n",
      "average Percentage (%) of gold gold-standard features retrieved in the top 20 features of the predicted vector:  0.09090909090909091\n",
      "Average % @k (derby metric) 0.18181818181818182\n",
      "correlation between gold and predicted vectors: 0.05355777153726799 \n",
      "total number of predictions:  130\n",
      "****************************************\n",
      "*** Evaluating ../trained_models/model.modabs.mc_rae_real.5k.mu1_1.mu2_0.1.mu3_0.001.mu4_5.nnk_4 model ***\n",
      "****************************************\n",
      "owl_4\n",
      "owl_4\n",
      "cat_2\n",
      "ball_3\n",
      "cat_2\n",
      "cat_4\n",
      "cat_2\n",
      "rat_4\n",
      "spider_0\n",
      "frog_0\n",
      "ball_3\n",
      "ball_4\n",
      "ball_0\n",
      "machete_1\n",
      "machete_4\n",
      "ball_4\n",
      "ball_3\n",
      "ball_0\n",
      "ball_0\n",
      "ball_4\n",
      "desk_1\n",
      "desk_1\n",
      "desk_1\n",
      "desk_1\n",
      "plate_4\n",
      "desk_1\n",
      "plate_4\n",
      "plate_4\n",
      "desk_1\n",
      "desk_1\n",
      "box_3\n",
      "surfboard_1\n",
      "plate_4\n",
      "surfboard_1\n",
      "plate_4\n",
      "skis_3\n",
      "box_3\n",
      "surfboard_4\n",
      "plate_4\n",
      "freezer_4\n",
      "rope_0\n",
      "bouquet_4\n",
      "rope_0\n",
      "fawn_1\n",
      "rope_0\n",
      "shawl_0\n",
      "necklace_4\n",
      "rope_0\n",
      "rope_1\n",
      "rope_4\n",
      "spear_3\n",
      "crossbow_4\n",
      "spear_4\n",
      "ship_0\n",
      "spear_4\n",
      "crossbow_3\n",
      "spear_4\n",
      "spear_4\n",
      "spear_4\n",
      "spear_4\n",
      "clamp_3\n",
      "clamp_3\n",
      "clamp_3\n",
      "spatula_0\n",
      "clamp_3\n",
      "helmet_4\n",
      "seal_2\n",
      "bottle_0\n",
      "bottle_2\n",
      "helmet_4\n",
      "helmet_1\n",
      "helmet_1\n",
      "jacket_3\n",
      "helmet_1\n",
      "helmet_1\n",
      "helmet_3\n",
      "jacket_3\n",
      "helmet_1\n",
      "helmet_1\n",
      "helmet_1\n",
      "elevator_4\n",
      "rake_2\n",
      "elevator_4\n",
      "elevator_4\n",
      "drill_2\n",
      "elevator_4\n",
      "truck_3\n",
      "helicopter_3\n",
      "elevator_4\n",
      "elevator_4\n",
      "hose_1\n",
      "hose_1\n",
      "hose_1\n",
      "hose_3\n",
      "hose_1\n",
      "hose_3\n",
      "hose_1\n",
      "hose_1\n",
      "hose_1\n",
      "hose_1\n",
      "hose_2\n",
      "hose_0\n",
      "hose_2\n",
      "hose_0\n",
      "hose_0\n",
      "hose_0\n",
      "hose_1\n",
      "hose_1\n",
      "hose_0\n",
      "hose_2\n",
      "mink_2\n",
      "mink_2\n",
      "mink_1\n",
      "mink_2\n",
      "mink_4\n",
      "mink_4\n",
      "mink_3\n",
      "mink_4\n",
      "mink_0\n",
      "mink_4\n",
      "mink_1\n",
      "mink_1\n",
      "mink_1\n",
      "mink_1\n",
      "mink_1\n",
      "mink_1\n",
      "mink_4\n",
      "mink_1\n",
      "mink_1\n",
      "mink_1\n",
      "Average cosine between gold and predicted feature norms: 0.2241771079883443\n",
      "average Percentage (%) of gold gold-standard features retrieved in the top 10 features of the predicted vector:  0.0\n",
      "average Percentage (%) of gold gold-standard features retrieved in the top 20 features of the predicted vector:  0.0\n",
      "Average % @k (derby metric) 0.0\n",
      "correlation between gold and predicted vectors: 0.044487352420526295 \n",
      "total number of predictions:  130\n",
      "****************************************\n",
      "*** Evaluating ../trained_models/model.modabs.mc_rae_real.1k.mu1_1.mu2_0.1.mu3_1e-07.mu4_10.nnk_4 model ***\n",
      "****************************************\n",
      "owl_4\n",
      "owl_4\n",
      "cat_2\n",
      "ball_3\n",
      "cat_2\n",
      "cat_4\n",
      "cat_2\n",
      "rat_4\n",
      "spider_0\n",
      "frog_0\n",
      "ball_3\n",
      "ball_4\n",
      "ball_0\n",
      "machete_1\n",
      "machete_4\n",
      "ball_4\n",
      "ball_3\n",
      "ball_0\n",
      "ball_0\n",
      "ball_4\n",
      "desk_1\n",
      "desk_1\n",
      "desk_1\n",
      "desk_1\n",
      "plate_4\n",
      "desk_1\n",
      "plate_4\n",
      "plate_4\n",
      "desk_1\n",
      "desk_1\n",
      "box_3\n",
      "surfboard_1\n",
      "plate_4\n",
      "surfboard_1\n",
      "plate_4\n",
      "skis_3\n",
      "box_3\n",
      "surfboard_4\n",
      "plate_4\n",
      "freezer_4\n",
      "rope_0\n",
      "bouquet_4\n",
      "rope_0\n",
      "fawn_1\n",
      "rope_0\n",
      "shawl_0\n",
      "necklace_4\n",
      "rope_0\n",
      "rope_1\n",
      "rope_4\n",
      "spear_3\n",
      "crossbow_4\n",
      "spear_4\n",
      "ship_0\n",
      "spear_4\n",
      "crossbow_3\n",
      "spear_4\n",
      "spear_4\n",
      "spear_4\n",
      "spear_4\n",
      "clamp_3\n",
      "clamp_3\n",
      "clamp_3\n",
      "spatula_0\n",
      "clamp_3\n",
      "helmet_4\n",
      "seal_2\n",
      "bottle_0\n",
      "bottle_2\n",
      "helmet_4\n",
      "helmet_1\n",
      "helmet_1\n",
      "jacket_3\n",
      "helmet_1\n",
      "helmet_1\n",
      "helmet_3\n",
      "jacket_3\n",
      "helmet_1\n",
      "helmet_1\n",
      "helmet_1\n",
      "elevator_4\n",
      "rake_2\n",
      "elevator_4\n",
      "elevator_4\n",
      "drill_2\n",
      "elevator_4\n",
      "truck_3\n",
      "helicopter_3\n",
      "elevator_4\n",
      "elevator_4\n",
      "hose_1\n",
      "hose_1\n",
      "hose_1\n",
      "hose_3\n",
      "hose_1\n",
      "hose_3\n",
      "hose_1\n",
      "hose_1\n",
      "hose_1\n",
      "hose_1\n",
      "hose_2\n",
      "hose_0\n",
      "hose_2\n",
      "hose_0\n",
      "hose_0\n",
      "hose_0\n",
      "hose_1\n",
      "hose_1\n",
      "hose_0\n",
      "hose_2\n",
      "mink_2\n",
      "mink_2\n",
      "mink_1\n",
      "mink_2\n",
      "mink_4\n",
      "mink_4\n",
      "mink_3\n",
      "mink_4\n",
      "mink_0\n",
      "mink_4\n",
      "mink_1\n",
      "mink_1\n",
      "mink_1\n",
      "mink_1\n",
      "mink_1\n",
      "mink_1\n",
      "mink_4\n",
      "mink_1\n",
      "mink_1\n",
      "mink_1\n",
      "Average cosine between gold and predicted feature norms: 0.22419249076241743\n",
      "average Percentage (%) of gold gold-standard features retrieved in the top 10 features of the predicted vector:  0.0\n",
      "average Percentage (%) of gold gold-standard features retrieved in the top 20 features of the predicted vector:  0.0\n",
      "Average % @k (derby metric) 0.0\n",
      "correlation between gold and predicted vectors: 0.044486651143452974 \n",
      "total number of predictions:  130\n"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "\n",
    "for save_path in mcrae_models:\n",
    "    print(\"****************************************\")\n",
    "    print(\"*** Evaluating %s model ***\" % save_path)\n",
    "    print(\"****************************************\")\n",
    "    model = torch.load(save_path)\n",
    "\n",
    "    df = evaluate_in_context(model, eval_words, bert, model_name=save_path, dataset='mc_rae')\n",
    "    dfs.append(df)\n",
    "    \n",
    "# need to do this separately because we dont have nice self examining attributes for our model classes to tell when its \n",
    "# been trained on glove\n",
    "for save_path in mcrae_glove_models:\n",
    "    print(\"****************************************\")\n",
    "    print(\"*** Evaluating %s model ***\" % save_path)\n",
    "    print(\"****************************************\")\n",
    "    model = torch.load(save_path)\n",
    "    df = evaluate_in_context(model, eval_words, bert, glove=True, model_name=save_path, dataset='mc_rae')\n",
    "    dfs.append(df)\n",
    "\n",
    "df = pd.concat(dfs)\n",
    "df.to_csv('../results/homonymous_words_eval_mcrae_models_expanded_10_20_22.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cue_word</th>\n",
       "      <th>context</th>\n",
       "      <th>lemma</th>\n",
       "      <th>top_10_prec</th>\n",
       "      <th>top_20_prec</th>\n",
       "      <th>top_k_prec</th>\n",
       "      <th>top_10</th>\n",
       "      <th>top_10_gold</th>\n",
       "      <th>top_k</th>\n",
       "      <th>gold_feats</th>\n",
       "      <th>cos</th>\n",
       "      <th>avg_top_10_prec</th>\n",
       "      <th>avg_top_20_prec</th>\n",
       "      <th>avg_top_k_prec</th>\n",
       "      <th>avg_correlation</th>\n",
       "      <th>avg_cosine</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bat_(animal)</td>\n",
       "      <td>causes and consequences of sociality in animals. Detailed studies on bat sociality are rare, however, when compared with the information available for other social mammals, such as primates, carnivores, ungulates, and rodents. Modern field technologies and new molecular methods are now providing opportunities to study aspects of bat biology that were previously inaccessible. Consequently, bat social systems are emerging as farmore complex than had been imagined. Variable dispersal patterns, complex olfactory and acoustic communication, flexible context-related interactions, striking cooperative behaviors, and cryptic colony structures in the form of fission-fusion systems have been documented. Bat</td>\n",
       "      <td>bat</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>[is_furry, an_animal, a_mammal, is_large, has_4_legs, has_legs, is_black, is_small, an_insect, has_fur]</td>\n",
       "      <td>[an_animal, beh_-_sleeps_upside_down, is_blind, has_fur, lives_in_caves, is_black, beh_-_flies, beh_-_is_nocturnal, has_wings, associated_with_vampires]</td>\n",
       "      <td>[beh_-_eats, a_carnivore, a_musical_instrument, is_furry, is_large, has_legs, has_4_legs, an_animal, a_mammal, has_fur, has_a_tail, is_brown, is_black, is_small, an_insect, is_long]</td>\n",
       "      <td>[has_wings, beh_-_flies, beh_-_is_nocturnal, is_black, lives_in_caves, has_fur, an_animal, beh_-_sleeps_upside_down, is_blind, beh_-_uses_radar_to_navigate, beh_-_screeches, a_mammal, has_fangs, is_scary, is_small, associated_with_vampires]</td>\n",
       "      <td>0.323878</td>\n",
       "      <td>0.215021</td>\n",
       "      <td>0.201111</td>\n",
       "      <td>0.213409</td>\n",
       "      <td>0.055783</td>\n",
       "      <td>0.241126</td>\n",
       "      <td>../trained_models/model.ffnn.mc_rae_real.allbuthomonyms.5k.50epochs.0.5dropout.lr1e-4.hsize300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bat_(animal)</td>\n",
       "      <td>and as a consequence the resultant bat swarms are transient, have a flexible composition, and comprise mostly unrelated bats (Kerth et al. 2003a). How bats from separated colonies find swarming sites that are often located dozens of kilometers away from the maternity colonies remains one of the puzzles of bat biology. # Several cooperative behaviors in bats are linked to communal breeding. Mutualwarming and babysitting of pups have thus far been documented in only a few species, such as Antrozous pallidus (figure 3a; Wilkinson 1988, Lewis 1996, Zubaid et al. 2006, Willis and Brigham 2007</td>\n",
       "      <td>bat</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>[is_green, has_wings, is_large, has_legs, is_long, an_animal, beh_-_flies, an_insect, is_small, is_black]</td>\n",
       "      <td>[an_animal, beh_-_sleeps_upside_down, is_blind, has_fur, lives_in_caves, is_black, beh_-_flies, beh_-_is_nocturnal, has_wings, associated_with_vampires]</td>\n",
       "      <td>[has_4_legs, has_fur, a_musical_instrument, beh_-_stings, has_wings, is_large, is_green, beh_-_flies, an_animal, is_long, has_legs, made_of_wood, beh_-_crawls, an_insect, is_small, is_black]</td>\n",
       "      <td>[has_wings, beh_-_flies, beh_-_is_nocturnal, is_black, lives_in_caves, has_fur, an_animal, beh_-_sleeps_upside_down, is_blind, beh_-_uses_radar_to_navigate, beh_-_screeches, a_mammal, has_fangs, is_scary, is_small, associated_with_vampires]</td>\n",
       "      <td>0.333446</td>\n",
       "      <td>0.215021</td>\n",
       "      <td>0.201111</td>\n",
       "      <td>0.213409</td>\n",
       "      <td>0.055783</td>\n",
       "      <td>0.241126</td>\n",
       "      <td>../trained_models/model.ffnn.mc_rae_real.allbuthomonyms.5k.50epochs.0.5dropout.lr1e-4.hsize300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bat_(animal)</td>\n",
       "      <td>blades. Why would bats be around wind turbines in the first place? And which species were most at risk? \" It took all of us by surprise \" says Cryan, a research biologist at the US Geological Survey's Fort Collins Science Center in Colorado. Cryan was speaking of the bat community's reaction to news that hundreds of the flying mammals had died one nightat Mountaineer Wind Energy Center in Thomas, West Virginia, and, later, at other wind farms throughout the United States, Canada, and Europe. \" The researchers at Mountaineer were looking for birds, not</td>\n",
       "      <td>bat</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>[has_legs, a_musical_instrument, beh_-_flies, has_wings, is_green, an_animal, is_long, is_small, is_black, an_insect]</td>\n",
       "      <td>[an_animal, beh_-_sleeps_upside_down, is_blind, has_fur, lives_in_caves, is_black, beh_-_flies, beh_-_is_nocturnal, has_wings, associated_with_vampires]</td>\n",
       "      <td>[is_slimy, beh_-_eats, has_a_shell, is_brown, has_fur, beh_-_stings, has_legs, a_musical_instrument, beh_-_flies, has_wings, is_green, an_animal, is_long, is_small, is_black, an_insect]</td>\n",
       "      <td>[has_wings, beh_-_flies, beh_-_is_nocturnal, is_black, lives_in_caves, has_fur, an_animal, beh_-_sleeps_upside_down, is_blind, beh_-_uses_radar_to_navigate, beh_-_screeches, a_mammal, has_fangs, is_scary, is_small, associated_with_vampires]</td>\n",
       "      <td>0.364130</td>\n",
       "      <td>0.215021</td>\n",
       "      <td>0.201111</td>\n",
       "      <td>0.213409</td>\n",
       "      <td>0.055783</td>\n",
       "      <td>0.241126</td>\n",
       "      <td>../trained_models/model.ffnn.mc_rae_real.allbuthomonyms.5k.50epochs.0.5dropout.lr1e-4.hsize300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bat_(animal)</td>\n",
       "      <td>I thought you'd know. Do n't all you spandex boys... -... have club meetings? - We're not exactly friends. I'll be your friend. Sorry, little girl. You're going home. - What about the alarm? - The cops are swamped and the Bat's a no-show. Who's gon na come? - There's always therelief pitcher. - Smoke him! You see where he went? Let's just get out of here. It's cold out, better bundle up. Black and blue are definitely your color. Batman Schmatman</td>\n",
       "      <td>bat</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>[a_tool, a_bird, is_long, made_of_metal, is_green, is_small, an_insect, beh_-_flies, has_wings, made_of_wood]</td>\n",
       "      <td>[an_animal, beh_-_sleeps_upside_down, is_blind, has_fur, lives_in_caves, is_black, beh_-_flies, beh_-_is_nocturnal, has_wings, associated_with_vampires]</td>\n",
       "      <td>[has_a_shell, worn_for_warmth, has_a_handle, a_vegetable, is_black, used_for_storage, a_tool, is_long, a_bird, made_of_metal, is_green, is_small, an_insect, beh_-_flies, has_wings, made_of_wood]</td>\n",
       "      <td>[has_wings, beh_-_flies, beh_-_is_nocturnal, is_black, lives_in_caves, has_fur, an_animal, beh_-_sleeps_upside_down, is_blind, beh_-_uses_radar_to_navigate, beh_-_screeches, a_mammal, has_fangs, is_scary, is_small, associated_with_vampires]</td>\n",
       "      <td>0.281827</td>\n",
       "      <td>0.215021</td>\n",
       "      <td>0.201111</td>\n",
       "      <td>0.213409</td>\n",
       "      <td>0.055783</td>\n",
       "      <td>0.241126</td>\n",
       "      <td>../trained_models/model.ffnn.mc_rae_real.allbuthomonyms.5k.50epochs.0.5dropout.lr1e-4.hsize300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bat_(animal)</td>\n",
       "      <td>mates (Wilkinson 1986, Kerth et al. 2003b). # In several species, colony members transfer information about food (McCracken and Bradbury 1981, Wilkinson 1992a). The benefits of this kind of cooperation depend on a species' foraging strategy. Information transfer about food occurs mostly in bat species that forage on ephemeral and unpredictable food resources, such as aerial insect swarmsor irregularly appearing fruits (McCracken and Bradbury 1981, Wilkinson 1992a, Sail and Kerth 2007). In contrast, in the species whose colony members are faithful to individual foraging areas, information transfer about food seems</td>\n",
       "      <td>bat</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>[is_small, is_long, is_black, is_green, a_bird, beh_-_flies, has_wings, is_large, an_animal, an_insect]</td>\n",
       "      <td>[an_animal, beh_-_sleeps_upside_down, is_blind, has_fur, lives_in_caves, is_black, beh_-_flies, beh_-_is_nocturnal, has_wings, associated_with_vampires]</td>\n",
       "      <td>[beh_-_eats, has_a_beak, beh_-_stings, is_furry, a_vegetable, has_legs, is_small, is_long, is_black, is_green, a_bird, beh_-_flies, has_wings, is_large, an_animal, an_insect]</td>\n",
       "      <td>[has_wings, beh_-_flies, beh_-_is_nocturnal, is_black, lives_in_caves, has_fur, an_animal, beh_-_sleeps_upside_down, is_blind, beh_-_uses_radar_to_navigate, beh_-_screeches, a_mammal, has_fangs, is_scary, is_small, associated_with_vampires]</td>\n",
       "      <td>0.429560</td>\n",
       "      <td>0.215021</td>\n",
       "      <td>0.201111</td>\n",
       "      <td>0.213409</td>\n",
       "      <td>0.055783</td>\n",
       "      <td>0.241126</td>\n",
       "      <td>../trained_models/model.ffnn.mc_rae_real.allbuthomonyms.5k.50epochs.0.5dropout.lr1e-4.hsize300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>hose</td>\n",
       "      <td>, and they must be chafe-protected wherever they contact bulkheads or stringers. Also, automotive hoses are not designed for marine cooling systems, even though they may fit. Automotive hoses are stiffened by spiral wire between the layers of rubber. Those wires will eventually abrade and wear through. Proper marine hose is so marked and is never wire-stiffened. The hoses on those ill-fated boats areoften single-clamped instead of double-clamped. And the clamps either are n't the all-stainless marine variety or they are the inexpensive type that are notched to take the worm-gear tightening stud through the whole band of the clamp. When</td>\n",
       "      <td>hose</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>[found_in_kitchens, is_small, used_for_holding, lives_in_water, is_electrical, is_green, made_of_plastic, made_of_metal, is_long, a_vegetable]</td>\n",
       "      <td>[inbeh_-_water_flows_through_it, is_green, used_for_watering_gardens, is_long, a_tube, used_for_washing_cars, found_outdoors, has_a_nozzle, inbeh_-_sprays_water, is_thin]</td>\n",
       "      <td>[made_of_paper, a_vegetable, lives_in_water, is_long, an_animal, beh_-_swims, is_edible, is_green, is_electrical, made_of_metal, found_in_kitchens, a_fish, is_small, used_for_cooking, used_for_holding, made_of_plastic]</td>\n",
       "      <td>[is_long, is_green, used_for_watering_gardens, inbeh_-_water_flows_through_it, used_for_watering_lawns, is_round, used_by_firemen, made_of_rubber, used_with_sprinklers, found_outdoors, has_a_nozzle, inbeh_-_sprays_water, is_thin, made_of_plastic, a_tube, used_for_washing_cars]</td>\n",
       "      <td>0.147991</td>\n",
       "      <td>0.215021</td>\n",
       "      <td>0.201111</td>\n",
       "      <td>0.213409</td>\n",
       "      <td>0.055783</td>\n",
       "      <td>0.241126</td>\n",
       "      <td>../trained_models/model.ffnn.mc_rae_real.allbuthomonyms.5k.50epochs.0.5dropout.lr1e-4.hsize300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>hose</td>\n",
       "      <td>crew remained. Nonetheless, the couple was checked into Room 217, the Presidential Suite, as the only paying guests. # That night, the author had a nightmare in which he saw his young son being chased down the hotel's long, empty corridors by a predatory, possessed fire hose. He woke drenched in sweat and stepped to the balcony to smoke a cigarette. By the time he stubbed it out, he'd worked out the \" bones \" of what would become his third novel, and first best-seller, \" The Shining. \" # King's nightmare turned out</td>\n",
       "      <td>hose</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>[grows_in_gardens, eaten_in_salads, a_tool, is_electrical, found_in_bathrooms, is_green, is_long, made_of_metal, a_vegetable, has_a_handle]</td>\n",
       "      <td>[inbeh_-_water_flows_through_it, is_green, used_for_watering_gardens, is_long, a_tube, used_for_washing_cars, found_outdoors, has_a_nozzle, inbeh_-_sprays_water, is_thin]</td>\n",
       "      <td>[made_of_plastic, is_sharp, is_edible, grows_in_gardens, used_for_transportation, eaten_in_salads, used_for_making_light, is_fast, a_tool, is_electrical, found_in_bathrooms, is_green, is_long, made_of_metal, a_vegetable, has_a_handle]</td>\n",
       "      <td>[is_long, is_green, used_for_watering_gardens, inbeh_-_water_flows_through_it, used_for_watering_lawns, is_round, used_by_firemen, made_of_rubber, used_with_sprinklers, found_outdoors, has_a_nozzle, inbeh_-_sprays_water, is_thin, made_of_plastic, a_tube, used_for_washing_cars]</td>\n",
       "      <td>0.203672</td>\n",
       "      <td>0.215021</td>\n",
       "      <td>0.201111</td>\n",
       "      <td>0.213409</td>\n",
       "      <td>0.055783</td>\n",
       "      <td>0.241126</td>\n",
       "      <td>../trained_models/model.ffnn.mc_rae_real.allbuthomonyms.5k.50epochs.0.5dropout.lr1e-4.hsize300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>hose</td>\n",
       "      <td>spreads into pasture, destroying valuable fodder. Conway preps for the assault while Taber mixes herbicide with soap, which makes it stick to the plants, and blue dye, so they know where they've sprayed. In her blue coveralls, Kacey Conway tromps through waist-high weed, with a long hose connected to the tank of herbicide back on the raft. She stops and takesaim. (Soundbite-of-spray) BRADY: This is a good time of year to spray Russian knapweed, says Conway. It's preparing for winter by sending nutrients down to the roots. Now, she says, it'll</td>\n",
       "      <td>hose</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>[used_for_cooking, is_sharp, is_green, a_tool, made_of_metal, is_long, has_a_handle, made_of_plastic, a_vegetable, found_in_kitchens]</td>\n",
       "      <td>[inbeh_-_water_flows_through_it, is_green, used_for_watering_gardens, is_long, a_tube, used_for_washing_cars, found_outdoors, has_a_nozzle, inbeh_-_sprays_water, is_thin]</td>\n",
       "      <td>[grows_in_gardens, is_electrical, is_round, eaten_in_salads, used_for_holding_things, found_in_bathrooms, used_for_cooking, is_sharp, is_green, a_tool, made_of_metal, is_long, has_a_handle, made_of_plastic, a_vegetable, found_in_kitchens]</td>\n",
       "      <td>[is_long, is_green, used_for_watering_gardens, inbeh_-_water_flows_through_it, used_for_watering_lawns, is_round, used_by_firemen, made_of_rubber, used_with_sprinklers, found_outdoors, has_a_nozzle, inbeh_-_sprays_water, is_thin, made_of_plastic, a_tube, used_for_washing_cars]</td>\n",
       "      <td>0.220974</td>\n",
       "      <td>0.215021</td>\n",
       "      <td>0.201111</td>\n",
       "      <td>0.213409</td>\n",
       "      <td>0.055783</td>\n",
       "      <td>0.241126</td>\n",
       "      <td>../trained_models/model.ffnn.mc_rae_real.allbuthomonyms.5k.50epochs.0.5dropout.lr1e-4.hsize300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>hose</td>\n",
       "      <td>\" There's a magic in the water that's untold inside of me, \" Barrowman said the other day as he ate lunch on the Michigan campus. \" I can go to a lake and just stare at it for hours. As a child, I screamed if they took a hose away from me. I wanted to be near water. '' # Barrowman'sgrandmother, Jean Albert, was a Red Cross swimming instructor in the Maryland suburbs and began taking him to the pool when he was 6 months old. Mike and his younger sister, Sophia, now on the</td>\n",
       "      <td>hose</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>[is_electrical, a_tool, found_in_bathrooms, found_in_kitchens, made_of_plastic, is_long, is_green, a_vegetable, made_of_metal, has_a_handle]</td>\n",
       "      <td>[inbeh_-_water_flows_through_it, is_green, used_for_watering_gardens, is_long, a_tube, used_for_washing_cars, found_outdoors, has_a_nozzle, inbeh_-_sprays_water, is_thin]</td>\n",
       "      <td>[used_for_water, has_leaves, an_insect, eaten_in_salads, found_in_bathrooms, found_in_kitchens, a_tool, grows_in_gardens, used_for_cooking, is_electrical, made_of_plastic, is_long, is_green, a_vegetable, made_of_metal, has_a_handle]</td>\n",
       "      <td>[is_long, is_green, used_for_watering_gardens, inbeh_-_water_flows_through_it, used_for_watering_lawns, is_round, used_by_firemen, made_of_rubber, used_with_sprinklers, found_outdoors, has_a_nozzle, inbeh_-_sprays_water, is_thin, made_of_plastic, a_tube, used_for_washing_cars]</td>\n",
       "      <td>0.199526</td>\n",
       "      <td>0.215021</td>\n",
       "      <td>0.201111</td>\n",
       "      <td>0.213409</td>\n",
       "      <td>0.055783</td>\n",
       "      <td>0.241126</td>\n",
       "      <td>../trained_models/model.ffnn.mc_rae_real.allbuthomonyms.5k.50epochs.0.5dropout.lr1e-4.hsize300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>hose</td>\n",
       "      <td>even a broom, It's kinda like drinkin and driving, except this is water. # Report abuse Oct. 18, 2007, 9:48 am # Tim says: # My kids draw on the sidewalk all the time! What I think there missing is this cleans up wiff water rain, hose, even a broom, It's kinda like drinkin and driving, except thisis water. # Report abuse Oct. 18, 2007, 9:50 am # doofus says: # Lock her up and toss away the key!!! # Report abuse Oct. 18, 2007, 10:47 am #</td>\n",
       "      <td>hose</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>[made_of_plastic, a_tool, used_for_cooking, made_of_wood, found_in_kitchens, is_green, has_a_handle, is_long, a_vegetable, made_of_metal]</td>\n",
       "      <td>[inbeh_-_water_flows_through_it, is_green, used_for_watering_gardens, is_long, a_tube, used_for_washing_cars, found_outdoors, has_a_nozzle, inbeh_-_sprays_water, is_thin]</td>\n",
       "      <td>[an_insect, found_in_bathrooms, eaten_in_salads, grows_in_gardens, made_of_metal, is_green, is_long, a_tool, has_a_handle, is_electrical, found_in_kitchens, used_for_cooking, has_leaves, made_of_plastic, a_vegetable, made_of_wood]</td>\n",
       "      <td>[is_long, is_green, used_for_watering_gardens, inbeh_-_water_flows_through_it, used_for_watering_lawns, is_round, used_by_firemen, made_of_rubber, used_with_sprinklers, found_outdoors, has_a_nozzle, inbeh_-_sprays_water, is_thin, made_of_plastic, a_tube, used_for_washing_cars]</td>\n",
       "      <td>0.215315</td>\n",
       "      <td>0.215021</td>\n",
       "      <td>0.201111</td>\n",
       "      <td>0.213409</td>\n",
       "      <td>0.055783</td>\n",
       "      <td>0.241126</td>\n",
       "      <td>../trained_models/model.ffnn.mc_rae_real.allbuthomonyms.5k.50epochs.0.5dropout.lr1e-4.hsize300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        cue_word  \\\n",
       "0   bat_(animal)   \n",
       "1   bat_(animal)   \n",
       "2   bat_(animal)   \n",
       "3   bat_(animal)   \n",
       "4   bat_(animal)   \n",
       "..           ...   \n",
       "95          hose   \n",
       "96          hose   \n",
       "97          hose   \n",
       "98          hose   \n",
       "99          hose   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               context  \\\n",
       "0   causes and consequences of sociality in animals. Detailed studies on bat sociality are rare, however, when compared with the information available for other social mammals, such as primates, carnivores, ungulates, and rodents. Modern field technologies and new molecular methods are now providing opportunities to study aspects of bat biology that were previously inaccessible. Consequently, bat social systems are emerging as farmore complex than had been imagined. Variable dispersal patterns, complex olfactory and acoustic communication, flexible context-related interactions, striking cooperative behaviors, and cryptic colony structures in the form of fission-fusion systems have been documented. Bat   \n",
       "1                                                                                                                   and as a consequence the resultant bat swarms are transient, have a flexible composition, and comprise mostly unrelated bats (Kerth et al. 2003a). How bats from separated colonies find swarming sites that are often located dozens of kilometers away from the maternity colonies remains one of the puzzles of bat biology. # Several cooperative behaviors in bats are linked to communal breeding. Mutualwarming and babysitting of pups have thus far been documented in only a few species, such as Antrozous pallidus (figure 3a; Wilkinson 1988, Lewis 1996, Zubaid et al. 2006, Willis and Brigham 2007   \n",
       "2                                                                                                                                                        blades. Why would bats be around wind turbines in the first place? And which species were most at risk? \" It took all of us by surprise \" says Cryan, a research biologist at the US Geological Survey's Fort Collins Science Center in Colorado. Cryan was speaking of the bat community's reaction to news that hundreds of the flying mammals had died one nightat Mountaineer Wind Energy Center in Thomas, West Virginia, and, later, at other wind farms throughout the United States, Canada, and Europe. \" The researchers at Mountaineer were looking for birds, not   \n",
       "3                                                                                                                                                                                                                                                                    I thought you'd know. Do n't all you spandex boys... -... have club meetings? - We're not exactly friends. I'll be your friend. Sorry, little girl. You're going home. - What about the alarm? - The cops are swamped and the Bat's a no-show. Who's gon na come? - There's always therelief pitcher. - Smoke him! You see where he went? Let's just get out of here. It's cold out, better bundle up. Black and blue are definitely your color. Batman Schmatman   \n",
       "4                                                                                         mates (Wilkinson 1986, Kerth et al. 2003b). # In several species, colony members transfer information about food (McCracken and Bradbury 1981, Wilkinson 1992a). The benefits of this kind of cooperation depend on a species' foraging strategy. Information transfer about food occurs mostly in bat species that forage on ephemeral and unpredictable food resources, such as aerial insect swarmsor irregularly appearing fruits (McCracken and Bradbury 1981, Wilkinson 1992a, Sail and Kerth 2007). In contrast, in the species whose colony members are faithful to individual foraging areas, information transfer about food seems   \n",
       "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 ...   \n",
       "95                                                                 , and they must be chafe-protected wherever they contact bulkheads or stringers. Also, automotive hoses are not designed for marine cooling systems, even though they may fit. Automotive hoses are stiffened by spiral wire between the layers of rubber. Those wires will eventually abrade and wear through. Proper marine hose is so marked and is never wire-stiffened. The hoses on those ill-fated boats areoften single-clamped instead of double-clamped. And the clamps either are n't the all-stainless marine variety or they are the inexpensive type that are notched to take the worm-gear tightening stud through the whole band of the clamp. When   \n",
       "96                                                                                                                                                                                            crew remained. Nonetheless, the couple was checked into Room 217, the Presidential Suite, as the only paying guests. # That night, the author had a nightmare in which he saw his young son being chased down the hotel's long, empty corridors by a predatory, possessed fire hose. He woke drenched in sweat and stepped to the balcony to smoke a cigarette. By the time he stubbed it out, he'd worked out the \" bones \" of what would become his third novel, and first best-seller, \" The Shining. \" # King's nightmare turned out   \n",
       "97                                                                                                                                                                spreads into pasture, destroying valuable fodder. Conway preps for the assault while Taber mixes herbicide with soap, which makes it stick to the plants, and blue dye, so they know where they've sprayed. In her blue coveralls, Kacey Conway tromps through waist-high weed, with a long hose connected to the tank of herbicide back on the raft. She stops and takesaim. (Soundbite-of-spray) BRADY: This is a good time of year to spray Russian knapweed, says Conway. It's preparing for winter by sending nutrients down to the roots. Now, she says, it'll   \n",
       "98                                                                                                                                                                                                                                         \" There's a magic in the water that's untold inside of me, \" Barrowman said the other day as he ate lunch on the Michigan campus. \" I can go to a lake and just stare at it for hours. As a child, I screamed if they took a hose away from me. I wanted to be near water. '' # Barrowman'sgrandmother, Jean Albert, was a Red Cross swimming instructor in the Maryland suburbs and began taking him to the pool when he was 6 months old. Mike and his younger sister, Sophia, now on the   \n",
       "99                                                                                                                                                                                                                                                                            even a broom, It's kinda like drinkin and driving, except this is water. # Report abuse Oct. 18, 2007, 9:48 am # Tim says: # My kids draw on the sidewalk all the time! What I think there missing is this cleans up wiff water rain, hose, even a broom, It's kinda like drinkin and driving, except thisis water. # Report abuse Oct. 18, 2007, 9:50 am # doofus says: # Lock her up and toss away the key!!! # Report abuse Oct. 18, 2007, 10:47 am #   \n",
       "\n",
       "   lemma  top_10_prec  top_20_prec  top_k_prec  \\\n",
       "0    bat          0.3       0.3125      0.3125   \n",
       "1    bat          0.4       0.3125      0.3750   \n",
       "2    bat          0.4       0.3125      0.3750   \n",
       "3    bat          0.2       0.1875      0.2500   \n",
       "4    bat          0.4       0.3125      0.3125   \n",
       "..   ...          ...          ...         ...   \n",
       "95  hose          0.2       0.1875      0.1875   \n",
       "96  hose          0.2       0.1250      0.1875   \n",
       "97  hose          0.2       0.1875      0.2500   \n",
       "98  hose          0.2       0.1875      0.1875   \n",
       "99  hose          0.2       0.1875      0.1875   \n",
       "\n",
       "                                                                                                                                            top_10  \\\n",
       "0                                          [is_furry, an_animal, a_mammal, is_large, has_4_legs, has_legs, is_black, is_small, an_insect, has_fur]   \n",
       "1                                        [is_green, has_wings, is_large, has_legs, is_long, an_animal, beh_-_flies, an_insect, is_small, is_black]   \n",
       "2                            [has_legs, a_musical_instrument, beh_-_flies, has_wings, is_green, an_animal, is_long, is_small, is_black, an_insect]   \n",
       "3                                    [a_tool, a_bird, is_long, made_of_metal, is_green, is_small, an_insect, beh_-_flies, has_wings, made_of_wood]   \n",
       "4                                          [is_small, is_long, is_black, is_green, a_bird, beh_-_flies, has_wings, is_large, an_animal, an_insect]   \n",
       "..                                                                                                                                             ...   \n",
       "95  [found_in_kitchens, is_small, used_for_holding, lives_in_water, is_electrical, is_green, made_of_plastic, made_of_metal, is_long, a_vegetable]   \n",
       "96     [grows_in_gardens, eaten_in_salads, a_tool, is_electrical, found_in_bathrooms, is_green, is_long, made_of_metal, a_vegetable, has_a_handle]   \n",
       "97           [used_for_cooking, is_sharp, is_green, a_tool, made_of_metal, is_long, has_a_handle, made_of_plastic, a_vegetable, found_in_kitchens]   \n",
       "98    [is_electrical, a_tool, found_in_bathrooms, found_in_kitchens, made_of_plastic, is_long, is_green, a_vegetable, made_of_metal, has_a_handle]   \n",
       "99       [made_of_plastic, a_tool, used_for_cooking, made_of_wood, found_in_kitchens, is_green, has_a_handle, is_long, a_vegetable, made_of_metal]   \n",
       "\n",
       "                                                                                                                                                                   top_10_gold  \\\n",
       "0                     [an_animal, beh_-_sleeps_upside_down, is_blind, has_fur, lives_in_caves, is_black, beh_-_flies, beh_-_is_nocturnal, has_wings, associated_with_vampires]   \n",
       "1                     [an_animal, beh_-_sleeps_upside_down, is_blind, has_fur, lives_in_caves, is_black, beh_-_flies, beh_-_is_nocturnal, has_wings, associated_with_vampires]   \n",
       "2                     [an_animal, beh_-_sleeps_upside_down, is_blind, has_fur, lives_in_caves, is_black, beh_-_flies, beh_-_is_nocturnal, has_wings, associated_with_vampires]   \n",
       "3                     [an_animal, beh_-_sleeps_upside_down, is_blind, has_fur, lives_in_caves, is_black, beh_-_flies, beh_-_is_nocturnal, has_wings, associated_with_vampires]   \n",
       "4                     [an_animal, beh_-_sleeps_upside_down, is_blind, has_fur, lives_in_caves, is_black, beh_-_flies, beh_-_is_nocturnal, has_wings, associated_with_vampires]   \n",
       "..                                                                                                                                                                         ...   \n",
       "95  [inbeh_-_water_flows_through_it, is_green, used_for_watering_gardens, is_long, a_tube, used_for_washing_cars, found_outdoors, has_a_nozzle, inbeh_-_sprays_water, is_thin]   \n",
       "96  [inbeh_-_water_flows_through_it, is_green, used_for_watering_gardens, is_long, a_tube, used_for_washing_cars, found_outdoors, has_a_nozzle, inbeh_-_sprays_water, is_thin]   \n",
       "97  [inbeh_-_water_flows_through_it, is_green, used_for_watering_gardens, is_long, a_tube, used_for_washing_cars, found_outdoors, has_a_nozzle, inbeh_-_sprays_water, is_thin]   \n",
       "98  [inbeh_-_water_flows_through_it, is_green, used_for_watering_gardens, is_long, a_tube, used_for_washing_cars, found_outdoors, has_a_nozzle, inbeh_-_sprays_water, is_thin]   \n",
       "99  [inbeh_-_water_flows_through_it, is_green, used_for_watering_gardens, is_long, a_tube, used_for_washing_cars, found_outdoors, has_a_nozzle, inbeh_-_sprays_water, is_thin]   \n",
       "\n",
       "                                                                                                                                                                                                                                             top_k  \\\n",
       "0                                                            [beh_-_eats, a_carnivore, a_musical_instrument, is_furry, is_large, has_legs, has_4_legs, an_animal, a_mammal, has_fur, has_a_tail, is_brown, is_black, is_small, an_insect, is_long]   \n",
       "1                                                   [has_4_legs, has_fur, a_musical_instrument, beh_-_stings, has_wings, is_large, is_green, beh_-_flies, an_animal, is_long, has_legs, made_of_wood, beh_-_crawls, an_insect, is_small, is_black]   \n",
       "2                                                        [is_slimy, beh_-_eats, has_a_shell, is_brown, has_fur, beh_-_stings, has_legs, a_musical_instrument, beh_-_flies, has_wings, is_green, an_animal, is_long, is_small, is_black, an_insect]   \n",
       "3                                               [has_a_shell, worn_for_warmth, has_a_handle, a_vegetable, is_black, used_for_storage, a_tool, is_long, a_bird, made_of_metal, is_green, is_small, an_insect, beh_-_flies, has_wings, made_of_wood]   \n",
       "4                                                                   [beh_-_eats, has_a_beak, beh_-_stings, is_furry, a_vegetable, has_legs, is_small, is_long, is_black, is_green, a_bird, beh_-_flies, has_wings, is_large, an_animal, an_insect]   \n",
       "..                                                                                                                                                                                                                                             ...   \n",
       "95                      [made_of_paper, a_vegetable, lives_in_water, is_long, an_animal, beh_-_swims, is_edible, is_green, is_electrical, made_of_metal, found_in_kitchens, a_fish, is_small, used_for_cooking, used_for_holding, made_of_plastic]   \n",
       "96      [made_of_plastic, is_sharp, is_edible, grows_in_gardens, used_for_transportation, eaten_in_salads, used_for_making_light, is_fast, a_tool, is_electrical, found_in_bathrooms, is_green, is_long, made_of_metal, a_vegetable, has_a_handle]   \n",
       "97  [grows_in_gardens, is_electrical, is_round, eaten_in_salads, used_for_holding_things, found_in_bathrooms, used_for_cooking, is_sharp, is_green, a_tool, made_of_metal, is_long, has_a_handle, made_of_plastic, a_vegetable, found_in_kitchens]   \n",
       "98        [used_for_water, has_leaves, an_insect, eaten_in_salads, found_in_bathrooms, found_in_kitchens, a_tool, grows_in_gardens, used_for_cooking, is_electrical, made_of_plastic, is_long, is_green, a_vegetable, made_of_metal, has_a_handle]   \n",
       "99          [an_insect, found_in_bathrooms, eaten_in_salads, grows_in_gardens, made_of_metal, is_green, is_long, a_tool, has_a_handle, is_electrical, found_in_kitchens, used_for_cooking, has_leaves, made_of_plastic, a_vegetable, made_of_wood]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                               gold_feats  \\\n",
       "0                                        [has_wings, beh_-_flies, beh_-_is_nocturnal, is_black, lives_in_caves, has_fur, an_animal, beh_-_sleeps_upside_down, is_blind, beh_-_uses_radar_to_navigate, beh_-_screeches, a_mammal, has_fangs, is_scary, is_small, associated_with_vampires]   \n",
       "1                                        [has_wings, beh_-_flies, beh_-_is_nocturnal, is_black, lives_in_caves, has_fur, an_animal, beh_-_sleeps_upside_down, is_blind, beh_-_uses_radar_to_navigate, beh_-_screeches, a_mammal, has_fangs, is_scary, is_small, associated_with_vampires]   \n",
       "2                                        [has_wings, beh_-_flies, beh_-_is_nocturnal, is_black, lives_in_caves, has_fur, an_animal, beh_-_sleeps_upside_down, is_blind, beh_-_uses_radar_to_navigate, beh_-_screeches, a_mammal, has_fangs, is_scary, is_small, associated_with_vampires]   \n",
       "3                                        [has_wings, beh_-_flies, beh_-_is_nocturnal, is_black, lives_in_caves, has_fur, an_animal, beh_-_sleeps_upside_down, is_blind, beh_-_uses_radar_to_navigate, beh_-_screeches, a_mammal, has_fangs, is_scary, is_small, associated_with_vampires]   \n",
       "4                                        [has_wings, beh_-_flies, beh_-_is_nocturnal, is_black, lives_in_caves, has_fur, an_animal, beh_-_sleeps_upside_down, is_blind, beh_-_uses_radar_to_navigate, beh_-_screeches, a_mammal, has_fangs, is_scary, is_small, associated_with_vampires]   \n",
       "..                                                                                                                                                                                                                                                                                    ...   \n",
       "95  [is_long, is_green, used_for_watering_gardens, inbeh_-_water_flows_through_it, used_for_watering_lawns, is_round, used_by_firemen, made_of_rubber, used_with_sprinklers, found_outdoors, has_a_nozzle, inbeh_-_sprays_water, is_thin, made_of_plastic, a_tube, used_for_washing_cars]   \n",
       "96  [is_long, is_green, used_for_watering_gardens, inbeh_-_water_flows_through_it, used_for_watering_lawns, is_round, used_by_firemen, made_of_rubber, used_with_sprinklers, found_outdoors, has_a_nozzle, inbeh_-_sprays_water, is_thin, made_of_plastic, a_tube, used_for_washing_cars]   \n",
       "97  [is_long, is_green, used_for_watering_gardens, inbeh_-_water_flows_through_it, used_for_watering_lawns, is_round, used_by_firemen, made_of_rubber, used_with_sprinklers, found_outdoors, has_a_nozzle, inbeh_-_sprays_water, is_thin, made_of_plastic, a_tube, used_for_washing_cars]   \n",
       "98  [is_long, is_green, used_for_watering_gardens, inbeh_-_water_flows_through_it, used_for_watering_lawns, is_round, used_by_firemen, made_of_rubber, used_with_sprinklers, found_outdoors, has_a_nozzle, inbeh_-_sprays_water, is_thin, made_of_plastic, a_tube, used_for_washing_cars]   \n",
       "99  [is_long, is_green, used_for_watering_gardens, inbeh_-_water_flows_through_it, used_for_watering_lawns, is_round, used_by_firemen, made_of_rubber, used_with_sprinklers, found_outdoors, has_a_nozzle, inbeh_-_sprays_water, is_thin, made_of_plastic, a_tube, used_for_washing_cars]   \n",
       "\n",
       "         cos  avg_top_10_prec  avg_top_20_prec  avg_top_k_prec  \\\n",
       "0   0.323878         0.215021         0.201111        0.213409   \n",
       "1   0.333446         0.215021         0.201111        0.213409   \n",
       "2   0.364130         0.215021         0.201111        0.213409   \n",
       "3   0.281827         0.215021         0.201111        0.213409   \n",
       "4   0.429560         0.215021         0.201111        0.213409   \n",
       "..       ...              ...              ...             ...   \n",
       "95  0.147991         0.215021         0.201111        0.213409   \n",
       "96  0.203672         0.215021         0.201111        0.213409   \n",
       "97  0.220974         0.215021         0.201111        0.213409   \n",
       "98  0.199526         0.215021         0.201111        0.213409   \n",
       "99  0.215315         0.215021         0.201111        0.213409   \n",
       "\n",
       "    avg_correlation  avg_cosine  \\\n",
       "0          0.055783    0.241126   \n",
       "1          0.055783    0.241126   \n",
       "2          0.055783    0.241126   \n",
       "3          0.055783    0.241126   \n",
       "4          0.055783    0.241126   \n",
       "..              ...         ...   \n",
       "95         0.055783    0.241126   \n",
       "96         0.055783    0.241126   \n",
       "97         0.055783    0.241126   \n",
       "98         0.055783    0.241126   \n",
       "99         0.055783    0.241126   \n",
       "\n",
       "                                                                                             model  \n",
       "0   ../trained_models/model.ffnn.mc_rae_real.allbuthomonyms.5k.50epochs.0.5dropout.lr1e-4.hsize300  \n",
       "1   ../trained_models/model.ffnn.mc_rae_real.allbuthomonyms.5k.50epochs.0.5dropout.lr1e-4.hsize300  \n",
       "2   ../trained_models/model.ffnn.mc_rae_real.allbuthomonyms.5k.50epochs.0.5dropout.lr1e-4.hsize300  \n",
       "3   ../trained_models/model.ffnn.mc_rae_real.allbuthomonyms.5k.50epochs.0.5dropout.lr1e-4.hsize300  \n",
       "4   ../trained_models/model.ffnn.mc_rae_real.allbuthomonyms.5k.50epochs.0.5dropout.lr1e-4.hsize300  \n",
       "..                                                                                             ...  \n",
       "95  ../trained_models/model.ffnn.mc_rae_real.allbuthomonyms.5k.50epochs.0.5dropout.lr1e-4.hsize300  \n",
       "96  ../trained_models/model.ffnn.mc_rae_real.allbuthomonyms.5k.50epochs.0.5dropout.lr1e-4.hsize300  \n",
       "97  ../trained_models/model.ffnn.mc_rae_real.allbuthomonyms.5k.50epochs.0.5dropout.lr1e-4.hsize300  \n",
       "98  ../trained_models/model.ffnn.mc_rae_real.allbuthomonyms.5k.50epochs.0.5dropout.lr1e-4.hsize300  \n",
       "99  ../trained_models/model.ffnn.mc_rae_real.allbuthomonyms.5k.50epochs.0.5dropout.lr1e-4.hsize300  \n",
       "\n",
       "[100 rows x 17 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Buchanan models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************\n",
      "*** Evaluating ../trained_models/model.plsr.buchanan.allbuthomonyms.5k.300components.500max_iters model ***\n",
      "****************************************\n",
      "Average cosine between gold and predicted feature norms: 0.31045676424942326\n",
      "average Percentage (%) of gold gold-standard features retrieved in the top 10 features of the predicted vector:  0.3\n",
      "average Percentage (%) of gold gold-standard features retrieved in the top 20 features of the predicted vector:  0.23076923076923078\n",
      "Average % @k (derby metric) 0.3076923076923077\n",
      "correlation between gold and predicted vectors: 0.07929599521142182 \n",
      "total number of predictions:  130\n",
      "****************************************\n",
      "*** Evaluating ../trained_models/model.plsr.buchanan.allbuthomonyms.1k.300components.500max_iters model ***\n",
      "****************************************\n",
      "Average cosine between gold and predicted feature norms: 0.2873325056371902\n",
      "average Percentage (%) of gold gold-standard features retrieved in the top 10 features of the predicted vector:  0.3\n",
      "average Percentage (%) of gold gold-standard features retrieved in the top 20 features of the predicted vector:  0.23076923076923078\n",
      "Average % @k (derby metric) 0.23076923076923078\n",
      "correlation between gold and predicted vectors: 0.07379963201829273 \n",
      "total number of predictions:  130\n",
      "****************************************\n",
      "*** Evaluating ../trained_models/model.ffnn.buchanan.allbuthomonyms.5k.50epochs.0.5dropout.lr1e-4.hsize300 model ***\n",
      "****************************************\n",
      "Average cosine between gold and predicted feature norms: 0.30241336028761373\n",
      "average Percentage (%) of gold gold-standard features retrieved in the top 10 features of the predicted vector:  0.1\n",
      "average Percentage (%) of gold gold-standard features retrieved in the top 20 features of the predicted vector:  0.07692307692307693\n",
      "Average % @k (derby metric) 0.23076923076923078\n",
      "correlation between gold and predicted vectors: 0.07330876728020179 \n",
      "total number of predictions:  130\n",
      "****************************************\n",
      "*** Evaluating ../trained_models/model.ffnn.buchanan.allbuthomonyms.1k.50epochs.0.5dropout.lr1e-4.hsize300 model ***\n",
      "****************************************\n",
      "Average cosine between gold and predicted feature norms: 0.3146712663925684\n",
      "average Percentage (%) of gold gold-standard features retrieved in the top 10 features of the predicted vector:  0.3\n",
      "average Percentage (%) of gold gold-standard features retrieved in the top 20 features of the predicted vector:  0.23076923076923078\n",
      "Average % @k (derby metric) 0.23076923076923078\n",
      "correlation between gold and predicted vectors: 0.07241094780420883 \n",
      "total number of predictions:  130\n",
      "****************************************\n",
      "*** Evaluating ../trained_models/model.plsr.buchanan.allbuthomonyms.glove.300components.300max_iters model ***\n",
      "****************************************\n",
      "Average cosine between gold and predicted feature norms: 0.26025731510025224\n",
      "average Percentage (%) of gold gold-standard features retrieved in the top 10 features of the predicted vector:  0.3\n",
      "average Percentage (%) of gold gold-standard features retrieved in the top 20 features of the predicted vector:  0.23076923076923078\n",
      "Average % @k (derby metric) 0.38461538461538464\n",
      "correlation between gold and predicted vectors: 0.07046067925574014 \n",
      "total number of predictions:  130\n",
      "****************************************\n",
      "*** Evaluating ../trained_models/model.ffnn.buchanan.allbuthomonyms.glove.50epochs.0.5dropout.lr1e-4.hsize300 model ***\n",
      "****************************************\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9m/vzvx58rs51v_x5nm620fz4xr0000gn/T/ipykernel_33368/4190633328.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"*** Evaluating %s model ***\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"****************************************\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_in_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglove\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'buchanan'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mdfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    605\u001b[0m                     \u001b[0mopened_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_position\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m    880\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnpicklerWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mfind_class\u001b[0;34m(self, mod_name, name)\u001b[0m\n\u001b[1;32m    873\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfind_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m             \u001b[0mmod_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_module_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 875\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m     \u001b[0;31m# Load the data (which may in turn use `persistent_load` to load tensors)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lib'"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "\n",
    "for save_path in buchanan_models:\n",
    "    print(\"****************************************\")\n",
    "    print(\"*** Evaluating %s model ***\" % save_path)\n",
    "    print(\"****************************************\")\n",
    "    model = torch.load(save_path)\n",
    "    df = evaluate_in_context(model, eval_words, bert, model_name=save_path, dataset='buchanan')\n",
    "    dfs.append(df)\n",
    "\n",
    "for save_path in buchanan_glove_models:\n",
    "    print(\"****************************************\")\n",
    "    print(\"*** Evaluating %s model ***\" % save_path)\n",
    "    print(\"****************************************\")\n",
    "    model = torch.load(save_path)\n",
    "    df = evaluate_in_context(model, eval_words, bert, glove=True, model_name=save_path, dataset='buchanan')\n",
    "    dfs.append(df)\n",
    "\n",
    "\n",
    "df = pd.concat(dfs)\n",
    "df.to_csv('../results/homonymous_words_eval_buchanan_models_expanded_10_20_22.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../results/homonymous_words_eval_mcrae_models_expanded_10_20_22.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9m/vzvx58rs51v_x5nm620fz4xr0000gn/T/ipykernel_33368/319823370.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmcrae_res_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../results/homonymous_words_eval_mcrae_models_expanded_10_20_22.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmcrae_res_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmcrae_res_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'Unnamed: 0'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'token_index'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmcrae_res_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../results/homonymous_words_eval_mcrae_models_expanded_10_20_22.csv'"
     ]
    }
   ],
   "source": [
    "mcrae_res_df = pd.read_csv('../results/homonymous_words_eval_mcrae_models_expanded_10_20_22.csv')\n",
    "mcrae_res_df = mcrae_res_df.rename(columns={'Unnamed: 0': 'token_index'})\n",
    "mcrae_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which model does best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which words get highest precision? lowest?\n",
    "\n",
    "print(mcrae_res_df.model.unique())\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.colheader_justify', 'center')\n",
    "pd.set_option('display.precision', 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcrae_res_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get best and worst performing words on plsr 5k --- avg prec .5\n",
    "print(mcrae_res_df[mcrae_res_df.model == '../trained_models/model.plsr.mc_rae_real.allbuthomoyms.5k.100components.500max_iters'\n",
    "].sort_values(by=['top_k_prec'], ascending=False).head(5))\n",
    "\n",
    "print(mcrae_res_df[mcrae_res_df.model == '../trained_models/model.plsr.mc_rae_real.allbuthomoyms.5k.100components.500max_iters'\n",
    "].sort_values(by=['top_k_prec'], ascending=True).head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get best and worst performing words on ffnn 1k --- avg prec .5\n",
    "print(mcrae_res_df[mcrae_res_df.model == '../trained_models/model.ffnn.mc_rae_real.allbuthomoyms.1k.50epochs.0.5dropout.lr1e-4.hsize300'\n",
    "].sort_values(by=['top_k_prec'], ascending=False).head(5))\n",
    "\n",
    "print(mcrae_res_df[mcrae_res_df.model == '../trained_models/model.ffnn.mc_rae_real.allbuthomoyms.1k.50epochs.0.5dropout.lr1e-4.hsize300'\n",
    "].sort_values(by=['top_k_prec'], ascending=True).head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get best and worst performing words on glove ffnn\n",
    "print(mcrae_res_df[mcrae_res_df.model == '../trained_models/model.ffnn.mc_rae_real.allbuthomoyms.glove.50epochs.0.5dropout.lr1e-4.hsize300'\n",
    "].sort_values(by=['top_k_prec'], ascending=False).head(5))\n",
    "\n",
    "print(mcrae_res_df[mcrae_res_df.model == '../trained_models/model.ffnn.mc_rae_real.allbuthomoyms.glove.50epochs.0.5dropout.lr1e-4.hsize300'\n",
    "].sort_values(by=['top_k_prec'], ascending=True).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get best and worst performing words on glove plsr\n",
    "print(mcrae_res_df[mcrae_res_df.model == '../trained_models/model.plsr.mc_rae_real.allbuthomoyms.glove.100components.300max_iters'\n",
    "].sort_values(by=['top_k_prec'], ascending=False).head(5))\n",
    "\n",
    "print(mcrae_res_df[mcrae_res_df.model == '../trained_models/model.plsr.mc_rae_real.allbuthomoyms.glove.100components.300max_iters'\n",
    "].sort_values(by=['top_k_prec'], ascending=True).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at differential features for tank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look side by side at the predictions for a few homonymous pairs, across all models.\n",
    "# we'll want to look at the gold features, the features predicted, and the scores, for each of the models\n",
    "\n",
    "# tank\n",
    "# army = 54\n",
    "# container = 51\n",
    "\n",
    "# get gold feats for tank\n",
    "# print(mcrae_res_df[mcrae_res_df.token_index in (54, 51)][:1][['lemma','context', 'gold_feats']])\n",
    "mcrae_res_df.loc[mcrae_res_df['token_index'].isin([54,51])][['lemma','context', 'gold_feats']][:2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predicted feats for each of the models\n",
    "mcrae_res_df.loc[mcrae_res_df['token_index'].isin([54,51])][['model','lemma','top_10_prec', 'top_10']].sort_values(by='model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at differentia features for pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look side by side at the predictions for a few homonymous pairs, across all models.\n",
    "# we'll want to look at the gold features, the features predicted, and the scores, for each of the models\n",
    "\n",
    "# pipe\n",
    "# smoke = 38\n",
    "# plumbing = 41\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predicted feats for each of the models\n",
    "mcrae_res_df.loc[mcrae_res_df['token_index'].isin([38,41])][['model','lemma','top_10_prec', 'top_10']].sort_values(by='model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlate predicted features with wu-palmer similarity\n",
    "\n",
    "use homonym token dataset\n",
    " lemma cue_word context \n",
    " \n",
    "add label row to df\n",
    "\n",
    "Get predictions on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/bnc_contexts_for_mcrae_homonyms.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform wordnet string into wordnet lemma\n",
    "\n",
    "ws = []\n",
    "wn_lemmas = []\n",
    "for index, row in df.iterrows():\n",
    "    lemma = re.findall(r\"'(.*?)'\", row.lemma)[0]\n",
    "    lemma = wn.lemma(lemma)\n",
    "    \n",
    "    word = lemma.name()\n",
    "    #print(word)\n",
    "    ws.append(word)\n",
    "    wn_lemmas.append(lemma)\n",
    "    \n",
    "df['label'] = ws\n",
    "df['wn_lemma'] = wn_lemmas\n",
    "\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_wu_palmer_analysis(df):\n",
    "    \"\"\"\n",
    "    input is a dataframe with columns\n",
    "        cue_word\n",
    "        lemma\n",
    "        context\n",
    "        label\n",
    "    \"\"\"\n",
    "    wup_sims = []\n",
    "    cossine_sims = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        word = row.label\n",
    "\n",
    "        # select other examples of this token\n",
    "        othertokens = df[df.label == word]\n",
    "        # filter out the token itself\n",
    "        othertokens = othertokens[othertokens.index != index]\n",
    "        #print(othertokens)\n",
    "\n",
    "        for index, otherword in othertokens.iterrows():\n",
    "            # find the wordnet distance between these two wordnet senses\n",
    "            synset1 = row.wn_lemma.synset()\n",
    "            synset2 = otherword.wn_lemma.synset()\n",
    "\n",
    "            wup_sim = synset1.wup_similarity(synset2)\n",
    "            wup_sims.append(wup_sim)\n",
    "            cossim = 1 - cosine(row.predictions, otherword.predictions)\n",
    "            #cossim = 1 - cosine(row.single_prototype_model_preds, otherword.single_prototype_model_preds)\n",
    "            cossine_sims.append(cossim)\n",
    "            #print(synset1)\n",
    "            #print(synset2)\n",
    "            #print(wup_sim)\n",
    "            #print(cossim)\n",
    "    return (wup_sims, cossine_sims)\n",
    "\n",
    "def plot_sims(wup_sims, cossine_sims):\n",
    "    plt.scatter(wup_sims, cossine_sims)\n",
    "    plt.title(\"Wordnet similarity of homonymous senses plotted against cosine similarity of predicted vectors of two tokens in semantic feature space\")\n",
    "    plt.xlabel(\"Wu and Palmer Similarity\")\n",
    "    plt.ylabel(\"Cosine Similarity\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "create a list of predicted feature vectors for each of the models being evaluated\n",
    "and then stick them in the dataframe\n",
    "\"\"\"\n",
    "\n",
    "for save_path in mcrae_models:\n",
    "    print(\"****************************************\")\n",
    "    print(\"*** doing wu palmer correlation for %s ***\" % save_path)\n",
    "    print(\"****************************************\")\n",
    "    model = torch.load(save_path)\n",
    "    \n",
    "    predictions = []\n",
    "    for index, row in df.iterrows():\n",
    "            singular = row.wn_lemma.name()\n",
    "            plural = pluralize(singular)\n",
    "\n",
    "            try:\n",
    "                predicted_vector = model.predict_in_context(singular, row.context, bert)\n",
    "            except:\n",
    "                predicted_vector = model.predict_in_context(plural, row.context, bert)\n",
    "\n",
    "\n",
    "            predictions.append(predicted_vector)\n",
    "\n",
    "    df['predictions'] = predictions\n",
    "    \n",
    "    wup_sims, cossine_sims = run_wu_palmer_analysis(df)\n",
    "    plot_sims(wup_sims, cossine_sims)\n",
    "\n",
    "    corr, p = pearsonr(wup_sims, cossine_sims)\n",
    "    print('Pearsons correlation: %.3f, p-value: %s'  % (corr, p))\n",
    "\n",
    "    corr, p = spearmanr(wup_sims, cossine_sims)\n",
    "    print('Spearmans correlation: %.3f, p-value: %s'  % (corr, p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the analysis for the non-contextual model (trained on GloVe)\n",
    "\n",
    "Here we expect to see no correlation, because the model should be making the same prediction for every token of a word form, irrespective of the surrounding context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "create a list of predicted feature vectors for each of the models being evaluated\n",
    "and then stick them in the dataframe\n",
    "\"\"\"\n",
    "\n",
    "for save_path in mcrae_glove_models:\n",
    "    print(\"****************************************\")\n",
    "    print(\"*** doing wu palmer correlation for %s ***\" % save_path)\n",
    "    print(\"****************************************\")\n",
    "    model = torch.load(save_path)\n",
    "    \n",
    "    predictions = []\n",
    "    for index, row in df.iterrows():\n",
    "            singular = row.wn_lemma.name()\n",
    "            plural = pluralize(singular)\n",
    "\n",
    "            try:\n",
    "                predicted_vector = model.predict_in_context(singular, row.context, bert, glove=True)\n",
    "            except:\n",
    "                predicted_vector = model.predict_in_context(plural, row.context, bert, glove=True)\n",
    "\n",
    "\n",
    "            predictions.append(predicted_vector)\n",
    "\n",
    "    df['predictions'] = predictions\n",
    "    \n",
    "    wup_sims, cossine_sims = run_wu_palmer_analysis(df)\n",
    "    plot_sims(wup_sims, cossine_sims)\n",
    "\n",
    "    corr, p = pearsonr(wup_sims, cossine_sims)\n",
    "    print('Pearsons correlation: %.3f, p-value: %s'  % (corr, p))\n",
    "\n",
    "    corr, p = spearmanr(wup_sims, cossine_sims)\n",
    "    print('Spearmans correlation: %.3f, p-value: %s'  % (corr, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_list = []\n",
    "word_indexer = Indexer()\n",
    "with open(\"../data/glove.6B/glove.6B.300d.txt\", 'r') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeddings_list.append([vector])\n",
    "\n",
    "        #print(embeddings_dict)\n",
    "        #raise Exception(\"hfelfnl\")\n",
    "        word_indexer.add_and_get_index(word)\n",
    "\n",
    "embs = MultiProtoTypeEmbeddings(word_indexer, np.array(embeddings_list), 0, 1)\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
