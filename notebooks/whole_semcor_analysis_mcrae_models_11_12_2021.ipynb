{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data\n",
    "\n",
    "First we will create a dictionary of Semcor words, and look at them and their frequencies.\n",
    "\n",
    "\n",
    "Next, we want to create a dataset of a subsample of semcor. We want to remove the most common and least common words\n",
    "\n",
    "\n",
    "We limit this set in several ways:\n",
    "    - only noun senses\n",
    "    - max 30 examples of each sense of a word.\n",
    "    - concrete\n",
    "    - remove nominalizations, which tend to have eventive readings (we are interested in nouns denoting entities)\n",
    "\n",
    "So, we begin iterating through a randomly shuffled semcor. For each word, we throw it out if it does not fit our criteria. Then, we look at the senses.\n",
    "\n",
    "\n",
    "\n",
    "At the end, we store a list of all of the words we've collected. For each item in the dictionary, we should know:\n",
    "- the number of tokens\n",
    "- the wordnet senses\n",
    "- a list of the semcor sentence indices of the tokens of each word. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.modeling:loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /Users/gabriellachronis/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "INFO:pytorch_pretrained_bert.modeling:extracting archive file /Users/gabriellachronis/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /var/folders/9m/vzvx58rs51v_x5nm620fz4xr0000gn/T/tmpy4hhbilk\n",
      "INFO:pytorch_pretrained_bert.modeling:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/gabriellachronis/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../lib/\")\n",
    "\n",
    "from nltk.corpus import semcor\n",
    "from nltk.tree import Tree\n",
    "import itertools\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "from bert import *\n",
    "import csv\n",
    "from nltk.corpus.reader.wordnet import Lemma\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import inflect\n",
    "import os\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "import re\n",
    "\n",
    "\n",
    "bert = BERTBase()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize(\"impressed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "load semcor stats\n",
    "\"\"\"\n",
    "\n",
    "#uncomment for whole dataset\n",
    "sents = semcor.sents()\n",
    "tagged_sents = semcor.tagged_sents( tag = ' sem ' )\n",
    "words = semcor.words()\n",
    "\n",
    "\n",
    "##########\n",
    "# DEBUG ONLY\n",
    "############\n",
    "\n",
    "# tagged_sents = semcor.tagged_sents( tag = ' sem ' )[:20]\n",
    "# sents = semcor.sents()[:20]\n",
    "# words = semcor.words()[:1000]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lala = semcor.tagged_sents( tag = ' sem ' )[:20]\n",
    "lala = lala[0][1]\n",
    "lala.pos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_senses_in_tagged_sentence(tagged_sentence):\n",
    "    \"\"\"\n",
    "    given a sense-tagged corpus sentence,returns a list of lemmas and senses in that sentence\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for chunk in tagged_sentence:\n",
    "        \n",
    "        \n",
    "        chunk_string = ' '.join(chunk.leaves())\n",
    "\n",
    "        word = chunk_string.lower()\n",
    "        lemma = lemmatizer.lemmatize(word)\n",
    "        poss = chunk.pos()\n",
    "        \n",
    "        \"\"\"\n",
    "        if we find a wordnet sense (function words dont)\n",
    "        then scoop it up\n",
    "\n",
    "        \"\"\"            \n",
    "        if isinstance(chunk.label() , Lemma):\n",
    "            sense = chunk.label()\n",
    "            for wordform, pos in poss:\n",
    "                res.append((lemma, sense, pos))\n",
    "    # if we get to the end of the loop. we didn't find the word we were looking for\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sents[0])\n",
    "get_senses_in_tagged_sentence(tagged_sents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "little script on semcor_lexicon to see where we should cut off most and least frequent\n",
    "\"\"\"\n",
    "\n",
    "print(semcor_lexicon.most_common(200))\n",
    "n = 30000\n",
    "print(semcor_lexicon.most_common()[:-n-1:-1])\n",
    "\n",
    "# we want to keep words with a count < 600\n",
    "\n",
    "# and with a count greater than > 10 (which is knocking off the l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "get basic semcor stats\n",
    "\"\"\"\n",
    "print(\"number of sentences:\")\n",
    "print(len(sents))\n",
    "print(\"number of tokens:\")\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Create Token Index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Next step is to create an index of all of the tokens of a single lemma. \n",
    "So, we build a data structure with all of the word forms found in semcor. With each word form,\n",
    "we store a list of all of the sentences containing it.\n",
    "\"\"\"\n",
    "\n",
    "class Vividict(dict):\n",
    "    def __missing__(self, key):\n",
    "        value = self[key] = type(self)() # retain local pointer to value\n",
    "        return value                     # faster to return than dict lookup\n",
    "\n",
    "\n",
    "#word_index = {}\n",
    "sense_index = Vividict()\n",
    "\n",
    "semcor_indices = list(range(0,len(sents)))\n",
    "#print(semcor_indices)\n",
    "random.shuffle(semcor_indices)\n",
    "#print(semcor_indices)\n",
    "\n",
    "\n",
    "# go through the dataset sentence by sentence\n",
    "for random_index in semcor_indices:\n",
    "\n",
    "    sentence_id = random_index\n",
    "    sent = tagged_sents[sentence_id]\n",
    "\n",
    "    \n",
    "    # go through the sentence word by word to get semcor senses in it\n",
    "    for word in sent:\n",
    "        senses = get_senses_in_tagged_sentence(sent)\n",
    "        for lemma, sense, pos in senses:\n",
    "            sense = str(sense)\n",
    "            \n",
    "            if pos != 'NN':\n",
    "                continue\n",
    "            # if this is our first time seeing this word, add it to the index and put the sentence id in the entry\n",
    "            elif sense not in sense_index[lemma]:\n",
    "                sense_index[lemma][sense] = {sentence_id}\n",
    "            # if we have too many instances of this sense, stop\n",
    "            elif len(sense_index[lemma][sense]) >= 30:\n",
    "                continue\n",
    "            # otherwise add it\n",
    "            else:\n",
    "                sense_index[lemma][sense].add(sentence_id)\n",
    "        \n",
    "#     # we need to make sure we are collecting only those tokens which have semcor senses, or we make note of which ones do\n",
    "    \n",
    "#         # if this is our first time seeing this word, add it to the index and put the sentence id in the entry\n",
    "#         if word not in word_index:\n",
    "#             word = word.lower()\n",
    "#             word_index[word] = {sentence_id}\n",
    "#         # otherwise, add the sentence id to the entry for the word\n",
    "#         else:\n",
    "#             word_index[word].add(sentence_id)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "let's take a look at it\n",
    "\"\"\"\n",
    "import pprint\n",
    "#pprint.pprint(sense_index, width=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "this is a little nonsense to figure out how to use nltk lemma types - - not needed for script\n",
    "\"\"\"\n",
    "\n",
    "render1 = wn.lemma('render.v.07.return')\n",
    "render2 = wn.lemma('return.v.01.return')\n",
    "\n",
    "# \"\"\"\n",
    "# importnt point about nltk wordnet lemmas. their representation is confusing so be careful. i think equals or differentequals are implmementd in\n",
    "# unsuspected ways, because you get issues where they dont act like their display name\n",
    "# \"\"\"\n",
    "\n",
    "dictz = {render1: \"foo\", render2: \"bar\"}\n",
    "print(dictz)\n",
    "\n",
    "dixt = {str(render1): \"foo\"}\n",
    "dixt[str(render2)] = \"bar\"\n",
    "print(dixt)\n",
    "\n",
    "\n",
    "dixt = {str(render1): \"foo\"}\n",
    "dixt[str(render2)] = \"bar\"\n",
    "print(dixt)\n",
    "\n",
    "#re.findall(r\"\\.(.*?)\\.\", 'render.v.07.return')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now that we have our word index, we want to construct the evaluation dataset\n",
    "\n",
    "for each word in the index, we want\n",
    "\n",
    "We iterate through the words in the dictionary.\n",
    "we shuffle these indices and access in random order. \n",
    "We go through the shuffled indices,\n",
    "    and we check if we have collected < 50 of this sense.\n",
    "    if not, we collect this token for the evaluation dataset\n",
    "\n",
    "collection means:\n",
    "    we construct a row of data like\n",
    "        word lemma\n",
    "        word sense\n",
    "        token sentence\n",
    "        \n",
    "        \n",
    "at the end we save the data in a csv file called 'semcor_wu_palmer_eval_data.csv'\n",
    "\"\"\"\n",
    "\n",
    "def get_sense_in_tagged_sentence(word, tagged_sentence):\n",
    "    for chunk in tagged_sentence:\n",
    "\n",
    "        chunk_string = ' '.join(chunk.leaves())\n",
    "\n",
    "        \"\"\"\n",
    "        if we find the word we're looking for in this chunk,\n",
    "        and that chunk has a wordnet sense (function words dont)\n",
    "        then scoop it up\n",
    "\n",
    "        \"\"\"            \n",
    "        if chunk_string.lower() == word:\n",
    "            #print(\"found %s\" % word)\n",
    "            #print(chunk.label())\n",
    "\n",
    "            #wn_lemma = cunk.label()\n",
    "            if isinstance(chunk.label() , Lemma):\n",
    "                return chunk.label()\n",
    "    # if we get to the end of the loop. we didn't find the word we were looking for\n",
    "    return None\n",
    "\n",
    "\n",
    "def collect_tokens(indices, sents, tagged_sents):\n",
    "    \"\"\"\n",
    "    takes a word and a list of indices\n",
    "    returns tuples containing \n",
    "        word\n",
    "        sentence_string\n",
    "        sense\n",
    "    \"\"\"\n",
    "    #sense_count = 0\n",
    "    tokens = []\n",
    "    \n",
    "    # indices is a list of all of the sentence ids containing this word\n",
    "    indices = list(indices)\n",
    "    # visit these sentences in random order\n",
    "    random.shuffle(indices)\n",
    "    for index in indices[:25]:\n",
    "\n",
    "        sentence = sents[index]\n",
    "        sentence = ' '.join(sentence)\n",
    "\n",
    "        tokens.append(sentence)\n",
    "        #sense_count += 1\n",
    "        \n",
    "        \n",
    "    \n",
    "    #print(sense_count.items())\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def collect_tokens_for_all_words_to_file(path, sense_path, sense_index, sents, tagged_sents):\n",
    "    with open(path, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "\n",
    "        with open(sense_path, 'w', newline='') as sensefile:\n",
    "            sensewriter = csv.writer(sensefile)\n",
    "        \n",
    "            for lemma in sense_index.keys():\n",
    "                #print(lemma)\n",
    "                for sense, indices in sense_index[lemma].items():\n",
    "                    print(lemma)\n",
    "                    print(sense)\n",
    "                    #print(indices)\n",
    "\n",
    "                \n",
    "                    frequency = len(indices)\n",
    "                    tokens = collect_tokens(indices, sents, tagged_sents)\n",
    "                    #print(tokens)\n",
    "                    #raise Exception(\"nfwip\")\n",
    "\n",
    "                    for token in tokens:\n",
    "                        row = (lemma, sense, token)\n",
    "                        writer.writerow(row)                    \n",
    "                    #sensewriter.writerow(sense_count.items())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "collect_tokens_for_all_words_to_file('../data/semcor_wu_palmer_eval_datamcrae.csv', '../data/semcor_sense_counts_mcrae.csv', sense_index, sents, tagged_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "models and save paths\n",
    "\n",
    " (mislabeled as buchanan.mc_rae but really mc_rae)\n",
    "\"\"\"\n",
    "# models = [\n",
    "#     'trained_models/model.plsr.buchanan.mc_rae_real.allbuthomoyms.5k.300components.500max_iters',\n",
    "#     'trained_models/model.plsr.buchanan.mc_rae_real.allbuthomoyms.1k.300components.500max_iters',\n",
    "#     #'trained_models/model.plsr.buchanan.mc_rae_real.allbuthomoyms.glove.300components.300max_iters',\n",
    "#     'trained_models/model.ffnn.buchanan.mc_rae_real.allbuthomoyms.5k.50epochs.0.5dropout.lr1e-4.hsize300',\n",
    "#     'trained_models/model.ffnn.buchanan.mc_rae_real.allbuthomoyms.1k.50epochs.0.5dropout.lr1e-4.hsize300',\n",
    "#     #'trained_models/model.ffnn.buchanan.mc_rae_real.allbuthomoyms.glove.50epochs.0.5dropout.lr1e-4.hsize300',\n",
    "#     'trained_models/model.modabs.buchanan.mc_rae_real.allbuthomoyms.5k',\n",
    "#     'trained_models/model.modabs.buchanan.mc_rae_real.allbuthomoyms.1k',\n",
    "#     #'trained_models/model.modabs.buchanan.mc_rae_real.allbuthomoyms.glove'\n",
    "# ]\n",
    "\n",
    "models = [\n",
    "    'trained_models/model.plsr.mc_rae_real.1k.50components.500max_iters',\n",
    "    'trained_models/model.plsr.mc_rae_real.5k.100components.500max_iters',\n",
    "    #'trained_models/model.plsr.mc_rae_real.glove.100components.300max_iters',\n",
    "    'trained_models/model.ffnn.mc_rae_real.1k.50epochs.0.5dropout.lr1e-4.hsize300',\n",
    "    'trained_models/model.ffnn.mc_rae_real.5k.50epochs.0.5dropout.lr1e-4.hsize300',\n",
    "    #'trained_models/model.ffnn.mc_rae_real.glove.50epochs.0.5dropout.lr1e-4.hsize300',\n",
    "    'trained_models/model.modabs.mc_rae_real.1k',\n",
    "    'trained_models/model.modabs.mc_rae_real.5k',\n",
    "    #'trained_models/model.modabs.mc_rae_real.glove'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma_from_string(lemma_string):\n",
    "    # grabs everything in between (' ') in a string\n",
    "    # (needed to update from r\"'(.*?)'\" to deal with cases with quotes in word like o'clock)\n",
    "    string = re.findall(r\"\\('(.*?)'\\)\", lemma_string)[0]\n",
    "    #print(string)\n",
    "    lemma = wn.lemma(string)\n",
    "    return lemma\n",
    "\n",
    "def lemma_name_from_string(lemma_string):\n",
    "    # grabs everything in between (' ') in a string\n",
    "    # (needed to update from r\"'(.*?)'\" to deal with cases with quotes in word like o'clock)\n",
    "    string = re.findall(r\"\\('(.*?)'\\)\", lemma_string)[0]\n",
    "    #print(string)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now, we have our dataset that we want to analyze. We just need to do:\n",
    "\n",
    "for each model we want to evaluate, run the following script:\n",
    "\n",
    "open the file of data\n",
    "\n",
    "read it in as a dataframe\n",
    "\n",
    "for each of the unique words in that dataset\n",
    "\n",
    "    we calculate pairwise distances between each otoken and every otehr token\n",
    "    and construct a similarities dataset. \n",
    "    \n",
    "    then we run correlations for that word???\n",
    "    and store into a file\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def make_predictions(df, model, bert):\n",
    "    predictions = []\n",
    "    for index, row in df.iterrows():\n",
    "\n",
    "        #print(row.word_form)\n",
    "        #print(row.context)\n",
    "\n",
    "        predicted_vector = model.predict_in_context(row.word_form, row.context, bert)\n",
    "\n",
    "        predictions.append(predicted_vector)\n",
    "    return predictions\n",
    "\n",
    "def get_pairwise_wu_palmer_data(model, df, bert, outfile):\n",
    "    unique_words = df.word_form.unique()\n",
    "    \n",
    "    #run_stats = [0] * len(unique_words)\n",
    "    run_stats = []\n",
    "\n",
    "    for i in range(0, len(unique_words)):\n",
    "        if i % 500 == 0:\n",
    "            print(\"processed %s words\" % i)\n",
    "        \n",
    "        # a dataframe containing all the tokens of this word\n",
    "        word = unique_words[i]\n",
    "        word_data = df[df.word_form == word].copy()\n",
    "\n",
    "        n_senses = len(word_data['wn_lemma'].unique())\n",
    "\n",
    "        predictions = make_predictions(word_data, model, bert)\n",
    "        \n",
    "        word_data['prediction'] = predictions\n",
    "        \n",
    "        #print(word_data)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        then we calculate the pairwise distances between all of the vectors, only counting one pair one time\n",
    "\n",
    "        \"\"\"\n",
    "        vals_for_this_word = []\n",
    "#         cosines_for_this_word = []\n",
    "#         wup_sims_for_this_word = []\n",
    "#         sense_1 = []\n",
    "#         sense_2 = []\n",
    "\n",
    "        # pop the first token off the list\n",
    "        num_toks = len(word_data)\n",
    "        for i in range(0,num_toks):\n",
    "            # compare it with each of the other tokens\n",
    "            # dont have to compare to any earlier\n",
    "            for j in range(i+1,num_toks):\n",
    "\n",
    "                #print(df.iloc[i])\n",
    "                #print(df.iloc[j])\n",
    "\n",
    "                # calculate cosine similarity between the two vectors\n",
    "                cos_sim = 1 - cosine(word_data.iloc[i].prediction, word_data.iloc[j].prediction)\n",
    "\n",
    "                # and wu palmer similarity between the two wn lemmas\n",
    "                lemma_1 = lemma_from_string(word_data.iloc[i].wn_lemma)\n",
    "                lemma_2 = lemma_from_string(word_data.iloc[j].wn_lemma)\n",
    "                synset1 = lemma_1.synset()\n",
    "                synset2 = lemma_2.synset()\n",
    "                wup_sim = synset1.wup_similarity(synset2)\n",
    "\n",
    "                # if we can't compute a distance for these senses / recognize them, discard\n",
    "                if type(wup_sim) == float:\n",
    "                    # store this data point into a list\n",
    "                    vals_for_this_word.append((word, lemma_1, lemma_2, cos_sim, wup_sim))\n",
    "        \n",
    "        \n",
    "        token_similarities = pd.DataFrame.from_records(vals_for_this_word, columns = [\"lemma\", \"token_sense_1\", \"token_sense_2\", \"cos_sim\", \"wup_sim\"])\n",
    "        token_similarities['n_senses'] = n_senses\n",
    "        \n",
    "        #print(token_similarities)\n",
    "        token_similarities.to_csv(outfile, mode='a', header=True)\n",
    "        \n",
    "        \n",
    "        # run correlation on this word\n",
    "#         if sense_similarities is not None:\n",
    "#             n = len(word_data)\n",
    "#             n_senses = len(word_data['wn_lemma'].unique())\n",
    "#             #print(word)\n",
    "#             #print(n)\n",
    "#             #print(sense_similarities)\n",
    "#             pearson, pearson_p, spearman, spearman_p = run_correlation(sense_similarities)\n",
    "#             # store the values for this word into a dataframe\n",
    "#             #run_stats[i] = (word, n, pearson, pearson_p, spearman, spearman_p)\n",
    "#             run_stats.append((word, n, n_senses, pearson, pearson_p, spearman, spearman_p))\n",
    "\n",
    "    #res = pd.DataFrame.from_records(run_stats, columns = [\"word\", \"n\", \"n_senses\", \"pearson\", \"pearson_p\", \"spearman\", \"spearman_p\"])\n",
    "    return None\n",
    "\n",
    "    #print(all_sense_similarities)\n",
    "    #return all_sense_similarities\n",
    "        \n",
    "        \n",
    "def run_correlation(sense_similarities):\n",
    "    \"\"\"\n",
    "    :sense_similarities: dataframe with columns\n",
    "        cosine_sims \n",
    "        wup_sims\n",
    "    \"\"\"\n",
    "    if sense_similarities is None:\n",
    "        # not really sure why we're getting none values here it should be impossible\n",
    "        return (float(\"nan\"), float(\"nan\"), float(\"nan\"), float(\"nan\"))\n",
    "    elif len(sense_similarities['wup_sim'].unique()) == 1:\n",
    "        # the correlation will be garbage with a constant y value; skip to avoid warnings\n",
    "        return (float(\"nan\"), float(\"nan\"), float(\"nan\"), float(\"nan\"))\n",
    "\n",
    "    if len(sense_similarities) > 1 :\n",
    "        #print(word)\n",
    "\n",
    "        cos_sims = sense_similarities['cos_sim']\n",
    "        wup_sims = sense_similarities['wup_sim']\n",
    "\n",
    "        pearson, pearson_p = pearsonr(cos_sims, wup_sims )\n",
    "        #print('Pearsons correlation: %.3f, p-value: %s'  % (pearson, pearson_p))\n",
    "\n",
    "        spearman, spearman_p = spearmanr(cos_sims, wup_sims )\n",
    "        #print('Spearmans correlation: %.3f, p-value: %s'  % (spearman, spearman_p))\n",
    "\n",
    "        return (pearson, pearson_p, spearman, spearman_p)\n",
    "    return (float(\"nan\"), float(\"nan\"), float(\"nan\"), float(\"nan\"))\n",
    "\n",
    "def plot_sims():\n",
    "    cos_sims = sense_similarities['cos_sim']\n",
    "    wup_sims = sense_similarities['wup_sim']\n",
    "    plt.scatter(wup_sims, cos_sims)\n",
    "    plt.title(\"Wordnet similarity of homonymous senses plotted against cosine similarity of predicted vectors of two tokens in semantic feature space\")\n",
    "    plt.xlabel(\"Wu and Palmer Similarity\")\n",
    "    plt.ylabel(\"Cosine Similarity\")\n",
    "    plt.show()\n",
    "        \n",
    "\n",
    "df = pd.read_csv('semcor_wu_palmer_eval_data.csv', names = [\"word_form\", \"context\", \"wn_lemma\"])\n",
    "\n",
    "\n",
    "\n",
    "for save_path in models:\n",
    "    print(\"****************************************\")\n",
    "    print(\"*** Evaluating %s model ***\" % save_path)\n",
    "    print(\"****************************************\")\n",
    "    model = torch.load(save_path)\n",
    "    out_path = '../results/semcor_pairwise_data_' + os.path.split(save_path)[1] + '.csv'\n",
    "\n",
    "    # remove results file if exists\n",
    "    if os.path.exists(out_path):\n",
    "        os.remove(out_path)\n",
    "    get_pairwise_wu_palmer_data(model, df, bert, out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brysbaert_filename = \"/Users/gabriellachronis/data/Concreteness_ratings_Brysbaert_et_al_BRM.csv\"\n",
    "concreteness_df = pd.read_csv(brysbaert_filename, sep='\\t')\n",
    "concreteness_df= concreteness_df[[\"Word\", \"Conc.M\"]]\n",
    "concreteness_df = concreteness_df.set_index(\"Word\")\n",
    "concreteness_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now we need to go in and add abstractness value and \n",
    "bin number of senses into polysemy band\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "cols = [\"lemma\", \"token_sense_1\", \"token_sense_2\", \"cos_sim\", \"wup_sim\", \"n_senses\"]\n",
    "\n",
    "for save_path in models:\n",
    "    print(\"****************************************\")\n",
    "    print(\"*** Saturating pairwise data for model: %s ***\" % save_path)\n",
    "    print(\"****************************************\")\n",
    "    infile = '../results/semcor_pairwise_data_' + os.path.split(save_path)[1] + '.csv'\n",
    "\n",
    "    outfile = '../results/saturated_semcor_pairwise_data_' + os.path.split(save_path)[1] + '.csv'\n",
    "\n",
    "    csv_input = pd.read_csv(infile, names=cols)\n",
    "    \n",
    "    # add polysemy bin\n",
    "    csv_input['wn_bin'] = pd.cut(csv_input.n_senses, \n",
    "                        bins = [0, 2.1, 4.1, 6.1, 8.1, 10.1, 20.1, 50.1, 200], labels = False)\n",
    "\n",
    "    # add POS rows\n",
    "    pos1s = []\n",
    "    pos2s = []\n",
    "    sense1s = []\n",
    "    sense2s = []\n",
    "    for index, row in csv_input.iterrows():\n",
    "        pos1 = re.findall(r\"\\.(.*?)\\.\", row.token_sense_1)[0]\n",
    "        pos2 = re.findall(r\"\\.(.*?)\\.\", row.token_sense_2)[0]\n",
    "        pos1s.append(pos1)\n",
    "        pos2s.append(pos2)\n",
    "        sense1 = lemma_name_from_string(row.token_sense_1)\n",
    "        sense2 = lemma_name_from_string(row.token_sense_2)\n",
    "        sense1s.append(sense1)\n",
    "        sense2s.append(sense2)\n",
    "        \n",
    "    csv_input['sense1_pos'] = pos1s\n",
    "    csv_input['sense2_pos'] = pos2s\n",
    "    csv_input['token_sense_1'] = sense_1s\n",
    "    csv_input['token_sense_2'] = sense_2s\n",
    "    \n",
    "    \n",
    "    # add concreteness\n",
    "    csv_input = csv_input.join(concreteness_df, how = \"left\", on = \"lemma\")\n",
    "    \n",
    "    csv_input['conc_bin'] = pd.cut(csv_input['Conc.M'], \n",
    "                        bins = [0, 2.3, 4.5, 10], labels = False)\n",
    "    \n",
    "    # remove token sense columns\n",
    "#     csv_input.drop(['token_sense_1'], axis=1)\n",
    "#     csv_input.drop(['token_sense_2'], axis=1)\n",
    "    \n",
    "    #print(csv_input.where(csv_input['Conc.M'].notnull()))\n",
    "    print(csv_input.head(20))\n",
    "    csv_input.to_csv(outfile, index=False)\n",
    "    \n",
    "    #raise Exception(\"dewfieow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a little test\n",
    "lemma_from_string(csv_input.iloc[0].token_sense_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "and now we make another dataset for each one with the correlations.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def run_correlation(sense_similarities):\n",
    "    \"\"\"\n",
    "    :sense_similarities: dataframe with columns\n",
    "        cosine_sims \n",
    "        wup_sims\n",
    "    \"\"\"\n",
    "    #print(sense_similarities.head())\n",
    "    \n",
    "    if sense_similarities is None:\n",
    "        # not really sure why we're getting none values here it should be impossible\n",
    "        return (float(\"nan\"), float(\"nan\"), float(\"nan\"), float(\"nan\"))\n",
    "    elif len(sense_similarities['wup_sim'].unique()) == 1:\n",
    "        # the correlation will be garbage with a constant y value; skip to avoid warnings\n",
    "        return (float(\"nan\"), float(\"nan\"), float(\"nan\"), float(\"nan\"))\n",
    "\n",
    "    if len(sense_similarities) > 1 :\n",
    "        #print(word)\n",
    "\n",
    "        cos_sims = sense_similarities['cos_sim']\n",
    "        wup_sims = sense_similarities['wup_sim']\n",
    "\n",
    "        pearson, pearson_p = pearsonr(cos_sims, wup_sims )\n",
    "        #print('Pearsons correlation: %.3f, p-value: %s'  % (pearson, pearson_p))\n",
    "\n",
    "        spearman, spearman_p = spearmanr(cos_sims, wup_sims )\n",
    "        #print('Spearmans correlation: %.3f, p-value: %s'  % (spearman, spearman_p))\n",
    "\n",
    "        return (pearson, pearson_p, spearman, spearman_p)\n",
    "    return (float(\"nan\"), float(\"nan\"), float(\"nan\"), float(\"nan\"))\n",
    "\n",
    "def plot_sims():\n",
    "    cos_sims = sense_similarities['cos_sim']\n",
    "    wup_sims = sense_similarities['wup_sim']\n",
    "    plt.scatter(wup_sims, cos_sims)\n",
    "    plt.title(\"Wordnet similarity of homonymous senses plotted against cosine similarity of predicted vectors of two tokens in semantic feature space\")\n",
    "    plt.xlabel(\"Wu and Palmer Similarity\")\n",
    "    plt.ylabel(\"Cosine Similarity\")\n",
    "    plt.show()\n",
    "        \n",
    "# i think this is obsolete 11/16/21\n",
    "# df = pd.read_csv('semcor_wu_palmer_eval_data.csv', names = [\"word_form\", \"context\", \"wn_lemma\"])\n",
    "\n",
    "\n",
    "\n",
    "# for save_path in models:\n",
    "#     print(\"****************************************\")\n",
    "#     print(\"*** Evaluating %s model ***\" % save_path)\n",
    "#     print(\"****************************************\")\n",
    "#     model = torch.load(save_path)\n",
    "#     run_stats = run_wu_palmer_analysis(model, df, bert)\n",
    "#     #run_stats = run_correlation(similarities)\n",
    "#     out_path = 'results/semcor_analysis_' + os.path.split(save_path)[1] + '.csv'\n",
    "#     run_stats.to_csv(out_path)\n",
    "#     print(run_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************\n",
      "*** Running correlation on  trained_models/model.plsr.mc_rae_real.1k.50components.500max_iters model ***\n",
      "****************************************\n",
      "****************************************\n",
      "*** Running correlation on  trained_models/model.plsr.mc_rae_real.5k.100components.500max_iters model ***\n",
      "****************************************\n",
      "****************************************\n",
      "*** Running correlation on  trained_models/model.ffnn.mc_rae_real.1k.50epochs.0.5dropout.lr1e-4.hsize300 model ***\n",
      "****************************************\n",
      "****************************************\n",
      "*** Running correlation on  trained_models/model.ffnn.mc_rae_real.5k.50epochs.0.5dropout.lr1e-4.hsize300 model ***\n",
      "****************************************\n",
      "****************************************\n",
      "*** Running correlation on  trained_models/model.modabs.mc_rae_real.1k model ***\n",
      "****************************************\n",
      "****************************************\n",
      "*** Running correlation on  trained_models/model.modabs.mc_rae_real.5k model ***\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "for save_path in models:\n",
    "\n",
    "    print(\"****************************************\")\n",
    "    print(\"*** Running correlation on  %s model ***\" % save_path)\n",
    "    print(\"****************************************\")\n",
    "    infile = '../results/semcor_pairwise_data_' + os.path.split(save_path)[1] + '.csv'\n",
    "    outfile = '../results/semcor_analysis_' + os.path.split(save_path)[1] + '.csv'\n",
    "\n",
    "    out_data = []\n",
    "    \n",
    "    names = ['index', 'lemma', 'token_sense_1', 'token_sense_2',  'cos_sim',  'wup_sim',  'n_senses', 'wn_bin', 'sense1_pos', 'sense2_pos',  'Conc.M',  'conc_bin']\n",
    "    df = pd.read_csv(infile, names=names)\n",
    "    #print(df.head())\n",
    "\n",
    "    \n",
    "    lemmas = df.lemma.unique()\n",
    "    #print(lemmas[:10])\n",
    "\n",
    "    for word in lemmas:\n",
    "        word_data = df[df.lemma == word]\n",
    "        \n",
    "        n_senses = len(word_data.token_sense_1.unique())\n",
    "        \n",
    "        pearson, pearson_p, spearman, spearman_p = run_correlation(word_data)\n",
    "        row = (word, len(word_data), n_senses, pearson, pearson_p, spearman, spearman_p)\n",
    "        #print(corr)\n",
    "        out_data.append(row)\n",
    "        \n",
    "        #raise Exception(\"hfjesh\")\n",
    "    \n",
    "    out_df = pd.DataFrame.from_records(out_data, columns = ['word', 'n', 'n_senses', 'pearson', 'pearson_p', 'spearman', 'spearman_p'] )\n",
    "    out_df.to_csv(outfile)\n",
    "    \n",
    "    \n",
    "    #for word in \n",
    "    #run_correlation()\n",
    "    #cols = [\"lemma\", \"token_sense_1\", \"token_sense_2\", \"cos_sim\", \"wup_sim\", \"n_senses\"]\n",
    "    #df = csv_input = pd.read_csv(infile, names=cols)\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
