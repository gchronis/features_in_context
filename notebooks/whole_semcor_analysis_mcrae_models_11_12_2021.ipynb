{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data\n",
    "\n",
    "(we're doing this elsewhere now)\n",
    "\n",
    "First we will create a dictionary of Semcor words, and look at them and their frequencies.\n",
    "\n",
    "\n",
    "Next, we want to create a dataset of a subsample of semcor. We want to remove the most common and least common words\n",
    "\n",
    "\n",
    "We limit this set in several ways:\n",
    "    - only noun senses\n",
    "    - max 30 examples of each sense of a word.\n",
    "    - concrete\n",
    "    - remove nominalizations, which tend to have eventive readings (we are interested in nouns denoting entities)\n",
    "\n",
    "So, we begin iterating through a randomly shuffled semcor. For each word, we throw it out if it does not fit our criteria. Then, we look at the senses.\n",
    "\n",
    "\n",
    "\n",
    "At the end, we store a list of all of the words we've collected. For each item in the dictionary, we should know:\n",
    "- the number of tokens\n",
    "- the wordnet senses\n",
    "- a list of the semcor sentence indices of the tokens of each word. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from nltk.corpus import semcor\n",
    "#from nltk.tree import Tree\n",
    "#import itertools\n",
    "#import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "from bert import *\n",
    "import csv\n",
    "from nltk.corpus.reader.wordnet import Lemma\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import inflect\n",
    "import os\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "import re\n",
    "from src.utils import *\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.modeling:loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /Users/gabriellachronis/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "INFO:pytorch_pretrained_bert.modeling:extracting archive file /Users/gabriellachronis/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /var/folders/9m/vzvx58rs51v_x5nm620fz4xr0000gn/T/tmpmv913rzk\n",
      "INFO:pytorch_pretrained_bert.modeling:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/gabriellachronis/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "bert = BERTBase()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatizer = WordNetLemmatizer()\n",
    "# lemmatizer.lemmatize(\"impressed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# load semcor stats\n",
    "# \"\"\"\n",
    "\n",
    "# #uncomment for whole dataset\n",
    "# sents = semcor.sents()\n",
    "# tagged_sents = semcor.tagged_sents( tag = ' sem ' )\n",
    "# words = semcor.words()\n",
    "\n",
    "\n",
    "# ##########\n",
    "# # DEBUG ONLY\n",
    "# ############\n",
    "\n",
    "# # tagged_sents = semcor.tagged_sents( tag = ' sem ' )[:20]\n",
    "# # sents = semcor.sents()[:20]\n",
    "# # words = semcor.words()[:1000]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lala = semcor.tagged_sents( tag = ' sem ' )[:20]\n",
    "# lala = lala[0][1]\n",
    "# lala.pos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_senses_in_tagged_sentence(tagged_sentence):\n",
    "#     \"\"\"\n",
    "#     given a sense-tagged corpus sentence,returns a list of lemmas and senses in that sentence\n",
    "#     \"\"\"\n",
    "#     res = []\n",
    "#     for chunk in tagged_sentence:\n",
    "        \n",
    "        \n",
    "#         chunk_string = ' '.join(chunk.leaves())\n",
    "\n",
    "#         word = chunk_string.lower()\n",
    "#         lemma = lemmatizer.lemmatize(word)\n",
    "#         poss = chunk.pos()\n",
    "        \n",
    "#         \"\"\"\n",
    "#         if we find a wordnet sense (function words dont)\n",
    "#         then scoop it up\n",
    "\n",
    "#         \"\"\"            \n",
    "#         if isinstance(chunk.label() , Lemma):\n",
    "#             sense = chunk.label()\n",
    "#             for wordform, pos in poss:\n",
    "#                 res.append((lemma, sense, pos))\n",
    "#     # if we get to the end of the loop. we didn't find the word we were looking for\n",
    "#     return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sents[0])\n",
    "get_senses_in_tagged_sentence(tagged_sents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# little script on semcor_lexicon to see where we should cut off most and least frequent\n",
    "# \"\"\"\n",
    "\n",
    "# print(semcor_lexicon.most_common(200))\n",
    "# n = 30000\n",
    "# print(semcor_lexicon.most_common()[:-n-1:-1])\n",
    "\n",
    "# # we want to keep words with a count < 600\n",
    "\n",
    "# # and with a count greater than > 10 (which is knocking off the l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# get basic semcor stats\n",
    "# \"\"\"\n",
    "# print(\"number of sentences:\")\n",
    "# print(len(sents))\n",
    "# print(\"number of tokens:\")\n",
    "# print(len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Create Token Index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Next step is to create an index of all of the tokens of a single lemma. \n",
    "# So, we build a data structure with all of the word forms found in semcor. With each word form,\n",
    "# we store a list of all of the sentences containing it.\n",
    "# \"\"\"\n",
    "\n",
    "# class Vividict(dict):\n",
    "#     def __missing__(self, key):\n",
    "#         value = self[key] = type(self)() # retain local pointer to value\n",
    "#         return value                     # faster to return than dict lookup\n",
    "\n",
    "\n",
    "# #word_index = {}\n",
    "# sense_index = Vividict()\n",
    "\n",
    "# semcor_indices = list(range(0,len(sents)))\n",
    "# #print(semcor_indices)\n",
    "# random.shuffle(semcor_indices)\n",
    "# #print(semcor_indices)\n",
    "\n",
    "\n",
    "# # go through the dataset sentence by sentence\n",
    "# for random_index in semcor_indices:\n",
    "\n",
    "#     sentence_id = random_index\n",
    "#     sent = tagged_sents[sentence_id]\n",
    "\n",
    "    \n",
    "#     # go through the sentence word by word to get semcor senses in it\n",
    "#     for word in sent:\n",
    "#         senses = get_senses_in_tagged_sentence(sent)\n",
    "#         for lemma, sense, word_form, pos in senses:\n",
    "#             sense = str(sense)\n",
    "            \n",
    "#             if pos != 'NN':\n",
    "#                 continue\n",
    "#             # if this is our first time seeing this word, add it to the index and put the sentence id in the entry\n",
    "#             elif sense not in sense_index[lemma]:\n",
    "#                 sense_index[lemma][sense][word_form] = {sentence_id}\n",
    "#             # if we have too many instances of this sense, stop\n",
    "#             elif len(sense_index[lemma][sense][word_form]) >= 30:\n",
    "#                 continue\n",
    "#             # otherwise add it\n",
    "#             else:\n",
    "#                 sense_index[lemma][sense][word_form].add(sentence_id)\n",
    "        \n",
    "# #     # we need to make sure we are collecting only those tokens which have semcor senses, or we make note of which ones do\n",
    "    \n",
    "# #         # if this is our first time seeing this word, add it to the index and put the sentence id in the entry\n",
    "# #         if word not in word_index:\n",
    "# #             word = word.lower()\n",
    "# #             word_index[word] = {sentence_id}\n",
    "# #         # otherwise, add the sentence id to the entry for the word\n",
    "# #         else:\n",
    "# #             word_index[word].add(sentence_id)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "let's take a look at it\n",
    "\"\"\"\n",
    "import pprint\n",
    "#pprint.pprint(sense_index, width=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Lemma' object has no attribute 'string'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9m/vzvx58rs51v_x5nm620fz4xr0000gn/T/ipykernel_88747/1199131912.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mrender2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'return.v.01.return'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mrender1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m# # \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# # importnt point about nltk wordnet lemmas. their representation is confusing so be careful. i think equals or differentequals are implmementd in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Lemma' object has no attribute 'string'"
     ]
    }
   ],
   "source": [
    "# \"\"\"\n",
    "# this is a little nonsense to figure out how to use nltk lemma types - - not needed for script\n",
    "# \"\"\"\n",
    "\n",
    "# render1 = wn.lemma('render.v.07.return')\n",
    "# render2 = wn.lemma('return.v.01.return')\n",
    "\n",
    "# # \"\"\"\n",
    "# # importnt point about nltk wordnet lemmas. their representation is confusing so be careful. i think equals or differentequals are implmementd in\n",
    "# # unsuspected ways, because you get issues where they dont act like their display name\n",
    "# # \"\"\"\n",
    "\n",
    "# dictz = {render1: \"foo\", render2: \"bar\"}\n",
    "# print(dictz)\n",
    "\n",
    "# dixt = {str(render1): \"foo\"}\n",
    "# dixt[str(render2)] = \"bar\"\n",
    "# print(dixt)\n",
    "\n",
    "\n",
    "# dixt = {str(render1): \"foo\"}\n",
    "# dixt[str(render2)] = \"bar\"\n",
    "# print(dixt)\n",
    "\n",
    "# #re.findall(r\"\\('(.*?)'\\)\", lemma_string)[0]\n",
    "\n",
    "# #re.findall(r\".*(.*?)'\\)\", 'render.v.07.return')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Now that we have our word index, we want to construct the evaluation dataset\n",
    "\n",
    "# for each word in the index, we want\n",
    "\n",
    "# We iterate through the words in the dictionary.\n",
    "# we shuffle these indices and access in random order. \n",
    "# We go through the shuffled indices,\n",
    "#     and we check if we have collected < 50 of this sense.\n",
    "#     if not, we collect this token for the evaluation dataset\n",
    "\n",
    "# collection means:\n",
    "#     we construct a row of data like\n",
    "#         word lemma\n",
    "#         word sense\n",
    "#         token sentence\n",
    "        \n",
    "        \n",
    "# at the end we save the data in a csv file called 'semcor_wu_palmer_eval_data.csv'\n",
    "# \"\"\"\n",
    "\n",
    "# def get_sense_in_tagged_sentence(word, tagged_sentence):\n",
    "#     for chunk in tagged_sentence:\n",
    "\n",
    "#         chunk_string = ' '.join(chunk.leaves())\n",
    "\n",
    "#         \"\"\"\n",
    "#         if we find the word we're looking for in this chunk,\n",
    "#         and that chunk has a wordnet sense (function words dont)\n",
    "#         then scoop it up\n",
    "\n",
    "#         \"\"\"            \n",
    "#         if chunk_string.lower() == word:\n",
    "#             #print(\"found %s\" % word)\n",
    "#             #print(chunk.label())\n",
    "\n",
    "#             #wn_lemma = cunk.label()\n",
    "#             if isinstance(chunk.label() , Lemma):\n",
    "#                 return chunk.label()\n",
    "#     # if we get to the end of the loop. we didn't find the word we were looking for\n",
    "#     return None\n",
    "\n",
    "\n",
    "# def collect_tokens(indices, sents, tagged_sents):\n",
    "#     \"\"\"\n",
    "#     takes a word and a list of indices\n",
    "#     returns tuples containing \n",
    "#         word\n",
    "#         sentence_string\n",
    "#         sense\n",
    "#     \"\"\"\n",
    "#     #sense_count = 0\n",
    "#     tokens = []\n",
    "    \n",
    "#     # indices is a list of all of the sentence ids containing this word\n",
    "#     indices = list(indices)\n",
    "#     # visit these sentences in random order\n",
    "#     random.shuffle(indices)\n",
    "#     for index in indices[:25]:\n",
    "\n",
    "#         sentence = sents[index]\n",
    "#         sentence = ' '.join(sentence)\n",
    "\n",
    "#         tokens.append(sentence)\n",
    "#         #sense_count += 1\n",
    "        \n",
    "        \n",
    "    \n",
    "#     #print(sense_count.items())\n",
    "#     return tokens\n",
    "\n",
    "\n",
    "# def collect_tokens_for_all_words_to_file(path, sense_path, sense_index, sents, tagged_sents):\n",
    "#     with open(path, 'w', newline='') as csvfile:\n",
    "#         writer = csv.writer(csvfile)\n",
    "\n",
    "#         with open(sense_path, 'w', newline='') as sensefile:\n",
    "#             sensewriter = csv.writer(sensefile)\n",
    "        \n",
    "#             for lemma in sense_index.keys():\n",
    "#                 #print(lemma)\n",
    "#                 for sense, indices in sense_index[lemma].items():\n",
    "#                     print(lemma)\n",
    "#                     print(sense)\n",
    "#                     #print(indices)\n",
    "\n",
    "                \n",
    "#                     frequency = len(indices)\n",
    "#                     tokens = collect_tokens(indices, sents, tagged_sents)\n",
    "#                     #print(tokens)\n",
    "#                     #raise Exception(\"nfwip\")\n",
    "\n",
    "#                     for token in tokens:\n",
    "#                         row = (lemma, sense, token)\n",
    "#                         writer.writerow(row)                    \n",
    "#                     #sensewriter.writerow(sense_count.items())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# collect_tokens_for_all_words_to_file('../data/semcor_wu_palmer_eval_datamcrae.csv', '../data/semcor_sense_counts_mcrae.csv', sense_index, sents, tagged_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict feature vectors and save pairwise token data;\n",
    "# Run correlation analysis;\n",
    "# Saturate with other information;\n",
    "\n",
    "Because you already have the data from the\n",
    "    \"Collect semcor eval data.ipynb\" script\n",
    "in the right format in the form of \n",
    "    [lemma, sense, word_form, context]\n",
    "in the file\n",
    "    \"data/processed/semcor_eval_data_11_27_2021.csv\"\n",
    "    \n",
    "All the above code is moot and improved. We simply need to load in this file of eval data and work from there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "models and save paths\n",
    "\"\"\"\n",
    "\n",
    "models = [\n",
    "    # commented models have already been run\n",
    "    ## buchanan\n",
    "    #'../trained_models/model.plsr.buchanan.allbuthomonyms.5k.300components.500max_iters',\n",
    "    #'../trained_models/model.plsr.buchanan.allbuthomonyms.1k.300components.500max_iters',\n",
    "    #'../trained_models/model.ffnn.buchanan.allbuthomonyms.5k.50epochs.0.5dropout.lr1e-4.hsize300',\n",
    "    #'../trained_models/model.ffnn.buchanan.allbuthomonyms.1k.50epochs.0.5dropout.lr1e-4.hsize300',\n",
    "\n",
    "    ### mcrae\n",
    "    #'../trained_models/model.plsr.mc_rae_real.allbuthomonyms.5k.100components.500max_iters',\n",
    "    #'../trained_models/model.plsr.mc_rae_real.allbuthomonyms.1k.50components.500max_iters',\n",
    "    #'../trained_models/model.ffnn.mc_rae_real.allbuthomonyms.5k.50epochs.0.5dropout.lr1e-4.hsize300',\n",
    "    #'../trained_models/model.ffnn.mc_rae_real.allbuthomonyms.1k.50epochs.0.5dropout.lr1e-4.hsize300',\n",
    "    '../trained_models/model.modabs.mc_rae_real.5k.mu1_1.mu2_0.1.mu3_0.001.mu4_5.nnk_4',\n",
    "    '../trained_models/model.modabs.mc_rae_real.1k.mu1_1.mu2_0.1.mu3_1e-07.mu4_10.nnk_4'\n",
    "    #### binder\n",
    "    #'../trained_models/model.ffnn.binder.5k.50epochs.0.5dropout.lr1e-4.hsize300',\n",
    "    #'../trained_models/model.ffnn.binder.1k.50epochs.0.5dropout.lr1e-4.hsize300',\n",
    "    #'../trained_models/model.plsr.binder.5k.30components.500max_iters',\n",
    "    #'../trained_models/model.plsr.binder.1k.30components.500max_iters',\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (124762250.py, line 107)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/9m/vzvx58rs51v_x5nm620fz4xr0000gn/T/ipykernel_38401/124762250.py\"\u001b[0;36m, line \u001b[0;32m107\u001b[0m\n\u001b[0;31m    for j in range(i+1,num_toks):\u001b[0m\n\u001b[0m                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Now, we have our dataset that we want to analyze. We just need to do:\n",
    "\n",
    "for each model we want to evaluate, run the following script:\n",
    "\n",
    "open the file of data\n",
    "\n",
    "read it in as a dataframe\n",
    "\n",
    "for each of the unique words in that dataset\n",
    "\n",
    "    we calculate pairwise distances between each otoken and every otehr token\n",
    "    and construct a similarities dataset. \n",
    "    \n",
    "    add some extra infor about sense and polysemy bins\n",
    "    \n",
    "    and store into a file\n",
    "    \n",
    "    \n",
    "    (no longer do this-then we run correlations for that word-???)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def make_predictions(df, model, bert):\n",
    "    \"\"\"\n",
    "    df has columns\n",
    "    [lemma, sense, word_form, context]\n",
    "    \n",
    "    for a single semcor lemma\n",
    "    \"\"\"    \n",
    "    predictions = []\n",
    "    for index, row in df.iterrows():\n",
    "\n",
    "        #print(row.word_form)\n",
    "        #print(row.context)\n",
    "\n",
    "        predicted_vector = model.predict_in_context(row.word_form, row.context, bert)\n",
    "\n",
    "        predictions.append(predicted_vector)\n",
    "    return predictions\n",
    "\n",
    "def read_concreteness_values():\n",
    "    \"\"\"\n",
    "    returns df with columns\n",
    "    word\n",
    "    Conc.M\n",
    "    \"\"\"\n",
    "    brysbaert_filename = \"/Users/gabriellachronis/data/Concreteness_ratings_Brysbaert_et_al_BRM.csv\"\n",
    "    concreteness_df = pd.read_csv(brysbaert_filename, sep='\\t')\n",
    "    concreteness_df= concreteness_df[[\"Word\", \"Conc.M\"]]\n",
    "    concreteness_df = concreteness_df.set_index(\"Word\")\n",
    "    return concreteness_df\n",
    "\n",
    "def get_pairwise_wu_palmer_data(model, df, bert):\n",
    "    \n",
    "    \"\"\"\n",
    "    df has columns\n",
    "    [lemma, sense, word_form, context]\n",
    "    \"\"\"\n",
    "    \n",
    "    # add concreteness\n",
    "    concreteness_df = read_concreteness_values()\n",
    "    df = df.join(concreteness_df, how = \"left\", on = \"lemma\")\n",
    "\n",
    "    \n",
    "    # storage\n",
    "    run_stats = []\n",
    "    vals = []\n",
    "    item = 0\n",
    "\n",
    "    # run through lemma by lemma\n",
    "    unique_words = df.lemma.unique()\n",
    "    for l in range(0, len(unique_words)):\n",
    "        if l % 1000 == 0:\n",
    "            print(\"processed %s lemmas\" % l)\n",
    "        \n",
    "        # get a dataframe containing all the tokens of this word\n",
    "        word = unique_words[l]\n",
    "        word_data = df[df.lemma == word].copy()\n",
    "        \n",
    "        # lemma level info\n",
    "        n_senses = len(word_data['sense'].unique())\n",
    "        n_word_forms = len(word_data['word_form'].unique())\n",
    "        \n",
    "        # token level predictions\n",
    "        predictions = make_predictions(word_data, model, bert)\n",
    "        word_data['prediction'] = predictions\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        then we calculate the pairwise distances between all of the vectors, only counting one pair one time\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # pop the first token off the list\n",
    "        num_toks = len(word_data)\n",
    "        for i in range(0,num_toks):\n",
    "            item +=1\n",
    "            if item % 1000 == 0:\n",
    "                print(\"processed %s of %s tokens\" % (item,num_toks))            \n",
    "            # compare it with each of the other tokens\n",
    "            # dont have to compare to any earlier\n",
    "            for j in range(i+1,num_toks):\n",
    "\n",
    "                #print(df.iloc[i])\n",
    "                #print(df.iloc[j])\n",
    "\n",
    "                # calculate cosine similarity between the two vectors\n",
    "                cos_sim = 1 - cosine(word_data.iloc[i].prediction, word_data.iloc[j].prediction)\n",
    "\n",
    "                # and wu palmer similarity between the two wn lemmas\n",
    "                lemma1 = lemma_from_string(word_data.iloc[i].sense)\n",
    "                lemma1_str = lemma_name_from_string(word_data.iloc[i].sense)\n",
    "                lemma2 = lemma_from_string(word_data.iloc[j].sense)\n",
    "                lemma2_str = lemma_name_from_string(word_data.iloc[j].sense)\n",
    "                synset1 = lemma1.synset()\n",
    "                synset2 = lemma2.synset()\n",
    "                wup_sim = synset1.wup_similarity(synset2)\n",
    "\n",
    "                # get other token level data\n",
    "                pos1 = re.findall(r\"\\.(.*?)\\.\", lemma1_str)[0]\n",
    "                pos2 = re.findall(r\"\\.(.*?)\\.\", lemma2_str)[0]\n",
    "                concreteness = word_data.iloc[i][\"Conc.M\"]\n",
    "                \n",
    "                # if we can't compute a distance for these senses / recognize them, discard\n",
    "                if type(wup_sim) == float:\n",
    "                    # store this data point into a list\n",
    "                    vals.append((word, lemma1_str, lemma2_str, pos1, pos2, cos_sim, wup_sim, n_senses, n_word_forms, concreteness))\n",
    "        \n",
    "    # turn results into dataframe\n",
    "    columns = [\n",
    "        \"lemma\",\n",
    "        \"token_sense_1\",\n",
    "        \"token_sense_2\",\n",
    "        \"sense1_pos\",\n",
    "        \"sense2_pos\",\n",
    "        \"cos_sim\",\n",
    "        \"wup_sim\", \n",
    "        \"n_senses\",\n",
    "        \"n_word_forms\",\n",
    "        \"concreteness\"]\n",
    "    token_similarities = pd.DataFrame.from_records(vals, columns = columns)\n",
    "        \n",
    "    \n",
    "    # add in bins\n",
    "    token_similarities['wn_bin'] = pd.cut(token_similarities.n_senses, \n",
    "                        bins = [0, 2.1, 4.1, 6.1, 8.1, 10.1, 20.1, 50.1, 200], labels = False)\n",
    "    token_similarities['conc_bin'] = pd.cut(token_similarities.concreteness, \n",
    "                        bins = [0, 1.5, 2.5, 3.5, 4.5, 10], labels = False)\n",
    "   \n",
    "    return token_similarities\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "def plot_sims():\n",
    "    cos_sims = sense_similarities['cos_sim']\n",
    "    wup_sims = sense_similarities['wup_sim']\n",
    "    plt.scatter(wup_sims, cos_sims)\n",
    "    plt.title(\"Wordnet similarity of homonymous senses plotted against cosine similarity of predicted vectors of two tokens in semantic feature space\")\n",
    "    plt.xlabel(\"Wu and Palmer Similarity\")\n",
    "    plt.ylabel(\"Cosine Similarity\")\n",
    "    plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************\n",
      "*** Evaluating ../trained_models/model.modabs.mc_rae_real.5k.mu1_1.mu2_0.1.mu3_0.001.mu4_5.nnk_4 model ***\n",
      "****************************************\n",
      "processed 0 lemmas\n",
      "processed 1000 tokens\n",
      "processed 2000 tokens\n",
      "processed 3000 tokens\n",
      "processed 4000 tokens\n",
      "processed 5000 tokens\n",
      "processed 6000 tokens\n",
      "processed 7000 tokens\n",
      "processed 8000 tokens\n",
      "processed 9000 tokens\n",
      "processed 10000 tokens\n",
      "processed 11000 tokens\n",
      "processed 12000 tokens\n",
      "processed 13000 tokens\n",
      "processed 14000 tokens\n",
      "processed 500 lemmas\n",
      "processed 15000 tokens\n",
      "processed 16000 tokens\n",
      "processed 17000 tokens\n",
      "processed 18000 tokens\n",
      "processed 19000 tokens\n",
      "processed 20000 tokens\n",
      "processed 21000 tokens\n",
      "processed 22000 tokens\n",
      "processed 23000 tokens\n",
      "processed 24000 tokens\n",
      "processed 1000 lemmas\n",
      "processed 25000 tokens\n",
      "processed 26000 tokens\n",
      "processed 27000 tokens\n",
      "processed 28000 tokens\n",
      "processed 29000 tokens\n",
      "processed 30000 tokens\n",
      "processed 31000 tokens\n",
      "processed 32000 tokens\n",
      "processed 1500 lemmas\n",
      "processed 33000 tokens\n",
      "processed 34000 tokens\n",
      "processed 35000 tokens\n",
      "processed 36000 tokens\n",
      "processed 37000 tokens\n",
      "processed 38000 tokens\n",
      "processed 2000 lemmas\n",
      "processed 39000 tokens\n",
      "processed 40000 tokens\n",
      "processed 41000 tokens\n",
      "processed 42000 tokens\n",
      "processed 43000 tokens\n",
      "processed 2500 lemmas\n",
      "processed 44000 tokens\n",
      "processed 45000 tokens\n",
      "processed 46000 tokens\n",
      "processed 47000 tokens\n",
      "processed 3000 lemmas\n",
      "processed 48000 tokens\n",
      "processed 49000 tokens\n",
      "processed 3500 lemmas\n",
      "processed 50000 tokens\n",
      "processed 51000 tokens\n",
      "processed 52000 tokens\n",
      "processed 4000 lemmas\n",
      "processed 53000 tokens\n",
      "processed 54000 tokens\n",
      "processed 4500 lemmas\n",
      "processed 55000 tokens\n",
      "processed 56000 tokens\n",
      "processed 5000 lemmas\n",
      "processed 57000 tokens\n",
      "processed 58000 tokens\n",
      "processed 5500 lemmas\n",
      "processed 59000 tokens\n",
      "processed 60000 tokens\n",
      "processed 6000 lemmas\n",
      "processed 61000 tokens\n",
      "processed 6500 lemmas\n",
      "processed 62000 tokens\n",
      "processed 63000 tokens\n",
      "processed 7000 lemmas\n",
      "processed 64000 tokens\n",
      "processed 7500 lemmas\n",
      "processed 65000 tokens\n",
      "processed 8000 lemmas\n",
      "processed 66000 tokens\n",
      "processed 8500 lemmas\n",
      "processed 67000 tokens\n",
      "processed 9000 lemmas\n",
      "processed 68000 tokens\n",
      "processed 9500 lemmas\n",
      "processed 10000 lemmas\n",
      "processed 69000 tokens\n",
      "processed 10500 lemmas\n",
      "processed 70000 tokens\n",
      "processed 11000 lemmas\n",
      "processed 71000 tokens\n",
      "processed 11500 lemmas\n",
      "****************************************\n",
      "*** Evaluating ../trained_models/model.modabs.mc_rae_real.1k.mu1_1.mu2_0.1.mu3_1e-07.mu4_10.nnk_4 model ***\n",
      "****************************************\n",
      "processed 0 lemmas\n",
      "processed 1000 tokens\n",
      "processed 2000 tokens\n",
      "processed 3000 tokens\n",
      "processed 4000 tokens\n",
      "processed 5000 tokens\n",
      "processed 6000 tokens\n",
      "processed 7000 tokens\n",
      "processed 8000 tokens\n",
      "processed 9000 tokens\n",
      "processed 10000 tokens\n",
      "processed 11000 tokens\n",
      "processed 12000 tokens\n",
      "processed 13000 tokens\n",
      "processed 14000 tokens\n",
      "processed 500 lemmas\n",
      "processed 15000 tokens\n",
      "processed 16000 tokens\n",
      "processed 17000 tokens\n",
      "processed 18000 tokens\n",
      "processed 19000 tokens\n",
      "processed 20000 tokens\n",
      "processed 21000 tokens\n",
      "processed 22000 tokens\n",
      "processed 23000 tokens\n",
      "processed 24000 tokens\n",
      "processed 1000 lemmas\n",
      "processed 25000 tokens\n",
      "processed 26000 tokens\n",
      "processed 27000 tokens\n",
      "processed 28000 tokens\n",
      "processed 29000 tokens\n",
      "processed 30000 tokens\n",
      "processed 31000 tokens\n",
      "processed 32000 tokens\n",
      "processed 1500 lemmas\n",
      "processed 33000 tokens\n",
      "processed 34000 tokens\n",
      "processed 35000 tokens\n",
      "processed 36000 tokens\n",
      "processed 37000 tokens\n",
      "processed 38000 tokens\n",
      "processed 2000 lemmas\n",
      "processed 39000 tokens\n",
      "processed 40000 tokens\n",
      "processed 41000 tokens\n",
      "processed 42000 tokens\n",
      "processed 43000 tokens\n",
      "processed 2500 lemmas\n",
      "processed 44000 tokens\n",
      "processed 45000 tokens\n",
      "processed 46000 tokens\n",
      "processed 47000 tokens\n",
      "processed 3000 lemmas\n",
      "processed 48000 tokens\n",
      "processed 49000 tokens\n",
      "processed 3500 lemmas\n",
      "processed 50000 tokens\n",
      "processed 51000 tokens\n",
      "processed 52000 tokens\n",
      "processed 4000 lemmas\n",
      "processed 53000 tokens\n",
      "processed 54000 tokens\n",
      "processed 4500 lemmas\n",
      "processed 55000 tokens\n",
      "processed 56000 tokens\n",
      "processed 5000 lemmas\n",
      "processed 57000 tokens\n",
      "processed 58000 tokens\n",
      "processed 5500 lemmas\n",
      "processed 59000 tokens\n",
      "processed 60000 tokens\n",
      "processed 6000 lemmas\n",
      "processed 61000 tokens\n",
      "processed 6500 lemmas\n",
      "processed 62000 tokens\n",
      "processed 63000 tokens\n",
      "processed 7000 lemmas\n",
      "processed 64000 tokens\n",
      "processed 7500 lemmas\n",
      "processed 65000 tokens\n",
      "processed 8000 lemmas\n",
      "processed 66000 tokens\n",
      "processed 8500 lemmas\n",
      "processed 67000 tokens\n",
      "processed 9000 lemmas\n",
      "processed 68000 tokens\n",
      "processed 9500 lemmas\n",
      "processed 10000 lemmas\n",
      "processed 69000 tokens\n",
      "processed 10500 lemmas\n",
      "processed 70000 tokens\n",
      "processed 11000 lemmas\n",
      "processed 71000 tokens\n",
      "processed 11500 lemmas\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# we need the default na option turned off bc the word/lemma 'null' was being interpreted as 'NaN'\n",
    "names = [\"lemma\", \"sense\", \"word_form\", \"context\"]\n",
    "df = pd.read_csv('../data/processed/semcor_eval_data_11_27_2021.csv', names = names , keep_default_na=False)\n",
    "\n",
    "for save_path in models:\n",
    "    print(\"****************************************\")\n",
    "    print(\"*** Evaluating %s model ***\" % save_path)\n",
    "    print(\"****************************************\")\n",
    "    model = torch.load(save_path)\n",
    "    out_path = '../results/semcor_pairwise_data_' + os.path.split(save_path)[1] + '.csv'    \n",
    "\n",
    "    # remove results file if exists\n",
    "    if os.path.exists(out_path):\n",
    "        os.remove(out_path)\n",
    "    pairwise_data = get_pairwise_wu_palmer_data(model, df, bert)\n",
    "    pairwise_data.to_csv(out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Conc.M</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>roadsweeper</th>\n",
       "      <td>4.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>traindriver</th>\n",
       "      <td>4.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tush</th>\n",
       "      <td>4.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hairdress</th>\n",
       "      <td>3.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pharmaceutics</th>\n",
       "      <td>3.77</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Conc.M\n",
       "Word                 \n",
       "roadsweeper      4.85\n",
       "traindriver      4.54\n",
       "tush             4.45\n",
       "hairdress        3.93\n",
       "pharmaceutics    3.77"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brysbaert_filename = \"/Users/gabriellachronis/data/Concreteness_ratings_Brysbaert_et_al_BRM.csv\"\n",
    "concreteness_df = pd.read_csv(brysbaert_filename, sep='\\t')\n",
    "concreteness_df= concreteness_df[[\"Word\", \"Conc.M\"]]\n",
    "concreteness_df = concreteness_df.set_index(\"Word\")\n",
    "concreteness_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now we need to go in and add abstractness value and \n",
    "bin number of senses into polysemy band\n",
    "\n",
    "(already done since we fixed above)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# #cols = [\"lemma\", \"token_sense_1\", \"token_sense_2\", \"cos_sim\", \"wup_sim\", \"n_senses\"]\n",
    "\n",
    "\n",
    "# for save_path in models:\n",
    "#     print(\"****************************************\")\n",
    "#     print(\"*** Saturating pairwise data for model: %s ***\" % save_path)\n",
    "#     print(\"****************************************\")\n",
    "#     infile = '../results/semcor_pairwise_data_' + os.path.split(save_path)[1] + '.csv'\n",
    "\n",
    "#     outfile = '../results/saturated_semcor_pairwise_data_' + os.path.split(save_path)[1] + '.csv'\n",
    "\n",
    "    \n",
    "#     dtype = {\n",
    "#         'lemma':             str,\n",
    "#         'token_sense_1':     object,\n",
    "#         'token_sense_2':     object,\n",
    "#         'cos_sim':           float,\n",
    "#         'wup_sim':           float,\n",
    "#         'n_senses':          float   \n",
    "#     }\n",
    "#     csv_input = pd.read_csv(infile, encoding_errors='ignore') #, names=cols) # requires pandas 1.3\n",
    "#     #csv_input = pd.read_csv(infile)\n",
    "    \n",
    "#     print(len(csv_input))\n",
    "    \n",
    "#      # filtering out the rows with `POSITION_T` value in corresponding column\n",
    "#     csv_input = csv_input[csv_input.token_sense_1.str.contains('token_sense_1') == False]\n",
    "    \n",
    "#     print(len(csv_input))\n",
    "# #     print(csv_input.dtypes)\n",
    "\n",
    "    \n",
    "#     csv_input = csv_input.convert_dtypes(convert_floating=True)\n",
    "    \n",
    "# #     print(csv_input.dtypes)\n",
    "    \n",
    "#     print(csv_input.head())\n",
    "\n",
    "    \n",
    "#     # add polysemy bin\n",
    "#     csv_input['wn_bin'] = pd.cut(csv_input.n_senses, \n",
    "#                         bins = [0, 2.1, 4.1, 6.1, 8.1, 10.1, 20.1, 50.1, 200], labels = False)\n",
    "\n",
    "\n",
    "#     # add POS rows\n",
    "#     pos1s = []\n",
    "#     pos2s = []\n",
    "#     sense1s = []\n",
    "#     sense2s = []\n",
    "#     for index, row in csv_input.iterrows():\n",
    "#         pos1 = re.findall(r\"\\.(.*?)\\.\", row.token_sense_1)[0]\n",
    "#         pos2 = re.findall(r\"\\.(.*?)\\.\", row.token_sense_2)[0]\n",
    "#         pos1s.append(pos1)\n",
    "#         pos2s.append(pos2)\n",
    "#         sense1 = lemma_name_from_string(row.token_sense_1)\n",
    "#         sense2 = lemma_name_from_string(row.token_sense_2)\n",
    "#         sense1s.append(sense1)\n",
    "#         sense2s.append(sense2)\n",
    "        \n",
    "#     csv_input['sense1_pos'] = pos1s\n",
    "#     csv_input['sense2_pos'] = pos2s\n",
    "#     csv_input['token_sense_1'] = sense_1s\n",
    "#     csv_input['token_sense_2'] = sense_2s\n",
    "    \n",
    "    \n",
    "#     # add concreteness\n",
    "#     csv_input = csv_input.join(concreteness_df, how = \"left\", on = \"lemma\")\n",
    "    \n",
    "#     csv_input['conc_bin'] = pd.cut(csv_input['Conc.M'], \n",
    "#                         bins = [0, 2.3, 4.5, 10], labels = False)\n",
    "    \n",
    "#     # remove token sense columns\n",
    "# #     csv_input.drop(['token_sense_1'], axis=1)\n",
    "# #     csv_input.drop(['token_sense_2'], axis=1)\n",
    "    \n",
    "#     #print(csv_input.where(csv_input['Conc.M'].notnull()))\n",
    "#     print(csv_input.head(20))\n",
    "    \n",
    "#     raise Exception(\"dewfieow\")\n",
    "    \n",
    "#     csv_input.to_csv(outfile, index=False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a little test\n",
    "# lemma_from_string(csv_input.iloc[0].token_sense_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    # commented models have already been run\n",
    "    ## buchanan\n",
    "    #'../trained_models/model.plsr.buchanan.allbuthomonyms.5k.300components.500max_iters',\n",
    "    #'../trained_models/model.plsr.buchanan.allbuthomonyms.1k.300components.500max_iters',\n",
    "    #'../trained_models/model.ffnn.buchanan.allbuthomonyms.5k.50epochs.0.5dropout.lr1e-4.hsize300',\n",
    "    #'../trained_models/model.ffnn.buchanan.allbuthomonyms.1k.50epochs.0.5dropout.lr1e-4.hsize300',\n",
    "\n",
    "    ### mcrae\n",
    "    \n",
    "    '../trained_models/model.plsr.mc_rae_real.allbuthomonyms.5k.100components.500max_iters',\n",
    "    '../trained_models/model.plsr.mc_rae_real.allbuthomonyms.1k.50components.500max_iters',\n",
    "    '../trained_models/model.ffnn.mc_rae_real.allbuthomonyms.5k.50epochs.0.5dropout.lr1e-4.hsize300',\n",
    "    '../trained_models/model.ffnn.mc_rae_real.allbuthomonyms.1k.50epochs.0.5dropout.lr1e-4.hsize300',\n",
    "    '../trained_models/model.modabs.mc_rae_real.5k.mu1_1.mu2_0.1.mu3_0.001.mu4_5.nnk_4',\n",
    "    '../trained_models/model.modabs.mc_rae_real.1k.mu1_1.mu2_0.1.mu3_1e-07.mu4_10.nnk_4'\n",
    "    #### binder\n",
    "    #'../trained_models/model.ffnn.binder.5k.50epochs.0.5dropout.lr1e-4.hsize300',\n",
    "    #'../trained_models/model.ffnn.binder.1k.50epochs.0.5dropout.lr1e-4.hsize300',\n",
    "    #'../trained_models/model.plsr.binder.5k.30components.500max_iters',\n",
    "    #'../trained_models/model.plsr.binder.1k.30components.500max_iters',\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "and now we make another dataset for each one with the correlations.\n",
    "first we define some functions\n",
    "\"\"\"\n",
    "\n",
    "def run_correlation(sense_similarities):\n",
    "    \"\"\"\n",
    "    input\n",
    "    :sense_similarities: dataframe with columns\n",
    "        cosine_sims \n",
    "        wup_sims\n",
    "    output\n",
    "    4-tuple with\n",
    "        pearson\n",
    "        pearson_p\n",
    "        spearman\n",
    "        spearman_p\n",
    "    \"\"\"\n",
    "    #print(sense_similarities.head())\n",
    "    \n",
    "#     if sense_similarities is None:\n",
    "#         # not really sure why we're getting none values here it should be impossible\n",
    "#         return (float(\"nan\"), float(\"nan\"), float(\"nan\"), float(\"nan\"))\n",
    "#     elif len(sense_similarities['wup_sim'].unique()) == 1:\n",
    "\n",
    "    # if we only have 1 sense represented in semcor\n",
    "    # the correlation will be garbage with a constant y value; skip to avoid warnings\n",
    "    if sense_similarities.iloc[0]['n_senses'] == 1:\n",
    "        return (float(\"nan\"), float(\"nan\"), float(\"nan\"), float(\"nan\"))\n",
    "\n",
    "    if len(sense_similarities) > 1 :\n",
    "        #print(word)\n",
    "\n",
    "        cos_sims = sense_similarities['cos_sim']\n",
    "        wup_sims = sense_similarities['wup_sim']\n",
    "\n",
    "        pearson, pearson_p = pearsonr(cos_sims, wup_sims )\n",
    "        #print('Pearsons correlation: %.3f, p-value: %s'  % (pearson, pearson_p))\n",
    "\n",
    "        spearman, spearman_p = spearmanr(cos_sims, wup_sims )\n",
    "        #print('Spearmans correlation: %.3f, p-value: %s'  % (spearman, spearman_p))\n",
    "\n",
    "        return (pearson, pearson_p, spearman, spearman_p)\n",
    "    \n",
    "    # if we don't have enough, return dummies\n",
    "    print(\"not enough examples of lemma: \", sense_similarities.iloc[0].lemma)\n",
    "    return (float(\"nan\"), float(\"nan\"), float(\"nan\"), float(\"nan\"))\n",
    "\n",
    "def plot_sims():\n",
    "    cos_sims = sense_similarities['cos_sim']\n",
    "    wup_sims = sense_similarities['wup_sim']\n",
    "    plt.scatter(wup_sims, cos_sims)\n",
    "    plt.title(\"Wordnet similarity of homonymous senses plotted against cosine similarity of predicted vectors of two tokens in semantic feature space\")\n",
    "    plt.xlabel(\"Wu and Palmer Similarity\")\n",
    "    plt.ylabel(\"Cosine Similarity\")\n",
    "    plt.show()\n",
    "        \n",
    "# i think this is obsolete 11/16/21\n",
    "# df = pd.read_csv('semcor_wu_palmer_eval_data.csv', names = [\"word_form\", \"context\", \"wn_lemma\"])\n",
    "\n",
    "\n",
    "\n",
    "# for save_path in models:\n",
    "#     print(\"****************************************\")\n",
    "#     print(\"*** Evaluating %s model ***\" % save_path)\n",
    "#     print(\"****************************************\")\n",
    "#     model = torch.load(save_path)\n",
    "#     run_stats = run_wu_palmer_analysis(model, df, bert)\n",
    "#     #run_stats = run_correlation(similarities)\n",
    "#     out_path = 'results/semcor_analysis_' + os.path.split(save_path)[1] + '.csv'\n",
    "#     run_stats.to_csv(out_path)\n",
    "#     print(run_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#debug_df = pd.read_csv('../results/debug_semcor_pairwise_data_model.plsr.buchanan.allbuthomoyms.5k.300components.500max_iters.csv')\n",
    "#debug_df[debug_df.lemma=='tan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma</th>\n",
       "      <th>sense</th>\n",
       "      <th>wordform</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53086</th>\n",
       "      <td>herb</td>\n",
       "      <td>Lemma('herb.n.01.herb')</td>\n",
       "      <td>herbs</td>\n",
       "      <td>Gazing at her husband 's drugged body , his ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53087</th>\n",
       "      <td>herb</td>\n",
       "      <td>Lemma('herb.n.02.herb')</td>\n",
       "      <td>herbs</td>\n",
       "      <td>She stood still over the leg of lamb , rubbing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      lemma                    sense wordform  \\\n",
       "53086  herb  Lemma('herb.n.01.herb')    herbs   \n",
       "53087  herb  Lemma('herb.n.02.herb')    herbs   \n",
       "\n",
       "                                                 context  \n",
       "53086  Gazing at her husband 's drugged body , his ch...  \n",
       "53087  She stood still over the leg of lamb , rubbing...  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#debug_df = pd.read_csv('../data/semcor_eval_data_11_27_2021.csv', names = [\"lemma\", \"sense\", \"wordform\", \"context\"])\n",
    "#debug_df[debug_df.lemma=='tan']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "okay so the problem we are running into is that sometimes theres just one value per lemma, when we have two senses and only one token per sense in the dataset. we can fix this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************\n",
      "*** Running correlation on  ../trained_models/model.modabs.mc_rae_real.5k.mu1_1.mu2_0.1.mu3_0.001.mu4_5.nnk_4 model ***\n",
      "****************************************\n",
      "reading from  ../results/debug_semcor_pairwise_data_model.modabs.mc_rae_real.5k.mu1_1.mu2_0.1.mu3_0.001.mu4_5.nnk_4.csv\n",
      "not enough examples of lemma:  tan\n",
      "not enough examples of lemma:  velour\n",
      "not enough examples of lemma:  hillbilly\n",
      "not enough examples of lemma:  scouring\n",
      "not enough examples of lemma:  inferiority\n",
      "not enough examples of lemma:  shibboleth\n",
      "not enough examples of lemma:  traditionalism\n",
      "not enough examples of lemma:  bawh\n",
      "not enough examples of lemma:  carelessness\n",
      "not enough examples of lemma:  scraping\n",
      "not enough examples of lemma:  emanation\n",
      "not enough examples of lemma:  wattle\n",
      "not enough examples of lemma:  privy\n",
      "not enough examples of lemma:  penance\n",
      "not enough examples of lemma:  reverse\n",
      "not enough examples of lemma:  surrender\n",
      "not enough examples of lemma:  mask\n",
      "not enough examples of lemma:  pulse\n",
      "not enough examples of lemma:  noun\n",
      "not enough examples of lemma:  disregard\n",
      "not enough examples of lemma:  subscription\n",
      "not enough examples of lemma:  brotherhood\n",
      "not enough examples of lemma:  bankruptcy\n",
      "not enough examples of lemma:  booth\n",
      "not enough examples of lemma:  small\n",
      "not enough examples of lemma:  provincialism\n",
      "not enough examples of lemma:  pinpoint\n",
      "not enough examples of lemma:  inflection\n",
      "not enough examples of lemma:  believer\n",
      "not enough examples of lemma:  contributor\n",
      "not enough examples of lemma:  instability\n",
      "not enough examples of lemma:  rhyme\n",
      "not enough examples of lemma:  confederacy\n",
      "not enough examples of lemma:  zeal\n",
      "not enough examples of lemma:  florist\n",
      "not enough examples of lemma:  streak\n",
      "not enough examples of lemma:  landslide\n",
      "not enough examples of lemma:  lane\n",
      "not enough examples of lemma:  humanism\n",
      "not enough examples of lemma:  hulk\n",
      "not enough examples of lemma:  accessory\n",
      "not enough examples of lemma:  firmness\n",
      "not enough examples of lemma:  honour\n",
      "not enough examples of lemma:  italian\n",
      "not enough examples of lemma:  japanese\n",
      "not enough examples of lemma:  slant\n",
      "not enough examples of lemma:  urging\n",
      "not enough examples of lemma:  helper\n",
      "not enough examples of lemma:  merriment\n",
      "not enough examples of lemma:  heaven\n",
      "not enough examples of lemma:  anachronism\n",
      "not enough examples of lemma:  expansiveness\n",
      "not enough examples of lemma:  pine\n",
      "not enough examples of lemma:  violet\n",
      "not enough examples of lemma:  dictate\n",
      "not enough examples of lemma:  helm\n",
      "not enough examples of lemma:  refinement\n",
      "not enough examples of lemma:  sail\n",
      "not enough examples of lemma:  captivity\n",
      "not enough examples of lemma:  reliance\n",
      "not enough examples of lemma:  cultivation\n",
      "not enough examples of lemma:  random\n",
      "not enough examples of lemma:  binomial\n",
      "not enough examples of lemma:  dash\n",
      "not enough examples of lemma:  primate\n",
      "not enough examples of lemma:  roundhouse\n",
      "not enough examples of lemma:  renewal\n",
      "not enough examples of lemma:  dislocation\n",
      "not enough examples of lemma:  herb\n",
      "not enough examples of lemma:  incarnation\n",
      "not enough examples of lemma:  loophole\n",
      "not enough examples of lemma:  jab\n",
      "not enough examples of lemma:  assist\n",
      "not enough examples of lemma:  flare\n",
      "not enough examples of lemma:  trademark\n",
      "not enough examples of lemma:  drafting\n",
      "not enough examples of lemma:  burn\n",
      "not enough examples of lemma:  layout\n",
      "not enough examples of lemma:  nod\n",
      "not enough examples of lemma:  pore\n",
      "not enough examples of lemma:  dereliction\n",
      "not enough examples of lemma:  backbone\n",
      "not enough examples of lemma:  orgy\n",
      "not enough examples of lemma:  mentality\n",
      "not enough examples of lemma:  down\n",
      "not enough examples of lemma:  marking\n",
      "not enough examples of lemma:  dissent\n",
      "not enough examples of lemma:  allocation\n",
      "not enough examples of lemma:  hound\n",
      "not enough examples of lemma:  detachment\n",
      "not enough examples of lemma:  mouthpiece\n",
      "not enough examples of lemma:  infliction\n",
      "not enough examples of lemma:  allegation\n",
      "not enough examples of lemma:  falsity\n",
      "not enough examples of lemma:  lad\n",
      "not enough examples of lemma:  hysteria\n",
      "not enough examples of lemma:  circus\n",
      "not enough examples of lemma:  syndicate\n",
      "not enough examples of lemma:  gage\n",
      "not enough examples of lemma:  hardship\n",
      "not enough examples of lemma:  bound\n",
      "not enough examples of lemma:  alligator\n",
      "not enough examples of lemma:  tangle\n",
      "not enough examples of lemma:  hoop\n",
      "not enough examples of lemma:  southeast\n",
      "not enough examples of lemma:  keynote\n",
      "not enough examples of lemma:  preoccupation\n",
      "not enough examples of lemma:  signature\n",
      "not enough examples of lemma:  premise\n",
      "not enough examples of lemma:  duplication\n",
      "not enough examples of lemma:  knob\n",
      "not enough examples of lemma:  chute\n",
      "not enough examples of lemma:  brilliance\n",
      "not enough examples of lemma:  frost\n",
      "not enough examples of lemma:  subtlety\n",
      "not enough examples of lemma:  rigger\n",
      "not enough examples of lemma:  showing\n",
      "not enough examples of lemma:  upset\n",
      "not enough examples of lemma:  percussion\n",
      "not enough examples of lemma:  genus\n",
      "not enough examples of lemma:  ringing\n",
      "not enough examples of lemma:  miniature\n",
      "not enough examples of lemma:  viewer\n",
      "not enough examples of lemma:  setup\n",
      "not enough examples of lemma:  rut\n",
      "not enough examples of lemma:  outrage\n",
      "not enough examples of lemma:  rearing\n",
      "not enough examples of lemma:  squatting\n",
      "not enough examples of lemma:  sash\n",
      "not enough examples of lemma:  marketplace\n",
      "not enough examples of lemma:  fancy\n",
      "not enough examples of lemma:  prophecy\n",
      "not enough examples of lemma:  configuration\n",
      "not enough examples of lemma:  congestion\n",
      "not enough examples of lemma:  brevity\n",
      "not enough examples of lemma:  conveyor\n",
      "not enough examples of lemma:  liaison\n",
      "not enough examples of lemma:  reunion\n",
      "not enough examples of lemma:  pearl\n",
      "not enough examples of lemma:  sage\n",
      "not enough examples of lemma:  cadre\n",
      "not enough examples of lemma:  profundity\n",
      "not enough examples of lemma:  handicap\n",
      "not enough examples of lemma:  coupon\n",
      "not enough examples of lemma:  rod\n",
      "not enough examples of lemma:  ransom\n",
      "not enough examples of lemma:  flush\n",
      "not enough examples of lemma:  bass\n",
      "not enough examples of lemma:  open\n",
      "not enough examples of lemma:  coordination\n",
      "not enough examples of lemma:  peculiarity\n",
      "not enough examples of lemma:  video\n",
      "not enough examples of lemma:  pickup\n",
      "not enough examples of lemma:  stubbornness\n",
      "not enough examples of lemma:  trim\n",
      "not enough examples of lemma:  illumination\n",
      "not enough examples of lemma:  shit\n",
      "not enough examples of lemma:  abode\n",
      "not enough examples of lemma:  accompaniment\n",
      "not enough examples of lemma:  jelly\n",
      "not enough examples of lemma:  feast\n",
      "not enough examples of lemma:  rot\n",
      "not enough examples of lemma:  thaw\n",
      "not enough examples of lemma:  humming\n",
      "not enough examples of lemma:  ministry\n",
      "not enough examples of lemma:  contemplation\n",
      "not enough examples of lemma:  shrubbery\n",
      "not enough examples of lemma:  pretence\n",
      "not enough examples of lemma:  pharmacy\n",
      "not enough examples of lemma:  calendar\n",
      "not enough examples of lemma:  antecedent\n",
      "not enough examples of lemma:  landmark\n",
      "not enough examples of lemma:  gassing\n",
      "not enough examples of lemma:  knot\n",
      "not enough examples of lemma:  crudity\n",
      "not enough examples of lemma:  leave\n",
      "not enough examples of lemma:  hurt\n",
      "not enough examples of lemma:  murmur\n",
      "not enough examples of lemma:  oscillation\n",
      "not enough examples of lemma:  sardine\n",
      "not enough examples of lemma:  sanitation\n",
      "not enough examples of lemma:  monster\n",
      "not enough examples of lemma:  birmingham\n",
      "not enough examples of lemma:  humanity\n",
      "not enough examples of lemma:  creed\n",
      "not enough examples of lemma:  pioneer\n",
      "not enough examples of lemma:  groove\n",
      "not enough examples of lemma:  overture\n",
      "not enough examples of lemma:  lust\n",
      "not enough examples of lemma:  salvo\n",
      "not enough examples of lemma:  loft\n",
      "not enough examples of lemma:  confinement\n",
      "not enough examples of lemma:  index\n",
      "not enough examples of lemma:  malady\n",
      "not enough examples of lemma:  attachment\n",
      "not enough examples of lemma:  self-will\n",
      "not enough examples of lemma:  corduroy\n",
      "not enough examples of lemma:  diplomacy\n",
      "not enough examples of lemma:  arch\n",
      "not enough examples of lemma:  instrumentality\n",
      "not enough examples of lemma:  fullness\n",
      "not enough examples of lemma:  nurse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not enough examples of lemma:  cartwheel\n",
      "not enough examples of lemma:  founder\n",
      "not enough examples of lemma:  misconstruction\n",
      "not enough examples of lemma:  flag\n",
      "not enough examples of lemma:  currency\n",
      "not enough examples of lemma:  deposit\n",
      "not enough examples of lemma:  dealing\n",
      "not enough examples of lemma:  northeast\n",
      "not enough examples of lemma:  hollyhock\n",
      "not enough examples of lemma:  brooding\n",
      "not enough examples of lemma:  aggressor\n",
      "not enough examples of lemma:  tumbler\n",
      "not enough examples of lemma:  portrayal\n",
      "not enough examples of lemma:  inspiration\n",
      "not enough examples of lemma:  pavement\n",
      "not enough examples of lemma:  hypocrisy\n",
      "not enough examples of lemma:  chorus\n",
      "not enough examples of lemma:  toll\n",
      "not enough examples of lemma:  claw\n",
      "not enough examples of lemma:  palette\n",
      "not enough examples of lemma:  hint\n",
      "not enough examples of lemma:  consolidation\n",
      "not enough examples of lemma:  coverage\n",
      "not enough examples of lemma:  attainment\n",
      "not enough examples of lemma:  southwest\n",
      "not enough examples of lemma:  bathing\n",
      "not enough examples of lemma:  assyrian\n",
      "not enough examples of lemma:  nymph\n",
      "not enough examples of lemma:  equality\n",
      "not enough examples of lemma:  exterior\n",
      "not enough examples of lemma:  symbolism\n",
      "not enough examples of lemma:  fermentation\n",
      "not enough examples of lemma:  porter\n",
      "not enough examples of lemma:  chord\n",
      "not enough examples of lemma:  mold\n",
      "not enough examples of lemma:  clown\n",
      "not enough examples of lemma:  keeping\n",
      "not enough examples of lemma:  redundancy\n",
      "not enough examples of lemma:  descendant\n",
      "not enough examples of lemma:  tidewater\n",
      "not enough examples of lemma:  forerunner\n",
      "not enough examples of lemma:  villain\n",
      "not enough examples of lemma:  lease\n",
      "not enough examples of lemma:  grievance\n",
      "not enough examples of lemma:  concurrence\n",
      "not enough examples of lemma:  gum\n",
      "not enough examples of lemma:  emergence\n",
      "not enough examples of lemma:  stimulant\n",
      "not enough examples of lemma:  trio\n",
      "not enough examples of lemma:  blackout\n",
      "not enough examples of lemma:  propagation\n",
      "not enough examples of lemma:  inconvenience\n",
      "not enough examples of lemma:  nobility\n",
      "not enough examples of lemma:  wait\n",
      "not enough examples of lemma:  sitting\n",
      "not enough examples of lemma:  expanse\n",
      "not enough examples of lemma:  tiger\n",
      "not enough examples of lemma:  buzz\n",
      "not enough examples of lemma:  spacing\n",
      "writing to  ../results/semcor_analysis_model.modabs.mc_rae_real.5k.mu1_1.mu2_0.1.mu3_0.001.mu4_5.nnk_4.csv\n",
      "****************************************\n",
      "*** Running correlation on  ../trained_models/model.modabs.mc_rae_real.1k.mu1_1.mu2_0.1.mu3_1e-07.mu4_10.nnk_4 model ***\n",
      "****************************************\n",
      "reading from  ../results/debug_semcor_pairwise_data_model.modabs.mc_rae_real.1k.mu1_1.mu2_0.1.mu3_1e-07.mu4_10.nnk_4.csv\n",
      "not enough examples of lemma:  tan\n",
      "not enough examples of lemma:  velour\n",
      "not enough examples of lemma:  hillbilly\n",
      "not enough examples of lemma:  scouring\n",
      "not enough examples of lemma:  inferiority\n",
      "not enough examples of lemma:  shibboleth\n",
      "not enough examples of lemma:  traditionalism\n",
      "not enough examples of lemma:  bawh\n",
      "not enough examples of lemma:  carelessness\n",
      "not enough examples of lemma:  scraping\n",
      "not enough examples of lemma:  emanation\n",
      "not enough examples of lemma:  wattle\n",
      "not enough examples of lemma:  privy\n",
      "not enough examples of lemma:  penance\n",
      "not enough examples of lemma:  reverse\n",
      "not enough examples of lemma:  surrender\n",
      "not enough examples of lemma:  mask\n",
      "not enough examples of lemma:  pulse\n",
      "not enough examples of lemma:  noun\n",
      "not enough examples of lemma:  disregard\n",
      "not enough examples of lemma:  subscription\n",
      "not enough examples of lemma:  brotherhood\n",
      "not enough examples of lemma:  bankruptcy\n",
      "not enough examples of lemma:  booth\n",
      "not enough examples of lemma:  small\n",
      "not enough examples of lemma:  provincialism\n",
      "not enough examples of lemma:  pinpoint\n",
      "not enough examples of lemma:  inflection\n",
      "not enough examples of lemma:  believer\n",
      "not enough examples of lemma:  contributor\n",
      "not enough examples of lemma:  instability\n",
      "not enough examples of lemma:  rhyme\n",
      "not enough examples of lemma:  confederacy\n",
      "not enough examples of lemma:  zeal\n",
      "not enough examples of lemma:  florist\n",
      "not enough examples of lemma:  streak\n",
      "not enough examples of lemma:  landslide\n",
      "not enough examples of lemma:  lane\n",
      "not enough examples of lemma:  humanism\n",
      "not enough examples of lemma:  hulk\n",
      "not enough examples of lemma:  accessory\n",
      "not enough examples of lemma:  firmness\n",
      "not enough examples of lemma:  honour\n",
      "not enough examples of lemma:  italian\n",
      "not enough examples of lemma:  japanese\n",
      "not enough examples of lemma:  slant\n",
      "not enough examples of lemma:  urging\n",
      "not enough examples of lemma:  helper\n",
      "not enough examples of lemma:  merriment\n",
      "not enough examples of lemma:  heaven\n",
      "not enough examples of lemma:  anachronism\n",
      "not enough examples of lemma:  expansiveness\n",
      "not enough examples of lemma:  pine\n",
      "not enough examples of lemma:  violet\n",
      "not enough examples of lemma:  dictate\n",
      "not enough examples of lemma:  helm\n",
      "not enough examples of lemma:  refinement\n",
      "not enough examples of lemma:  sail\n",
      "not enough examples of lemma:  captivity\n",
      "not enough examples of lemma:  reliance\n",
      "not enough examples of lemma:  cultivation\n",
      "not enough examples of lemma:  random\n",
      "not enough examples of lemma:  binomial\n",
      "not enough examples of lemma:  dash\n",
      "not enough examples of lemma:  primate\n",
      "not enough examples of lemma:  roundhouse\n",
      "not enough examples of lemma:  renewal\n",
      "not enough examples of lemma:  dislocation\n",
      "not enough examples of lemma:  herb\n",
      "not enough examples of lemma:  incarnation\n",
      "not enough examples of lemma:  loophole\n",
      "not enough examples of lemma:  jab\n",
      "not enough examples of lemma:  assist\n",
      "not enough examples of lemma:  flare\n",
      "not enough examples of lemma:  trademark\n",
      "not enough examples of lemma:  drafting\n",
      "not enough examples of lemma:  burn\n",
      "not enough examples of lemma:  layout\n",
      "not enough examples of lemma:  nod\n",
      "not enough examples of lemma:  pore\n",
      "not enough examples of lemma:  dereliction\n",
      "not enough examples of lemma:  backbone\n",
      "not enough examples of lemma:  orgy\n",
      "not enough examples of lemma:  mentality\n",
      "not enough examples of lemma:  down\n",
      "not enough examples of lemma:  marking\n",
      "not enough examples of lemma:  dissent\n",
      "not enough examples of lemma:  allocation\n",
      "not enough examples of lemma:  hound\n",
      "not enough examples of lemma:  detachment\n",
      "not enough examples of lemma:  mouthpiece\n",
      "not enough examples of lemma:  infliction\n",
      "not enough examples of lemma:  allegation\n",
      "not enough examples of lemma:  falsity\n",
      "not enough examples of lemma:  lad\n",
      "not enough examples of lemma:  hysteria\n",
      "not enough examples of lemma:  circus\n",
      "not enough examples of lemma:  syndicate\n",
      "not enough examples of lemma:  gage\n",
      "not enough examples of lemma:  hardship\n",
      "not enough examples of lemma:  bound\n",
      "not enough examples of lemma:  alligator\n",
      "not enough examples of lemma:  tangle\n",
      "not enough examples of lemma:  hoop\n",
      "not enough examples of lemma:  southeast\n",
      "not enough examples of lemma:  keynote\n",
      "not enough examples of lemma:  preoccupation\n",
      "not enough examples of lemma:  signature\n",
      "not enough examples of lemma:  premise\n",
      "not enough examples of lemma:  duplication\n",
      "not enough examples of lemma:  knob\n",
      "not enough examples of lemma:  chute\n",
      "not enough examples of lemma:  brilliance\n",
      "not enough examples of lemma:  frost\n",
      "not enough examples of lemma:  subtlety\n",
      "not enough examples of lemma:  rigger\n",
      "not enough examples of lemma:  showing\n",
      "not enough examples of lemma:  upset\n",
      "not enough examples of lemma:  percussion\n",
      "not enough examples of lemma:  genus\n",
      "not enough examples of lemma:  ringing\n",
      "not enough examples of lemma:  miniature\n",
      "not enough examples of lemma:  viewer\n",
      "not enough examples of lemma:  setup\n",
      "not enough examples of lemma:  rut\n",
      "not enough examples of lemma:  outrage\n",
      "not enough examples of lemma:  rearing\n",
      "not enough examples of lemma:  squatting\n",
      "not enough examples of lemma:  sash\n",
      "not enough examples of lemma:  marketplace\n",
      "not enough examples of lemma:  fancy\n",
      "not enough examples of lemma:  prophecy\n",
      "not enough examples of lemma:  configuration\n",
      "not enough examples of lemma:  congestion\n",
      "not enough examples of lemma:  brevity\n",
      "not enough examples of lemma:  conveyor\n",
      "not enough examples of lemma:  liaison\n",
      "not enough examples of lemma:  reunion\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not enough examples of lemma:  pearl\n",
      "not enough examples of lemma:  sage\n",
      "not enough examples of lemma:  cadre\n",
      "not enough examples of lemma:  profundity\n",
      "not enough examples of lemma:  handicap\n",
      "not enough examples of lemma:  coupon\n",
      "not enough examples of lemma:  rod\n",
      "not enough examples of lemma:  ransom\n",
      "not enough examples of lemma:  flush\n",
      "not enough examples of lemma:  bass\n",
      "not enough examples of lemma:  open\n",
      "not enough examples of lemma:  coordination\n",
      "not enough examples of lemma:  peculiarity\n",
      "not enough examples of lemma:  video\n",
      "not enough examples of lemma:  pickup\n",
      "not enough examples of lemma:  stubbornness\n",
      "not enough examples of lemma:  trim\n",
      "not enough examples of lemma:  illumination\n",
      "not enough examples of lemma:  shit\n",
      "not enough examples of lemma:  abode\n",
      "not enough examples of lemma:  accompaniment\n",
      "not enough examples of lemma:  jelly\n",
      "not enough examples of lemma:  feast\n",
      "not enough examples of lemma:  rot\n",
      "not enough examples of lemma:  thaw\n",
      "not enough examples of lemma:  humming\n",
      "not enough examples of lemma:  ministry\n",
      "not enough examples of lemma:  contemplation\n",
      "not enough examples of lemma:  shrubbery\n",
      "not enough examples of lemma:  pretence\n",
      "not enough examples of lemma:  pharmacy\n",
      "not enough examples of lemma:  calendar\n",
      "not enough examples of lemma:  antecedent\n",
      "not enough examples of lemma:  landmark\n",
      "not enough examples of lemma:  gassing\n",
      "not enough examples of lemma:  knot\n",
      "not enough examples of lemma:  crudity\n",
      "not enough examples of lemma:  leave\n",
      "not enough examples of lemma:  hurt\n",
      "not enough examples of lemma:  murmur\n",
      "not enough examples of lemma:  oscillation\n",
      "not enough examples of lemma:  sardine\n",
      "not enough examples of lemma:  sanitation\n",
      "not enough examples of lemma:  monster\n",
      "not enough examples of lemma:  birmingham\n",
      "not enough examples of lemma:  humanity\n",
      "not enough examples of lemma:  creed\n",
      "not enough examples of lemma:  pioneer\n",
      "not enough examples of lemma:  groove\n",
      "not enough examples of lemma:  overture\n",
      "not enough examples of lemma:  lust\n",
      "not enough examples of lemma:  salvo\n",
      "not enough examples of lemma:  loft\n",
      "not enough examples of lemma:  confinement\n",
      "not enough examples of lemma:  index\n",
      "not enough examples of lemma:  malady\n",
      "not enough examples of lemma:  attachment\n",
      "not enough examples of lemma:  self-will\n",
      "not enough examples of lemma:  corduroy\n",
      "not enough examples of lemma:  diplomacy\n",
      "not enough examples of lemma:  arch\n",
      "not enough examples of lemma:  instrumentality\n",
      "not enough examples of lemma:  fullness\n",
      "not enough examples of lemma:  nurse\n",
      "not enough examples of lemma:  cartwheel\n",
      "not enough examples of lemma:  founder\n",
      "not enough examples of lemma:  misconstruction\n",
      "not enough examples of lemma:  flag\n",
      "not enough examples of lemma:  currency\n",
      "not enough examples of lemma:  deposit\n",
      "not enough examples of lemma:  dealing\n",
      "not enough examples of lemma:  northeast\n",
      "not enough examples of lemma:  hollyhock\n",
      "not enough examples of lemma:  brooding\n",
      "not enough examples of lemma:  aggressor\n",
      "not enough examples of lemma:  tumbler\n",
      "not enough examples of lemma:  portrayal\n",
      "not enough examples of lemma:  inspiration\n",
      "not enough examples of lemma:  pavement\n",
      "not enough examples of lemma:  hypocrisy\n",
      "not enough examples of lemma:  chorus\n",
      "not enough examples of lemma:  toll\n",
      "not enough examples of lemma:  claw\n",
      "not enough examples of lemma:  palette\n",
      "not enough examples of lemma:  hint\n",
      "not enough examples of lemma:  consolidation\n",
      "not enough examples of lemma:  coverage\n",
      "not enough examples of lemma:  attainment\n",
      "not enough examples of lemma:  southwest\n",
      "not enough examples of lemma:  bathing\n",
      "not enough examples of lemma:  assyrian\n",
      "not enough examples of lemma:  nymph\n",
      "not enough examples of lemma:  equality\n",
      "not enough examples of lemma:  exterior\n",
      "not enough examples of lemma:  symbolism\n",
      "not enough examples of lemma:  fermentation\n",
      "not enough examples of lemma:  porter\n",
      "not enough examples of lemma:  chord\n",
      "not enough examples of lemma:  mold\n",
      "not enough examples of lemma:  clown\n",
      "not enough examples of lemma:  keeping\n",
      "not enough examples of lemma:  redundancy\n",
      "not enough examples of lemma:  descendant\n",
      "not enough examples of lemma:  tidewater\n",
      "not enough examples of lemma:  forerunner\n",
      "not enough examples of lemma:  villain\n",
      "not enough examples of lemma:  lease\n",
      "not enough examples of lemma:  grievance\n",
      "not enough examples of lemma:  concurrence\n",
      "not enough examples of lemma:  gum\n",
      "not enough examples of lemma:  emergence\n",
      "not enough examples of lemma:  stimulant\n",
      "not enough examples of lemma:  trio\n",
      "not enough examples of lemma:  blackout\n",
      "not enough examples of lemma:  propagation\n",
      "not enough examples of lemma:  inconvenience\n",
      "not enough examples of lemma:  nobility\n",
      "not enough examples of lemma:  wait\n",
      "not enough examples of lemma:  sitting\n",
      "not enough examples of lemma:  expanse\n",
      "not enough examples of lemma:  tiger\n",
      "not enough examples of lemma:  buzz\n",
      "not enough examples of lemma:  spacing\n",
      "writing to  ../results/semcor_analysis_model.modabs.mc_rae_real.1k.mu1_1.mu2_0.1.mu3_1e-07.mu4_10.nnk_4.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "then we run it\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "for save_path in models:\n",
    "\n",
    "    print(\"****************************************\")\n",
    "    print(\"*** Running correlation on  %s model ***\" % save_path)\n",
    "    print(\"****************************************\")\n",
    "    infile = '../results/debug_semcor_pairwise_data_' + os.path.split(save_path)[1] + '.csv'\n",
    "    outfile = '../results/semcor_analysis_' + os.path.split(save_path)[1] + '.csv'\n",
    "    print(\"reading from \", infile)\n",
    "    out_data = []\n",
    "    \n",
    "    \"\"\"\n",
    "    infile pairwise data columns\n",
    "        \"lemma\",\n",
    "        \"token_sense_1\",\n",
    "        \"token_sense_2\",\n",
    "        \"sense1_pos\",\n",
    "        \"sense2_pos\",\n",
    "        \"cos_sim\",\n",
    "        \"wup_sim\", \n",
    "        \"n_senses\",\n",
    "        \"n_word_forms\",\n",
    "        \"concreteness\"\n",
    "        \"wn_bin\"\n",
    "        \"conc_bin\"\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(infile)\n",
    "    \n",
    "    lemmas = df.lemma.unique()\n",
    "    #print(lemmas[:10])\n",
    "\n",
    "    for word in lemmas:\n",
    "        word_data = df[df.lemma == word]\n",
    "        \n",
    "        n_senses = word_data.iloc[0].n_senses\n",
    "        polysemy_bin = word_data.iloc[0].wn_bin\n",
    "        concreteness_bin = word_data.iloc[0].conc_bin\n",
    "        \n",
    "        pearson, pearson_p, spearman, spearman_p = run_correlation(word_data)\n",
    "        row = (word, len(word_data), n_senses, polysemy_bin, concreteness_bin, pearson, pearson_p, spearman, spearman_p)\n",
    "        #print(corr)\n",
    "        out_data.append(row)\n",
    "        \n",
    "        #raise Exception(\"hfjesh\")\n",
    "    \n",
    "    print(\"writing to \", outfile)\n",
    "    out_df = pd.DataFrame.from_records(out_data, columns = ['word', 'n', 'n_senses', 'polysemy_bin', 'concreteness_bin', 'pearson', 'pearson_p', 'spearman', 'spearman_p'] )\n",
    "    out_df.to_csv(outfile)\n",
    "    \n",
    "    \n",
    "    #for word in \n",
    "    #run_correlation()\n",
    "    #cols = [\"lemma\", \"token_sense_1\", \"token_sense_2\", \"cos_sim\", \"wup_sim\", \"n_senses\"]\n",
    "    #df = csv_input = pd.read_csv(infile, names=cols)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************\n",
      "*** Running correlation on  ../trained_models/model.plsr.mc_rae_real.allbuthomonyms.5k.100components.500max_iters model ***\n",
      "****************************************\n",
      "pearson:  0.40737361212754075\n",
      "p-value:  0.0\n",
      "n lemmas:  8021\n",
      "n tokens:  1045966\n",
      "****************************************\n",
      "*** Running correlation on  ../trained_models/model.plsr.mc_rae_real.allbuthomonyms.1k.50components.500max_iters model ***\n",
      "****************************************\n",
      "pearson:  0.39164415600746316\n",
      "p-value:  0.0\n",
      "n lemmas:  8021\n",
      "n tokens:  1045966\n",
      "****************************************\n",
      "*** Running correlation on  ../trained_models/model.ffnn.mc_rae_real.allbuthomonyms.5k.50epochs.0.5dropout.lr1e-4.hsize300 model ***\n",
      "****************************************\n",
      "pearson:  0.35762292929502393\n",
      "p-value:  0.0\n",
      "n lemmas:  8021\n",
      "n tokens:  1045966\n",
      "****************************************\n",
      "*** Running correlation on  ../trained_models/model.ffnn.mc_rae_real.allbuthomonyms.1k.50epochs.0.5dropout.lr1e-4.hsize300 model ***\n",
      "****************************************\n",
      "pearson:  0.3554822178094441\n",
      "p-value:  0.0\n",
      "n lemmas:  8021\n",
      "n tokens:  1045966\n",
      "****************************************\n",
      "*** Running correlation on  ../trained_models/model.modabs.mc_rae_real.5k.mu1_1.mu2_0.1.mu3_0.001.mu4_5.nnk_4 model ***\n",
      "****************************************\n",
      "pearson:  -0.0335554473300416\n",
      "p-value:  3.040003213857792e-258\n",
      "n lemmas:  8021\n",
      "n tokens:  1045966\n",
      "****************************************\n",
      "*** Running correlation on  ../trained_models/model.modabs.mc_rae_real.1k.mu1_1.mu2_0.1.mu3_1e-07.mu4_10.nnk_4 model ***\n",
      "****************************************\n",
      "pearson:  -0.03320459018694799\n",
      "p-value:  6.507932743078112e-253\n",
      "n lemmas:  8021\n",
      "n tokens:  1045966\n"
     ]
    }
   ],
   "source": [
    "# lets do the correlation on the whole pairwise dataset\n",
    "\n",
    "for save_path in models:\n",
    "    print(\"****************************************\")\n",
    "    print(\"*** Running correlation on  %s model ***\" % save_path)\n",
    "    print(\"****************************************\")\n",
    "    infile = '../results/debug_semcor_pairwise_data_' + os.path.split(save_path)[1] + '.csv'\n",
    "    df = pd.read_csv(infile)\n",
    "    pearson, pearson_p = pearsonr(df.cos_sim, df.wup_sim)\n",
    "    print(\"pearson: \", pearson)\n",
    "    print(\"p-value: \", pearson_p)\n",
    "    print(\"n lemmas: \", len(df['lemma'].unique()))\n",
    "    print(\"n tokens: \", len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets do it on the whole dataset but just for the most concrete nouns\n",
    "# or rather broken down by concreteness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.,  1.,  3.,  0.,  4., nan])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.conc_bin.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************\n",
      "*** Running correlation on  ../trained_models/model.plsr.mc_rae_real.allbuthomonyms.5k.100components.500max_iters model ***\n",
      "****************************************\n",
      "   bin   pearson  pearson_p  n_lemmas  n_tokens\n",
      "3  0.0  0.033708   0.010322        45      5789\n",
      "1  1.0  0.340599   0.000000      1062    170306\n",
      "0  2.0  0.388057   0.000000      1244    361334\n",
      "2  3.0  0.423731   0.000000      1505    265386\n",
      "4  4.0  0.458281   0.000000      1247    192408\n",
      "****************************************\n",
      "*** Running correlation on  ../trained_models/model.plsr.mc_rae_real.allbuthomonyms.1k.50components.500max_iters model ***\n",
      "****************************************\n",
      "   bin   pearson  pearson_p  n_lemmas  n_tokens\n",
      "3  0.0  0.004167   0.751239        45      5789\n",
      "1  1.0  0.326853   0.000000      1062    170306\n",
      "0  2.0  0.357382   0.000000      1244    361334\n",
      "2  3.0  0.409905   0.000000      1505    265386\n",
      "4  4.0  0.438594   0.000000      1247    192408\n",
      "****************************************\n",
      "*** Running correlation on  ../trained_models/model.ffnn.mc_rae_real.allbuthomonyms.5k.50epochs.0.5dropout.lr1e-4.hsize300 model ***\n",
      "****************************************\n",
      "   bin   pearson  pearson_p  n_lemmas  n_tokens\n",
      "3  0.0  0.030294   0.021168        45      5789\n",
      "1  1.0  0.296271   0.000000      1062    170306\n",
      "0  2.0  0.325535   0.000000      1244    361334\n",
      "2  3.0  0.358286   0.000000      1505    265386\n",
      "4  4.0  0.396194   0.000000      1247    192408\n",
      "****************************************\n",
      "*** Running correlation on  ../trained_models/model.ffnn.mc_rae_real.allbuthomonyms.1k.50epochs.0.5dropout.lr1e-4.hsize300 model ***\n",
      "****************************************\n",
      "   bin   pearson  pearson_p  n_lemmas  n_tokens\n",
      "3  0.0  0.009286   0.479959        45      5789\n",
      "1  1.0  0.272961   0.000000      1062    170306\n",
      "0  2.0  0.321267   0.000000      1244    361334\n",
      "2  3.0  0.373254   0.000000      1505    265386\n",
      "4  4.0  0.388408   0.000000      1247    192408\n",
      "****************************************\n",
      "*** Running correlation on  ../trained_models/model.modabs.mc_rae_real.5k.mu1_1.mu2_0.1.mu3_0.001.mu4_5.nnk_4 model ***\n",
      "****************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/scipy/stats/stats.py:4023: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   bin   pearson     pearson_p  n_lemmas  n_tokens\n",
      "3  0.0       NaN           NaN        45      5789\n",
      "1  1.0  0.005503  2.314713e-02      1062    170306\n",
      "0  2.0 -0.009052  5.289346e-08      1244    361334\n",
      "2  3.0 -0.005804  2.789498e-03      1505    265386\n",
      "4  4.0 -0.016512  4.373231e-13      1247    192408\n",
      "****************************************\n",
      "*** Running correlation on  ../trained_models/model.modabs.mc_rae_real.1k.mu1_1.mu2_0.1.mu3_1e-07.mu4_10.nnk_4 model ***\n",
      "****************************************\n",
      "   bin   pearson     pearson_p  n_lemmas  n_tokens\n",
      "3  0.0       NaN           NaN        45      5789\n",
      "1  1.0  0.005444  2.466447e-02      1062    170306\n",
      "0  2.0 -0.009094  4.587882e-08      1244    361334\n",
      "2  3.0 -0.005805  2.784282e-03      1505    265386\n",
      "4  4.0 -0.015756  4.785256e-12      1247    192408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/scipy/stats/stats.py:4023: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for save_path in models:\n",
    "    print(\"****************************************\")\n",
    "    print(\"*** Running correlation on  %s model ***\" % save_path)\n",
    "    print(\"****************************************\")\n",
    "    \n",
    "    infile = '../results/debug_semcor_pairwise_data_' + os.path.split(save_path)[1] + '.csv'\n",
    "    df = pd.read_csv(infile)\n",
    "    #drop rows that contain specific 'value' in 'column_name'\n",
    "    #df = df[df.conc_bin != \"nan\"]\n",
    "    df = df.dropna()\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    for binn in df.conc_bin.unique():\n",
    "\n",
    "        conc_df = df[df.conc_bin == binn]\n",
    "        \n",
    "\n",
    "        pearson, pearson_p = pearsonr(conc_df.cos_sim, conc_df.wup_sim)\n",
    "\n",
    "        \n",
    "        res = {\n",
    "        'bin' : binn,\n",
    "        'pearson': pearson,\n",
    "        'pearson_p' : pearson_p,\n",
    "        'n_lemmas' : len(conc_df['lemma'].unique()),\n",
    "        'n_tokens' : len(conc_df)\n",
    "        }\n",
    "        #print(\"pearson: \", pearson)\n",
    "        #print(\"p-value: \", pearson_p)\n",
    "        #print(\"n lemmas: \", n_lemmas)\n",
    "        #print(\"n tokens: \", n_tokens)\n",
    "        \n",
    "        results.append(res)\n",
    "        \n",
    "    model_results = pd.DataFrame.from_records(results).sort_values(by='bin')\n",
    "    print(model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for reference, bins are\n",
    "\n",
    "    # add in bins\n",
    "    token_similarities['wn_bin'] = pd.cut(token_similarities.n_senses, \n",
    "                        bins = [0, 2.1, 4.1, 6.1, 8.1, 10.1, 20.1, 50.1, 200], labels = False)\n",
    "    token_similarities['conc_bin'] = pd.cut(token_similarities.concreteness, \n",
    "                        bins = [0, 1.5, 2.5, 3.5, 4.5, 10], labels = False)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 5, 2, 0, 3, 4, 6])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "do it for polysemy\n",
    "\"\"\"\n",
    "df.head(5)\n",
    "df.wn_bin.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************\n",
      "*** Running correlation on  ../trained_models/model.plsr.mc_rae_real.allbuthomonyms.5k.100components.500max_iters model ***\n",
      "****************************************\n",
      "   bin   pearson      pearson_p  n_lemmas  n_tokens\n",
      "3    0 -0.055523   8.104212e-89      5587    129304\n",
      "5    1  0.139565   0.000000e+00      1366    137699\n",
      "0    2  0.282986   0.000000e+00       794    290053\n",
      "2    3  0.298289   0.000000e+00       182    182929\n",
      "4    4  0.356167   0.000000e+00        62    107591\n",
      "6    5  0.404870   0.000000e+00        16     49761\n",
      "1    6  0.230541   0.000000e+00        13    141126\n",
      "7    7  0.278117  2.468899e-133         1      7503\n",
      "****************************************\n",
      "*** Running correlation on  ../trained_models/model.plsr.mc_rae_real.allbuthomonyms.1k.50components.500max_iters model ***\n",
      "****************************************\n",
      "   bin   pearson      pearson_p  n_lemmas  n_tokens\n",
      "3    0 -0.058430   3.592427e-98      5587    129304\n",
      "5    1  0.139922   0.000000e+00      1366    137699\n",
      "0    2  0.288850   0.000000e+00       794    290053\n",
      "2    3  0.286651   0.000000e+00       182    182929\n",
      "4    4  0.326469   0.000000e+00        62    107591\n",
      "6    5  0.350748   0.000000e+00        16     49761\n",
      "1    6  0.233465   0.000000e+00        13    141126\n",
      "7    7  0.281309  1.704097e-136         1      7503\n",
      "****************************************\n",
      "*** Running correlation on  ../trained_models/model.ffnn.mc_rae_real.allbuthomonyms.5k.50epochs.0.5dropout.lr1e-4.hsize300 model ***\n",
      "****************************************\n",
      "   bin   pearson      pearson_p  n_lemmas  n_tokens\n",
      "3    0 -0.084747  1.087575e-204      5587    129304\n",
      "5    1  0.119518   0.000000e+00      1366    137699\n",
      "0    2  0.225112   0.000000e+00       794    290053\n",
      "2    3  0.202594   0.000000e+00       182    182929\n",
      "4    4  0.309523   0.000000e+00        62    107591\n",
      "6    5  0.327409   0.000000e+00        16     49761\n",
      "1    6  0.180396   0.000000e+00        13    141126\n",
      "7    7  0.189041   2.585582e-61         1      7503\n",
      "****************************************\n",
      "*** Running correlation on  ../trained_models/model.ffnn.mc_rae_real.allbuthomonyms.1k.50epochs.0.5dropout.lr1e-4.hsize300 model ***\n",
      "****************************************\n",
      "   bin   pearson      pearson_p  n_lemmas  n_tokens\n",
      "3    0 -0.076305  3.211710e-166      5587    129304\n",
      "5    1  0.107587   0.000000e+00      1366    137699\n",
      "0    2  0.230900   0.000000e+00       794    290053\n",
      "2    3  0.239050   0.000000e+00       182    182929\n",
      "4    4  0.296365   0.000000e+00        62    107591\n",
      "6    5  0.300248   0.000000e+00        16     49761\n",
      "1    6  0.148029   0.000000e+00        13    141126\n",
      "7    7  0.190297   4.025149e-62         1      7503\n",
      "****************************************\n",
      "*** Running correlation on  ../trained_models/model.modabs.mc_rae_real.5k.mu1_1.mu2_0.1.mu3_0.001.mu4_5.nnk_4 model ***\n",
      "****************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/scipy/stats/stats.py:4023: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   bin   pearson     pearson_p  n_lemmas  n_tokens\n",
      "3    0 -0.005032  7.040193e-02      5587    129304\n",
      "5    1 -0.023076  1.090836e-17      1366    137699\n",
      "0    2  0.016272  1.883501e-18       794    290053\n",
      "2    3 -0.004632  4.756725e-02       182    182929\n",
      "4    4 -0.057261  7.931490e-79        62    107591\n",
      "6    5       NaN           NaN        16     49761\n",
      "1    6       NaN           NaN        13    141126\n",
      "7    7       NaN           NaN         1      7503\n",
      "****************************************\n",
      "*** Running correlation on  ../trained_models/model.modabs.mc_rae_real.1k.mu1_1.mu2_0.1.mu3_1e-07.mu4_10.nnk_4 model ***\n",
      "****************************************\n",
      "   bin   pearson     pearson_p  n_lemmas  n_tokens\n",
      "3    0 -0.004956  7.471817e-02      5587    129304\n",
      "5    1 -0.022819  2.481891e-17      1366    137699\n",
      "0    2  0.017037  4.465181e-20       794    290053\n",
      "2    3 -0.004741  4.259217e-02       182    182929\n",
      "4    4 -0.057841  2.142861e-80        62    107591\n",
      "6    5       NaN           NaN        16     49761\n",
      "1    6       NaN           NaN        13    141126\n",
      "7    7       NaN           NaN         1      7503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/scipy/stats/stats.py:4023: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for save_path in models:\n",
    "    print(\"****************************************\")\n",
    "    print(\"*** Running correlation on  %s model ***\" % save_path)\n",
    "    print(\"****************************************\")\n",
    "    \n",
    "    infile = '../results/debug_semcor_pairwise_data_' + os.path.split(save_path)[1] + '.csv'\n",
    "    df = pd.read_csv(infile)\n",
    "    \n",
    "    df['wn_bin'] = pd.cut(df.n_senses, \n",
    "                        bins = [0, 1.1, 2.1, 4.1, 6.1, 8.1, 10.1, 20.1,  200], labels = False)\n",
    "    results = []\n",
    "    \n",
    "    for binn in df.wn_bin.unique():\n",
    "\n",
    "        wn_df = df[df.wn_bin == binn]\n",
    "        \n",
    "\n",
    "        pearson, pearson_p = pearsonr(wn_df.cos_sim, wn_df.wup_sim)\n",
    "\n",
    "        \n",
    "        res = {\n",
    "        'bin' : binn,\n",
    "        'pearson': pearson,\n",
    "        'pearson_p' : pearson_p,\n",
    "        'n_lemmas' : len(wn_df['lemma'].unique()),\n",
    "        'n_tokens' : len(wn_df)\n",
    "        }\n",
    "        #print(\"pearson: \", pearson)\n",
    "        #print(\"p-value: \", pearson_p)\n",
    "        #print(\"n lemmas: \", n_lemmas)\n",
    "        #print(\"n tokens: \", n_tokens)\n",
    "        \n",
    "        results.append(res)\n",
    "        \n",
    "    model_results = pd.DataFrame.from_records(results).sort_values(by='bin')\n",
    "    print(model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.         0.75       0.875      0.9        0.91666667 0.9375\n",
      " 0.83333333 0.85714286 0.81818182 0.88888889 0.92857143 0.84615385\n",
      " 0.92307692 0.90909091 0.78571429 0.76470588 0.86666667 0.8125\n",
      " 0.88235294]\n",
      "        Unnamed: 0 lemma token_sense_1 token_sense_2 sense1_pos sense2_pos  \\\n",
      "220389      220389   jew  jew.n.01.Jew  jew.n.01.Jew          n          n   \n",
      "220390      220390   jew  jew.n.01.Jew  jew.n.01.Jew          n          n   \n",
      "220391      220391   jew  jew.n.01.Jew  jew.n.01.Jew          n          n   \n",
      "220392      220392   jew  jew.n.01.Jew  jew.n.01.Jew          n          n   \n",
      "220393      220393   jew  jew.n.01.Jew  jew.n.01.Jew          n          n   \n",
      "\n",
      "         cos_sim  wup_sim  n_senses  n_word_forms  concreteness  wn_bin  \\\n",
      "220389  0.993850     0.75         1             2           NaN       0   \n",
      "220390  0.998647     0.75         1             2           NaN       0   \n",
      "220391  0.994076     0.75         1             2           NaN       0   \n",
      "220392  0.996887     0.75         1             2           NaN       0   \n",
      "220393  0.997168     0.75         1             2           NaN       0   \n",
      "\n",
      "        conc_bin  \n",
      "220389       NaN  \n",
      "220390       NaN  \n",
      "220391       NaN  \n",
      "220392       NaN  \n",
      "220393       NaN  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['jew', 'amateur', 'enzyme', 'compulsive', 'virus', 'follower',\n",
       "       'grownup', 'savannah', 'murderer', 'wakefulness', 'christ',\n",
       "       'napoleon', 'alabaster', 'explorer', 'expert', 'person', 'police',\n",
       "       'thomas huxley', 'choreography', 'christianity', 'lip', 'neighbor',\n",
       "       'chick', 'roleplaying', 'st. louis', 'negro', 'applicant', 'dog',\n",
       "       'dancing', 'carbon tetrachloride', 'gambling', 'assassin',\n",
       "       'individualist', 'bomber', 'red clay', 'alabama', 'coal',\n",
       "       'chicago', 'dexamethasone', 'religious belief', 'avocado',\n",
       "       'houston', 'hand blower', 'high explosives', 'inhabitant',\n",
       "       'scotland yard', 'peer', 'acting', 'berry', 'newspaper critic',\n",
       "       'detroit', 'megaton bombs', 'neutron bombs', 'advocate',\n",
       "       'swelling', 'dweller', 'fingerprint', 'religious beliefs',\n",
       "       'blonde', 'sovereign', 'intellectual', 'national', 'protein',\n",
       "       'vitamin e', 'convert', 'siamese cats', 'patron saints',\n",
       "       'face powder', 'registrant', 'planner', 'jesus christ', 'desert',\n",
       "       'someone', 'baltimore', 'native', 'jesus', 'lefty', 'bel canto',\n",
       "       'antagonist', 'engineer', 'common man', 'pear', 'bartlett pear',\n",
       "       'traveler', 'george washington', 'abraham lincoln', 'bodybuilder',\n",
       "       'killer', 'posse', 'sleeping capsule', 'bath towel', 'monarch',\n",
       "       'nucleic acid', 'atomic bombs', 'sparkle', 'bloodhound', 'newport',\n",
       "       'salt water', 'insulin', 'gunman', 'electric shocks', 'coward',\n",
       "       'megaton bomb', 'working girl', 'singing', 'puppy', 'dead person',\n",
       "       'kidnapper', 'warrior', 'sleeping pill', 'strychnine', 'hound dog',\n",
       "       'scientist', 'butter', 'virgin mary', 'multiple sclerosis',\n",
       "       'heat sink', 'root cellar', 'honolulu', 'acetone', 'police force',\n",
       "       'jowl', 'good person', 'sea water', 'two-by-four', 'germanium',\n",
       "       'hitler', 'alley cat', 'rest rooms', 'gelding', 'fox terrier',\n",
       "       'the virgin', 'inexperienced person', 'mary magdalene',\n",
       "       'tranquilizer', 'game birds', 'julius caesar', 'ground water',\n",
       "       'hurler', 'celebrant', 'modern', 'fortitude', 'decedent',\n",
       "       'heart rate', 'prednisone', 'abe lincoln', 'carbon dioxide',\n",
       "       'kickoff', 'good guys', 'bad guys', 'grape', 'atomic bomb',\n",
       "       'atom bomb', 'distilled water'], dtype=object)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how can we have real correlations for bin 0?\n",
    "\n",
    "test = df[df['wn_bin'] ==0]\n",
    "print(test.wup_sim.unique())\n",
    "\n",
    "print(test[test.wup_sim != 1].head(5))\n",
    "\n",
    "\n",
    "test[test.wup_sim != 1].lemma.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9m/vzvx58rs51v_x5nm620fz4xr0000gn/T/ipykernel_88747/1891269078.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlemma_from_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'christianity.n.01.christianity'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Box Sync/src/features_in_context/notebooks/../lib/utils.py\u001b[0m in \u001b[0;36mlemma_from_string\u001b[0;34m(lemma_string)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;31m# grabs everything in between (' ') in a string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# (needed to update from r\"'(.*?)'\" to deal with cases with quotes in word like o'clock)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0mstring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"\\('(.*?)'\\)\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlemma_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m     \u001b[0;31m#print(string)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mlemma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "lemma_from_string('christianity.n.01.christianity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lemma('christianity.n.01.Christianity')"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('christianity')[0].lemmas()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn = wn.synsets('Christianity')[0].lemmas()[0].synset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8181818181818182"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn.wup_similarity(syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this means sometimes a sense's wup sim with itself is less than 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>lemma</th>\n",
       "      <th>token_sense_1</th>\n",
       "      <th>token_sense_2</th>\n",
       "      <th>sense1_pos</th>\n",
       "      <th>sense2_pos</th>\n",
       "      <th>cos_sim</th>\n",
       "      <th>wup_sim</th>\n",
       "      <th>n_senses</th>\n",
       "      <th>n_word_forms</th>\n",
       "      <th>concreteness</th>\n",
       "      <th>wn_bin</th>\n",
       "      <th>conc_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>149641</th>\n",
       "      <td>149641</td>\n",
       "      <td>line</td>\n",
       "      <td>cable.n.02.line</td>\n",
       "      <td>cable.n.02.line</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>0.490191</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>7</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149642</th>\n",
       "      <td>149642</td>\n",
       "      <td>line</td>\n",
       "      <td>cable.n.02.line</td>\n",
       "      <td>cable.n.02.line</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>0.745497</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>7</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149643</th>\n",
       "      <td>149643</td>\n",
       "      <td>line</td>\n",
       "      <td>cable.n.02.line</td>\n",
       "      <td>cable.n.02.line</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>0.628056</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>7</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149644</th>\n",
       "      <td>149644</td>\n",
       "      <td>line</td>\n",
       "      <td>cable.n.02.line</td>\n",
       "      <td>cable.n.02.line</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>0.709143</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>7</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149645</th>\n",
       "      <td>149645</td>\n",
       "      <td>line</td>\n",
       "      <td>cable.n.02.line</td>\n",
       "      <td>cable.n.02.line</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>0.684494</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>7</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157139</th>\n",
       "      <td>157139</td>\n",
       "      <td>line</td>\n",
       "      <td>telephone_line.n.02.line</td>\n",
       "      <td>channel.n.05.line</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>0.287194</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>7</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157140</th>\n",
       "      <td>157140</td>\n",
       "      <td>line</td>\n",
       "      <td>telephone_line.n.02.line</td>\n",
       "      <td>line.n.22.line</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>0.305778</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>7</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157141</th>\n",
       "      <td>157141</td>\n",
       "      <td>line</td>\n",
       "      <td>wrinkle.n.01.line</td>\n",
       "      <td>channel.n.05.line</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>0.319876</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>7</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157142</th>\n",
       "      <td>157142</td>\n",
       "      <td>line</td>\n",
       "      <td>wrinkle.n.01.line</td>\n",
       "      <td>line.n.22.line</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>0.245727</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>7</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157143</th>\n",
       "      <td>157143</td>\n",
       "      <td>line</td>\n",
       "      <td>channel.n.05.line</td>\n",
       "      <td>line.n.22.line</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>0.404815</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>7</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7503 rows  13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0 lemma             token_sense_1      token_sense_2  \\\n",
       "149641      149641  line           cable.n.02.line    cable.n.02.line   \n",
       "149642      149642  line           cable.n.02.line    cable.n.02.line   \n",
       "149643      149643  line           cable.n.02.line    cable.n.02.line   \n",
       "149644      149644  line           cable.n.02.line    cable.n.02.line   \n",
       "149645      149645  line           cable.n.02.line    cable.n.02.line   \n",
       "...            ...   ...                       ...                ...   \n",
       "157139      157139  line  telephone_line.n.02.line  channel.n.05.line   \n",
       "157140      157140  line  telephone_line.n.02.line     line.n.22.line   \n",
       "157141      157141  line         wrinkle.n.01.line  channel.n.05.line   \n",
       "157142      157142  line         wrinkle.n.01.line     line.n.22.line   \n",
       "157143      157143  line         channel.n.05.line     line.n.22.line   \n",
       "\n",
       "       sense1_pos sense2_pos   cos_sim   wup_sim  n_senses  n_word_forms  \\\n",
       "149641          n          n  0.490191  1.000000        22             2   \n",
       "149642          n          n  0.745497  1.000000        22             2   \n",
       "149643          n          n  0.628056  1.000000        22             2   \n",
       "149644          n          n  0.709143  1.000000        22             2   \n",
       "149645          n          n  0.684494  1.000000        22             2   \n",
       "...           ...        ...       ...       ...       ...           ...   \n",
       "157139          n          n  0.287194  0.133333        22             2   \n",
       "157140          n          n  0.305778  0.625000        22             2   \n",
       "157141          n          n  0.319876  0.266667        22             2   \n",
       "157142          n          n  0.245727  0.125000        22             2   \n",
       "157143          n          n  0.404815  0.133333        22             2   \n",
       "\n",
       "        concreteness  wn_bin  conc_bin  \n",
       "149641           4.5       7       3.0  \n",
       "149642           4.5       7       3.0  \n",
       "149643           4.5       7       3.0  \n",
       "149644           4.5       7       3.0  \n",
       "149645           4.5       7       3.0  \n",
       "...              ...     ...       ...  \n",
       "157139           4.5       7       3.0  \n",
       "157140           4.5       7       3.0  \n",
       "157141           4.5       7       3.0  \n",
       "157142           4.5       7       3.0  \n",
       "157143           4.5       7       3.0  \n",
       "\n",
       "[7503 rows x 13 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "lets check out this super polysemous word\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_csv('../results/debug_semcor_pairwise_data_model.plsr.buchanan.allbuthomonyms.5k.300components.500max_iters.csv')\n",
    "\n",
    "\n",
    "df['wn_bin'] = pd.cut(df.n_senses, \n",
    "                        bins = [0, 1.1, 2.1, 4.1, 6.1, 8.1, 10.1, 20.1,  200], labels = False)\n",
    "\n",
    "line = df[df['n_senses'] > 20]\n",
    "line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#dfSample = df.sample(1000, weights='wup_sim') # This is the importante line\n",
    "#xdataSample, ydataSample = dfSample[\"cos_sim\"], dfSample[\"wup_sim\"]\n",
    "\n",
    "lineminus1 = line[line['wup_sim'] != 1]\n",
    "\n",
    "xdataSample, ydataSample = lineminus1[\"cos_sim\"], lineminus1[\"wup_sim\"]\n",
    "\n",
    "sns.regplot(x=ydataSample, y=xdataSample) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['structure', 'need', 'washington', 'idea', 'growth', 'amount',\n",
       "       'earth', 'pain', 'box', 'hour', 'range', 'motif', 'game',\n",
       "       'material', 'water', 'example', 'rank', 'second', 'product',\n",
       "       'share', 'fork', 'film', 'start', 'officer', 'distribution',\n",
       "       'mess', 'help', 'provision', 'administration', 'bloom', 'fly',\n",
       "       'replacement', 'spread', 'conversion', 'hearing', 'trouble',\n",
       "       'contribution', 'bill', 'affair', 'question', 'match',\n",
       "       'production', 'function', 'community', 'literature', 'hope',\n",
       "       'bolt', 'boy', 'living', 'target', 'eye', 'circumstance', 'brain',\n",
       "       'nerve', 'office', 'lesson', 'church', 'gain', 'sphere', 'roll',\n",
       "       'capital', 'room', 'trace', 'post', 'program', 'reception',\n",
       "       'memory', 'wire', 'report', 'watch', 'nature', 'balance', 'item',\n",
       "       'truth', 'interference', 'floor', 'leadership', 'prayer', 'faith',\n",
       "       'act', 'energy', 'quality', 'card', 'limit', 'exercise', 'fact',\n",
       "       'politics', 'distance', 'influence', 'news', 'skin', 'difficulty',\n",
       "       'treatment', 'style', 'residence', 'south', 'discipline', 'role',\n",
       "       'possibility', 'market', 'weight', 'court', 'effect', 'dream',\n",
       "       'conflict', 'element', 'interpretation', 'argument', 'address',\n",
       "       'focus', 'decision', 'manner', 'chain', 'expedition', 'rest',\n",
       "       'formula', 'crack', 'wing', 'difference', 'book', 'grace',\n",
       "       'height', 'population', 'object', 'identity', 'consideration',\n",
       "       'stop', 'trust', 'painting', 'tension', 'variation', 'reason',\n",
       "       'scale', 'reservation', 'guard', 'peace', 'reach', 'contrast',\n",
       "       'portion', 'instruction', 'economy', 'success', 'organization',\n",
       "       'speech', 'color', 'grade', 'origin', 'north', 'mechanic', 'soul',\n",
       "       'understanding', 'conception', 'solution', 'estimate', 'blood',\n",
       "       'agreement', 'wisdom', 'institution', 'comfort', 'employment',\n",
       "       'crossroad', 'mission', 'factor', 'distinction', 'bank', 'frame',\n",
       "       'practice', 'gray', 'condition', 'label', 'abstraction', 'pot',\n",
       "       'occasion', 'race', 'twist', 'violation', 'strip', 'rule',\n",
       "       'sequence', 'suffering', 'passion', 'appreciation', 'canvas',\n",
       "       'couple', 'beam', 'tail', 'agent', 'exposure', 'peak', 'stream',\n",
       "       'culture', 'track', 'port', 'witness', 'middle', 'illustration',\n",
       "       'collection', 'taste', 'association', 'cap', 'drama', 'crash',\n",
       "       'receiver', 'print', 'reflection', 'quarter', 'first', 'shade',\n",
       "       'separation', 'excitement', 'counter', 'complement', 'allowance',\n",
       "       'bow', 'inventory', 'bench', 'regard', 'denial', 'talk', 'reserve',\n",
       "       'attraction', 'acceptance', 'medium', 'runner', 'discharge',\n",
       "       'pull', 'slice', 'stamp', 'disturbance', 'introduction'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foursenses = df[df['n_senses'] == 4]['lemma'].unique()\n",
    "foursenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "discip = df[df['lemma'] == 'crash']\n",
    "\n",
    "\n",
    "xdataSample, ydataSample = discip[\"cos_sim\"], discip[\"wup_sim\"]\n",
    "\n",
    "sns.regplot(x=ydataSample, y=xdataSample) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>lemma</th>\n",
       "      <th>token_sense_1</th>\n",
       "      <th>token_sense_2</th>\n",
       "      <th>sense1_pos</th>\n",
       "      <th>sense2_pos</th>\n",
       "      <th>cos_sim</th>\n",
       "      <th>wup_sim</th>\n",
       "      <th>n_senses</th>\n",
       "      <th>n_word_forms</th>\n",
       "      <th>concreteness</th>\n",
       "      <th>wn_bin</th>\n",
       "      <th>conc_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>communication</td>\n",
       "      <td>communication.n.01.communication</td>\n",
       "      <td>communication.n.01.communication</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>0.828119</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>communication</td>\n",
       "      <td>communication.n.01.communication</td>\n",
       "      <td>communication.n.01.communication</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>0.708595</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>communication</td>\n",
       "      <td>communication.n.01.communication</td>\n",
       "      <td>communication.n.01.communication</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>0.798475</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>communication</td>\n",
       "      <td>communication.n.01.communication</td>\n",
       "      <td>communication.n.01.communication</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>0.726443</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>communication</td>\n",
       "      <td>communication.n.01.communication</td>\n",
       "      <td>communication.n.01.communication</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>0.862673</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0          lemma                     token_sense_1  \\\n",
       "0           0  communication  communication.n.01.communication   \n",
       "1           1  communication  communication.n.01.communication   \n",
       "2           2  communication  communication.n.01.communication   \n",
       "3           3  communication  communication.n.01.communication   \n",
       "4           4  communication  communication.n.01.communication   \n",
       "\n",
       "                      token_sense_2 sense1_pos sense2_pos   cos_sim  wup_sim  \\\n",
       "0  communication.n.01.communication          n          n  0.828119      1.0   \n",
       "1  communication.n.01.communication          n          n  0.708595      1.0   \n",
       "2  communication.n.01.communication          n          n  0.798475      1.0   \n",
       "3  communication.n.01.communication          n          n  0.726443      1.0   \n",
       "4  communication.n.01.communication          n          n  0.862673      1.0   \n",
       "\n",
       "   n_senses  n_word_forms  concreteness  wn_bin  conc_bin  \n",
       "0         3             2           2.9       2       2.0  \n",
       "1         3             2           2.9       2       2.0  \n",
       "2         3             2           2.9       2       2.0  \n",
       "3         3             2           2.9       2       2.0  \n",
       "4         3             2           2.9       2       2.0  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
