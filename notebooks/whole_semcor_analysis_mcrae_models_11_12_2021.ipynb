{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data\n",
    "\n",
    "First we will create a dictionary of Semcor words, and look at them and their frequencies.\n",
    "\n",
    "\n",
    "Next, we want to create a dataset of a subsample of semcor. We want to remove the most common and least common words\n",
    "\n",
    "\n",
    "We limit this set in several ways:\n",
    "    - only noun senses\n",
    "    - max 30 examples of each sense of a word.\n",
    "    - concrete\n",
    "    - remove nominalizations, which tend to have eventive readings (we are interested in nouns denoting entities)\n",
    "\n",
    "So, we begin iterating through a randomly shuffled semcor. For each word, we throw it out if it does not fit our criteria. Then, we look at the senses.\n",
    "\n",
    "\n",
    "\n",
    "At the end, we store a list of all of the words we've collected. For each item in the dictionary, we should know:\n",
    "- the number of tokens\n",
    "- the wordnet senses\n",
    "- a list of the semcor sentence indices of the tokens of each word. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.modeling:loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /Users/gabriellachronis/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "INFO:pytorch_pretrained_bert.modeling:extracting archive file /Users/gabriellachronis/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /var/folders/9m/vzvx58rs51v_x5nm620fz4xr0000gn/T/tmp62oirfeo\n",
      "INFO:pytorch_pretrained_bert.modeling:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/gabriellachronis/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../lib/\")\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from nltk.corpus import semcor\n",
    "#from nltk.tree import Tree\n",
    "#import itertools\n",
    "#import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "from bert import *\n",
    "import csv\n",
    "from nltk.corpus.reader.wordnet import Lemma\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import inflect\n",
    "import os\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "import re\n",
    "from lib.utils import *\n",
    "\n",
    "\n",
    "bert = BERTBase()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatizer = WordNetLemmatizer()\n",
    "# lemmatizer.lemmatize(\"impressed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# load semcor stats\n",
    "# \"\"\"\n",
    "\n",
    "# #uncomment for whole dataset\n",
    "# sents = semcor.sents()\n",
    "# tagged_sents = semcor.tagged_sents( tag = ' sem ' )\n",
    "# words = semcor.words()\n",
    "\n",
    "\n",
    "# ##########\n",
    "# # DEBUG ONLY\n",
    "# ############\n",
    "\n",
    "# # tagged_sents = semcor.tagged_sents( tag = ' sem ' )[:20]\n",
    "# # sents = semcor.sents()[:20]\n",
    "# # words = semcor.words()[:1000]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lala = semcor.tagged_sents( tag = ' sem ' )[:20]\n",
    "# lala = lala[0][1]\n",
    "# lala.pos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_senses_in_tagged_sentence(tagged_sentence):\n",
    "#     \"\"\"\n",
    "#     given a sense-tagged corpus sentence,returns a list of lemmas and senses in that sentence\n",
    "#     \"\"\"\n",
    "#     res = []\n",
    "#     for chunk in tagged_sentence:\n",
    "        \n",
    "        \n",
    "#         chunk_string = ' '.join(chunk.leaves())\n",
    "\n",
    "#         word = chunk_string.lower()\n",
    "#         lemma = lemmatizer.lemmatize(word)\n",
    "#         poss = chunk.pos()\n",
    "        \n",
    "#         \"\"\"\n",
    "#         if we find a wordnet sense (function words dont)\n",
    "#         then scoop it up\n",
    "\n",
    "#         \"\"\"            \n",
    "#         if isinstance(chunk.label() , Lemma):\n",
    "#             sense = chunk.label()\n",
    "#             for wordform, pos in poss:\n",
    "#                 res.append((lemma, sense, pos))\n",
    "#     # if we get to the end of the loop. we didn't find the word we were looking for\n",
    "#     return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sents[0])\n",
    "get_senses_in_tagged_sentence(tagged_sents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# little script on semcor_lexicon to see where we should cut off most and least frequent\n",
    "# \"\"\"\n",
    "\n",
    "# print(semcor_lexicon.most_common(200))\n",
    "# n = 30000\n",
    "# print(semcor_lexicon.most_common()[:-n-1:-1])\n",
    "\n",
    "# # we want to keep words with a count < 600\n",
    "\n",
    "# # and with a count greater than > 10 (which is knocking off the l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# get basic semcor stats\n",
    "# \"\"\"\n",
    "# print(\"number of sentences:\")\n",
    "# print(len(sents))\n",
    "# print(\"number of tokens:\")\n",
    "# print(len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Create Token Index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Next step is to create an index of all of the tokens of a single lemma. \n",
    "# So, we build a data structure with all of the word forms found in semcor. With each word form,\n",
    "# we store a list of all of the sentences containing it.\n",
    "# \"\"\"\n",
    "\n",
    "# class Vividict(dict):\n",
    "#     def __missing__(self, key):\n",
    "#         value = self[key] = type(self)() # retain local pointer to value\n",
    "#         return value                     # faster to return than dict lookup\n",
    "\n",
    "\n",
    "# #word_index = {}\n",
    "# sense_index = Vividict()\n",
    "\n",
    "# semcor_indices = list(range(0,len(sents)))\n",
    "# #print(semcor_indices)\n",
    "# random.shuffle(semcor_indices)\n",
    "# #print(semcor_indices)\n",
    "\n",
    "\n",
    "# # go through the dataset sentence by sentence\n",
    "# for random_index in semcor_indices:\n",
    "\n",
    "#     sentence_id = random_index\n",
    "#     sent = tagged_sents[sentence_id]\n",
    "\n",
    "    \n",
    "#     # go through the sentence word by word to get semcor senses in it\n",
    "#     for word in sent:\n",
    "#         senses = get_senses_in_tagged_sentence(sent)\n",
    "#         for lemma, sense, word_form, pos in senses:\n",
    "#             sense = str(sense)\n",
    "            \n",
    "#             if pos != 'NN':\n",
    "#                 continue\n",
    "#             # if this is our first time seeing this word, add it to the index and put the sentence id in the entry\n",
    "#             elif sense not in sense_index[lemma]:\n",
    "#                 sense_index[lemma][sense][word_form] = {sentence_id}\n",
    "#             # if we have too many instances of this sense, stop\n",
    "#             elif len(sense_index[lemma][sense][word_form]) >= 30:\n",
    "#                 continue\n",
    "#             # otherwise add it\n",
    "#             else:\n",
    "#                 sense_index[lemma][sense][word_form].add(sentence_id)\n",
    "        \n",
    "# #     # we need to make sure we are collecting only those tokens which have semcor senses, or we make note of which ones do\n",
    "    \n",
    "# #         # if this is our first time seeing this word, add it to the index and put the sentence id in the entry\n",
    "# #         if word not in word_index:\n",
    "# #             word = word.lower()\n",
    "# #             word_index[word] = {sentence_id}\n",
    "# #         # otherwise, add the sentence id to the entry for the word\n",
    "# #         else:\n",
    "# #             word_index[word].add(sentence_id)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "let's take a look at it\n",
    "\"\"\"\n",
    "import pprint\n",
    "#pprint.pprint(sense_index, width=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Lemma('render.v.07.return'): 'bar'}\n",
      "{\"Lemma('render.v.07.return')\": 'foo', \"Lemma('return.v.01.return')\": 'bar'}\n",
      "{\"Lemma('render.v.07.return')\": 'foo', \"Lemma('return.v.01.return')\": 'bar'}\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-d37a57e9acc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#re.findall(r\"\\('(.*?)'\\)\", lemma_string)[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\".*(.*?)'\\)\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'render.v.07.return'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# \"\"\"\n",
    "# this is a little nonsense to figure out how to use nltk lemma types - - not needed for script\n",
    "# \"\"\"\n",
    "\n",
    "# render1 = wn.lemma('render.v.07.return')\n",
    "# render2 = wn.lemma('return.v.01.return')\n",
    "\n",
    "# # \"\"\"\n",
    "# # importnt point about nltk wordnet lemmas. their representation is confusing so be careful. i think equals or differentequals are implmementd in\n",
    "# # unsuspected ways, because you get issues where they dont act like their display name\n",
    "# # \"\"\"\n",
    "\n",
    "# dictz = {render1: \"foo\", render2: \"bar\"}\n",
    "# print(dictz)\n",
    "\n",
    "# dixt = {str(render1): \"foo\"}\n",
    "# dixt[str(render2)] = \"bar\"\n",
    "# print(dixt)\n",
    "\n",
    "\n",
    "# dixt = {str(render1): \"foo\"}\n",
    "# dixt[str(render2)] = \"bar\"\n",
    "# print(dixt)\n",
    "\n",
    "# #re.findall(r\"\\('(.*?)'\\)\", lemma_string)[0]\n",
    "\n",
    "# #re.findall(r\".*(.*?)'\\)\", 'render.v.07.return')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Now that we have our word index, we want to construct the evaluation dataset\n",
    "\n",
    "# for each word in the index, we want\n",
    "\n",
    "# We iterate through the words in the dictionary.\n",
    "# we shuffle these indices and access in random order. \n",
    "# We go through the shuffled indices,\n",
    "#     and we check if we have collected < 50 of this sense.\n",
    "#     if not, we collect this token for the evaluation dataset\n",
    "\n",
    "# collection means:\n",
    "#     we construct a row of data like\n",
    "#         word lemma\n",
    "#         word sense\n",
    "#         token sentence\n",
    "        \n",
    "        \n",
    "# at the end we save the data in a csv file called 'semcor_wu_palmer_eval_data.csv'\n",
    "# \"\"\"\n",
    "\n",
    "# def get_sense_in_tagged_sentence(word, tagged_sentence):\n",
    "#     for chunk in tagged_sentence:\n",
    "\n",
    "#         chunk_string = ' '.join(chunk.leaves())\n",
    "\n",
    "#         \"\"\"\n",
    "#         if we find the word we're looking for in this chunk,\n",
    "#         and that chunk has a wordnet sense (function words dont)\n",
    "#         then scoop it up\n",
    "\n",
    "#         \"\"\"            \n",
    "#         if chunk_string.lower() == word:\n",
    "#             #print(\"found %s\" % word)\n",
    "#             #print(chunk.label())\n",
    "\n",
    "#             #wn_lemma = cunk.label()\n",
    "#             if isinstance(chunk.label() , Lemma):\n",
    "#                 return chunk.label()\n",
    "#     # if we get to the end of the loop. we didn't find the word we were looking for\n",
    "#     return None\n",
    "\n",
    "\n",
    "# def collect_tokens(indices, sents, tagged_sents):\n",
    "#     \"\"\"\n",
    "#     takes a word and a list of indices\n",
    "#     returns tuples containing \n",
    "#         word\n",
    "#         sentence_string\n",
    "#         sense\n",
    "#     \"\"\"\n",
    "#     #sense_count = 0\n",
    "#     tokens = []\n",
    "    \n",
    "#     # indices is a list of all of the sentence ids containing this word\n",
    "#     indices = list(indices)\n",
    "#     # visit these sentences in random order\n",
    "#     random.shuffle(indices)\n",
    "#     for index in indices[:25]:\n",
    "\n",
    "#         sentence = sents[index]\n",
    "#         sentence = ' '.join(sentence)\n",
    "\n",
    "#         tokens.append(sentence)\n",
    "#         #sense_count += 1\n",
    "        \n",
    "        \n",
    "    \n",
    "#     #print(sense_count.items())\n",
    "#     return tokens\n",
    "\n",
    "\n",
    "# def collect_tokens_for_all_words_to_file(path, sense_path, sense_index, sents, tagged_sents):\n",
    "#     with open(path, 'w', newline='') as csvfile:\n",
    "#         writer = csv.writer(csvfile)\n",
    "\n",
    "#         with open(sense_path, 'w', newline='') as sensefile:\n",
    "#             sensewriter = csv.writer(sensefile)\n",
    "        \n",
    "#             for lemma in sense_index.keys():\n",
    "#                 #print(lemma)\n",
    "#                 for sense, indices in sense_index[lemma].items():\n",
    "#                     print(lemma)\n",
    "#                     print(sense)\n",
    "#                     #print(indices)\n",
    "\n",
    "                \n",
    "#                     frequency = len(indices)\n",
    "#                     tokens = collect_tokens(indices, sents, tagged_sents)\n",
    "#                     #print(tokens)\n",
    "#                     #raise Exception(\"nfwip\")\n",
    "\n",
    "#                     for token in tokens:\n",
    "#                         row = (lemma, sense, token)\n",
    "#                         writer.writerow(row)                    \n",
    "#                     #sensewriter.writerow(sense_count.items())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# collect_tokens_for_all_words_to_file('../data/semcor_wu_palmer_eval_datamcrae.csv', '../data/semcor_sense_counts_mcrae.csv', sense_index, sents, tagged_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict feature vectors and save pairwise token data;\n",
    "# Run correlation analysis;\n",
    "# Saturate with other information;\n",
    "\n",
    "Because you already have the data from the\n",
    "    \"Collect semcor eval data.ipynb\" script\n",
    "in the right format in the form of \n",
    "    [lemma, sense, word_form, context]\n",
    "in the file\n",
    "    \"data/semcor_eval_data_11_27_2021.csv\"\n",
    "    \n",
    "All the above code is moot and improved. We simply need to load in this file of eval data and work from there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "models and save paths\n",
    "\"\"\"\n",
    "\n",
    "models = [\n",
    "    ## buchanan\n",
    "    '../trained_models/model.plsr.buchanan.allbuthomoyms.5k.300components.500max_iters',\n",
    "    '../trained_models/model.plsr.buchanan.allbuthomoyms.1k.300components.500max_iters',\n",
    "    '../trained_models/model.ffnn.buchanan.allbuthomoyms.5k.50epochs.0.5dropout.lr1e-4.hsize300',\n",
    "    '../trained_models/model.ffnn.buchanan.allbuthomoyms.1k.50epochs.0.5dropout.lr1e-4.hsize300',\n",
    "    #'../trained_models/model.modabs.buchanan.allbuthomoyms.5k',\n",
    "    #'../trained_models/model.modabs.buchanan.allbuthomoyms.1k',\n",
    "    ### mcrae\n",
    "    '../trained_models/model.plsr.mc_rae_real.allbuthomoyms.5k.100components.500max_iters',\n",
    "    '../trained_models/model.plsr.mc_rae_real.allbuthomoyms.1k.50components.500max_iters',\n",
    "    '../trained_models/model.ffnn.mc_rae_real.allbuthomoyms.5k.50epochs.0.5dropout.lr1e-4.hsize300',\n",
    "    '../trained_models/model.ffnn.mc_rae_real.allbuthomoyms.1k.50epochs.0.5dropout.lr1e-4.hsize300',\n",
    "    #### binder\n",
    "    'trained_models/model.ffnn.binder.5k.50epochs.0.5dropout.lr1e-4.hsize300',\n",
    "    'trained_models/model.ffnn.binder.1k.50epochs.0.5dropout.lr1e-4.hsize300',\n",
    "    \n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def lemma_from_string(lemma_string):\n",
    "#     # grabs everything in between (' ') in a string\n",
    "#     # (needed to update from r\"'(.*?)'\" to deal with cases with quotes in word like o'clock)\n",
    "#     string = re.findall(r\"\\('(.*?)'\\)\", lemma_string)[0]\n",
    "#     #print(string)\n",
    "#     lemma = wn.lemma(string)\n",
    "#     return lemma\n",
    "\n",
    "# def lemma_name_from_string(lemma_string):\n",
    "#     # grabs everything in between (' ') in a string\n",
    "#     # (needed to update from r\"'(.*?)'\" to deal with cases with quotes in word like o'clock)\n",
    "#     string = re.findall(r\"\\('(.*?)'\\)\", lemma_string)[0]\n",
    "#     #print(string)\n",
    "#     return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now, we have our dataset that we want to analyze. We just need to do:\n",
    "\n",
    "for each model we want to evaluate, run the following script:\n",
    "\n",
    "open the file of data\n",
    "\n",
    "read it in as a dataframe\n",
    "\n",
    "for each of the unique words in that dataset\n",
    "\n",
    "    we calculate pairwise distances between each otoken and every otehr token\n",
    "    and construct a similarities dataset. \n",
    "    \n",
    "    then we run correlations for that word???\n",
    "    and store into a file\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def make_predictions(df, model, bert):\n",
    "    \"\"\"\n",
    "    df has columns\n",
    "    [lemma, sense, word_form, context]\n",
    "    \n",
    "    for a single semcor lemma\n",
    "    \"\"\"    \n",
    "    predictions = []\n",
    "    for index, row in df.iterrows():\n",
    "\n",
    "        #print(row.word_form)\n",
    "        #print(row.context)\n",
    "\n",
    "        predicted_vector = model.predict_in_context(row.word_form, row.context, bert)\n",
    "\n",
    "        predictions.append(predicted_vector)\n",
    "    return predictions\n",
    "\n",
    "def get_pairwise_wu_palmer_data(model, df, bert, outfile):\n",
    "    \n",
    "    \"\"\"\n",
    "    df has columns\n",
    "    [lemma, sense, word_form, context]\n",
    "    \"\"\"\n",
    "    unique_words = df.lemma.unique()\n",
    "    \n",
    "    #run_stats = [0] * len(unique_words)\n",
    "    run_stats = []\n",
    "\n",
    "    for i in range(0, len(unique_words)):\n",
    "        if i % 500 == 0:\n",
    "            print(\"processed %s words\" % i)\n",
    "        \n",
    "        # a dataframe containing all the tokens of this word\n",
    "        word = unique_words[i]\n",
    "        word_data = df[df.lemma == word].copy()\n",
    "\n",
    "        n_senses = len(word_data['sense'].unique())\n",
    "\n",
    "        predictions = make_predictions(word_data, model, bert)\n",
    "        \n",
    "        word_data['prediction'] = predictions\n",
    "        \n",
    "        #print(word_data)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        then we calculate the pairwise distances between all of the vectors, only counting one pair one time\n",
    "\n",
    "        \"\"\"\n",
    "        vals_for_this_word = []\n",
    "#         cosines_for_this_word = []\n",
    "#         wup_sims_for_this_word = []\n",
    "#         sense_1 = []\n",
    "#         sense_2 = []\n",
    "\n",
    "        # pop the first token off the list\n",
    "        num_toks = len(word_data)\n",
    "        for i in range(0,num_toks):\n",
    "            # compare it with each of the other tokens\n",
    "            # dont have to compare to any earlier\n",
    "            for j in range(i+1,num_toks):\n",
    "\n",
    "                #print(df.iloc[i])\n",
    "                #print(df.iloc[j])\n",
    "\n",
    "                # calculate cosine similarity between the two vectors\n",
    "                cos_sim = 1 - cosine(word_data.iloc[i].prediction, word_data.iloc[j].prediction)\n",
    "\n",
    "                # and wu palmer similarity between the two wn lemmas\n",
    "                lemma_1 = lemma_from_string(word_data.iloc[i].sense)\n",
    "                lemma_2 = lemma_from_string(word_data.iloc[j].sense)\n",
    "                synset1 = lemma_1.synset()\n",
    "                synset2 = lemma_2.synset()\n",
    "                wup_sim = synset1.wup_similarity(synset2)\n",
    "\n",
    "                # if we can't compute a distance for these senses / recognize them, discard\n",
    "                if type(wup_sim) == float:\n",
    "                    # store this data point into a list\n",
    "                    vals_for_this_word.append((word, lemma_1, lemma_2, cos_sim, wup_sim))\n",
    "        \n",
    "        \n",
    "        token_similarities = pd.DataFrame.from_records(vals_for_this_word, columns = [\"lemma\", \"token_sense_1\", \"token_sense_2\", \"cos_sim\", \"wup_sim\"])\n",
    "        token_similarities['n_senses'] = n_senses\n",
    "        \n",
    "        #print(token_similarities)\n",
    "        token_similarities.to_csv(outfile, mode='a', header=False) # TODO true creates problems here\n",
    "        \n",
    "    return None\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "def plot_sims():\n",
    "    cos_sims = sense_similarities['cos_sim']\n",
    "    wup_sims = sense_similarities['wup_sim']\n",
    "    plt.scatter(wup_sims, cos_sims)\n",
    "    plt.title(\"Wordnet similarity of homonymous senses plotted against cosine similarity of predicted vectors of two tokens in semantic feature space\")\n",
    "    plt.xlabel(\"Wu and Palmer Similarity\")\n",
    "    plt.ylabel(\"Cosine Similarity\")\n",
    "    plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************\n",
      "*** Evaluating ../trained_models/model.plsr.mc_rae_real.1k.50components.500max_iters model ***\n",
      "****************************************\n",
      "processed 0 words\n",
      "processed 500 words\n",
      "processed 1000 words\n",
      "processed 1500 words\n",
      "processed 2000 words\n",
      "processed 2500 words\n",
      "processed 3000 words\n",
      "processed 3500 words\n",
      "processed 4000 words\n",
      "processed 4500 words\n",
      "processed 5000 words\n",
      "processed 5500 words\n",
      "processed 6000 words\n",
      "processed 6500 words\n",
      "processed 7000 words\n",
      "processed 7500 words\n",
      "processed 8000 words\n",
      "processed 8500 words\n",
      "processed 9000 words\n",
      "processed 9500 words\n",
      "processed 10000 words\n",
      "processed 10500 words\n",
      "processed 11000 words\n",
      "processed 11500 words\n",
      "****************************************\n",
      "*** Evaluating ../trained_models/model.plsr.mc_rae_real.5k.100components.500max_iters model ***\n",
      "****************************************\n",
      "processed 0 words\n",
      "processed 500 words\n",
      "processed 1000 words\n",
      "processed 1500 words\n",
      "processed 2000 words\n",
      "processed 2500 words\n",
      "processed 3000 words\n",
      "processed 3500 words\n",
      "processed 4000 words\n",
      "processed 4500 words\n",
      "processed 5000 words\n",
      "processed 5500 words\n",
      "processed 6000 words\n",
      "processed 6500 words\n",
      "processed 7000 words\n",
      "processed 7500 words\n",
      "processed 8000 words\n",
      "processed 8500 words\n",
      "processed 9000 words\n",
      "processed 9500 words\n",
      "processed 10000 words\n",
      "processed 10500 words\n",
      "processed 11000 words\n",
      "processed 11500 words\n",
      "****************************************\n",
      "*** Evaluating ../trained_models/model.ffnn.mc_rae_real.1k.50epochs.0.5dropout.lr1e-4.hsize300 model ***\n",
      "****************************************\n",
      "processed 0 words\n",
      "processed 500 words\n",
      "processed 1000 words\n",
      "processed 1500 words\n",
      "processed 2000 words\n",
      "processed 2500 words\n",
      "processed 3000 words\n",
      "processed 3500 words\n",
      "processed 4000 words\n",
      "processed 4500 words\n",
      "processed 5000 words\n",
      "processed 5500 words\n",
      "processed 6000 words\n",
      "processed 6500 words\n",
      "processed 7000 words\n",
      "processed 7500 words\n",
      "processed 8000 words\n",
      "processed 8500 words\n",
      "processed 9000 words\n",
      "processed 9500 words\n",
      "processed 10000 words\n",
      "processed 10500 words\n",
      "processed 11000 words\n",
      "processed 11500 words\n",
      "****************************************\n",
      "*** Evaluating ../trained_models/model.ffnn.mc_rae_real.5k.50epochs.0.5dropout.lr1e-4.hsize300 model ***\n",
      "****************************************\n",
      "processed 0 words\n",
      "processed 500 words\n",
      "processed 1000 words\n",
      "processed 1500 words\n",
      "processed 2000 words\n",
      "processed 2500 words\n",
      "processed 3000 words\n",
      "processed 3500 words\n",
      "processed 4000 words\n",
      "processed 4500 words\n",
      "processed 5000 words\n",
      "processed 5500 words\n",
      "processed 6000 words\n",
      "processed 6500 words\n",
      "processed 7000 words\n",
      "processed 7500 words\n",
      "processed 8000 words\n",
      "processed 8500 words\n",
      "processed 9000 words\n",
      "processed 9500 words\n",
      "processed 10000 words\n",
      "processed 10500 words\n",
      "processed 11000 words\n",
      "processed 11500 words\n",
      "****************************************\n",
      "*** Evaluating ../trained_models/model.modabs.mc_rae_real.1k model ***\n",
      "****************************************\n",
      "processed 0 words\n",
      "processed 500 words\n",
      "processed 1000 words\n",
      "processed 1500 words\n",
      "processed 2000 words\n",
      "processed 2500 words\n",
      "processed 3000 words\n",
      "processed 3500 words\n",
      "processed 4000 words\n",
      "processed 4500 words\n",
      "processed 5000 words\n",
      "processed 5500 words\n",
      "processed 6000 words\n",
      "processed 6500 words\n",
      "processed 7000 words\n",
      "processed 7500 words\n",
      "processed 8000 words\n",
      "processed 8500 words\n",
      "processed 9000 words\n",
      "processed 9500 words\n",
      "processed 10000 words\n",
      "processed 10500 words\n",
      "processed 11000 words\n",
      "processed 11500 words\n",
      "****************************************\n",
      "*** Evaluating ../trained_models/model.modabs.mc_rae_real.5k model ***\n",
      "****************************************\n",
      "processed 0 words\n",
      "processed 500 words\n",
      "processed 1000 words\n",
      "processed 1500 words\n",
      "processed 2000 words\n",
      "processed 2500 words\n",
      "processed 3000 words\n",
      "processed 3500 words\n",
      "processed 4000 words\n",
      "processed 4500 words\n",
      "processed 5000 words\n",
      "processed 5500 words\n",
      "processed 6000 words\n",
      "processed 6500 words\n",
      "processed 7000 words\n",
      "processed 7500 words\n",
      "processed 8000 words\n",
      "processed 8500 words\n",
      "processed 9000 words\n",
      "processed 9500 words\n",
      "processed 10000 words\n",
      "processed 10500 words\n",
      "processed 11000 words\n",
      "processed 11500 words\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# we need the default na option turned off bc the word/lemma 'null' was being interpreted as 'NaN'\n",
    "names = [\"lemma\", \"sense\", \"word_form\", \"context\"]\n",
    "df = pd.read_csv('../data/semcor_eval_data_11_27_2021.csv', names = names , keep_default_na=False)\n",
    "\n",
    "for save_path in models:\n",
    "    print(\"****************************************\")\n",
    "    print(\"*** Evaluating %s model ***\" % save_path)\n",
    "    print(\"****************************************\")\n",
    "    model = torch.load(save_path)\n",
    "    out_path = '../results/semcor_pairwise_data_' + os.path.split(save_path)[1] + '.csv'\n",
    "\n",
    "    # remove results file if exists\n",
    "    if os.path.exists(out_path):\n",
    "        os.remove(out_path)\n",
    "    get_pairwise_wu_palmer_data(model, df, bert, out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Conc.M</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>roadsweeper</th>\n",
       "      <td>4.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>traindriver</th>\n",
       "      <td>4.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tush</th>\n",
       "      <td>4.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hairdress</th>\n",
       "      <td>3.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pharmaceutics</th>\n",
       "      <td>3.77</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Conc.M\n",
       "Word                 \n",
       "roadsweeper      4.85\n",
       "traindriver      4.54\n",
       "tush             4.45\n",
       "hairdress        3.93\n",
       "pharmaceutics    3.77"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brysbaert_filename = \"/Users/gabriellachronis/data/Concreteness_ratings_Brysbaert_et_al_BRM.csv\"\n",
    "concreteness_df = pd.read_csv(brysbaert_filename, sep='\\t')\n",
    "concreteness_df= concreteness_df[[\"Word\", \"Conc.M\"]]\n",
    "concreteness_df = concreteness_df.set_index(\"Word\")\n",
    "concreteness_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(pd.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************\n",
      "*** Saturating pairwise data for model: ../trained_models/model.plsr.mc_rae_real.1k.50components.500max_iters ***\n",
      "****************************************\n",
      "1057680\n",
      "1045966\n",
      "   Unnamed: 0          lemma                              token_sense_1  \\\n",
      "0           0  communication  Lemma('communication.n.01.communication')   \n",
      "1           1  communication  Lemma('communication.n.01.communication')   \n",
      "2           2  communication  Lemma('communication.n.01.communication')   \n",
      "3           3  communication  Lemma('communication.n.01.communication')   \n",
      "4           4  communication  Lemma('communication.n.01.communication')   \n",
      "\n",
      "                               token_sense_2             cos_sim wup_sim  \\\n",
      "0  Lemma('communication.n.01.communication')  0.8791080401798218     1.0   \n",
      "1  Lemma('communication.n.01.communication')  0.6476124003167473     1.0   \n",
      "2  Lemma('communication.n.01.communication')  0.8188914603533104     1.0   \n",
      "3  Lemma('communication.n.01.communication')  0.8006323068449781     1.0   \n",
      "4  Lemma('communication.n.01.communication')  0.8673688077203457     1.0   \n",
      "\n",
      "  n_senses  \n",
      "0        3  \n",
      "1        3  \n",
      "2        3  \n",
      "3        3  \n",
      "4        3  \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'float' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9m/vzvx58rs51v_x5nm620fz4xr0000gn/T/ipykernel_60363/13356459.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m# add polysemy bin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     csv_input['wn_bin'] = pd.cut(csv_input.n_senses, \n\u001b[0m\u001b[1;32m     51\u001b[0m                         bins = [0, 2.1, 4.1, 6.1, 8.1, 10.1, 20.1, 50.1, 200], labels = False)\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/core/reshape/tile.py\u001b[0m in \u001b[0;36mcut\u001b[0;34m(x, bins, right, labels, retbins, precision, include_lowest, duplicates, ordered)\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bins must increase monotonically.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m     fac, bins = _bins_to_cuts(\n\u001b[0m\u001b[1;32m    288\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0mbins\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/core/reshape/tile.py\u001b[0m in \u001b[0;36m_bins_to_cuts\u001b[0;34m(x, bins, right, labels, precision, include_lowest, dtype, duplicates, ordered)\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[0mside\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"left\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mright\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"right\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m     \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_platform_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearchsorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mside\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mside\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minclude_lowest\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'float' and 'str'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Now we need to go in and add abstractness value and \n",
    "bin number of senses into polysemy band\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#cols = [\"lemma\", \"token_sense_1\", \"token_sense_2\", \"cos_sim\", \"wup_sim\", \"n_senses\"]\n",
    "\n",
    "\n",
    "for save_path in models:\n",
    "    print(\"****************************************\")\n",
    "    print(\"*** Saturating pairwise data for model: %s ***\" % save_path)\n",
    "    print(\"****************************************\")\n",
    "    infile = '../results/semcor_pairwise_data_' + os.path.split(save_path)[1] + '.csv'\n",
    "\n",
    "    outfile = '../results/saturated_semcor_pairwise_data_' + os.path.split(save_path)[1] + '.csv'\n",
    "\n",
    "    \n",
    "    dtype = {\n",
    "        'lemma':             str,\n",
    "        'token_sense_1':     object,\n",
    "        'token_sense_2':     object,\n",
    "        'cos_sim':           float,\n",
    "        'wup_sim':           float,\n",
    "        'n_senses':          float   \n",
    "    }\n",
    "    csv_input = pd.read_csv(infile, encoding_errors='ignore') #, names=cols) # requires pandas 1.3\n",
    "    #csv_input = pd.read_csv(infile)\n",
    "    \n",
    "    print(len(csv_input))\n",
    "    \n",
    "     # filtering out the rows with `POSITION_T` value in corresponding column\n",
    "    csv_input = csv_input[csv_input.token_sense_1.str.contains('token_sense_1') == False]\n",
    "    \n",
    "    print(len(csv_input))\n",
    "#     print(csv_input.dtypes)\n",
    "\n",
    "    \n",
    "    csv_input = csv_input.convert_dtypes(convert_floating=True)\n",
    "    \n",
    "#     print(csv_input.dtypes)\n",
    "    \n",
    "    print(csv_input.head())\n",
    "\n",
    "    \n",
    "    # add polysemy bin\n",
    "    csv_input['wn_bin'] = pd.cut(csv_input.n_senses, \n",
    "                        bins = [0, 2.1, 4.1, 6.1, 8.1, 10.1, 20.1, 50.1, 200], labels = False)\n",
    "\n",
    "\n",
    "    # add POS rows\n",
    "    pos1s = []\n",
    "    pos2s = []\n",
    "    sense1s = []\n",
    "    sense2s = []\n",
    "    for index, row in csv_input.iterrows():\n",
    "        pos1 = re.findall(r\"\\.(.*?)\\.\", row.token_sense_1)[0]\n",
    "        pos2 = re.findall(r\"\\.(.*?)\\.\", row.token_sense_2)[0]\n",
    "        pos1s.append(pos1)\n",
    "        pos2s.append(pos2)\n",
    "        sense1 = lemma_name_from_string(row.token_sense_1)\n",
    "        sense2 = lemma_name_from_string(row.token_sense_2)\n",
    "        sense1s.append(sense1)\n",
    "        sense2s.append(sense2)\n",
    "        \n",
    "    csv_input['sense1_pos'] = pos1s\n",
    "    csv_input['sense2_pos'] = pos2s\n",
    "    csv_input['token_sense_1'] = sense_1s\n",
    "    csv_input['token_sense_2'] = sense_2s\n",
    "    \n",
    "    \n",
    "    # add concreteness\n",
    "    csv_input = csv_input.join(concreteness_df, how = \"left\", on = \"lemma\")\n",
    "    \n",
    "    csv_input['conc_bin'] = pd.cut(csv_input['Conc.M'], \n",
    "                        bins = [0, 2.3, 4.5, 10], labels = False)\n",
    "    \n",
    "    # remove token sense columns\n",
    "#     csv_input.drop(['token_sense_1'], axis=1)\n",
    "#     csv_input.drop(['token_sense_2'], axis=1)\n",
    "    \n",
    "    #print(csv_input.where(csv_input['Conc.M'].notnull()))\n",
    "    print(csv_input.head(20))\n",
    "    \n",
    "    raise Exception(\"dewfieow\")\n",
    "    \n",
    "    csv_input.to_csv(outfile, index=False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.dtype' object has no attribute 'token_sense_1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-5eba3d0ab7f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# just a little test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlemma_from_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_sense_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.dtype' object has no attribute 'token_sense_1'"
     ]
    }
   ],
   "source": [
    "# just a little test\n",
    "lemma_from_string(csv_input.iloc[0].token_sense_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "and now we make another dataset for each one with the correlations.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def run_correlation(sense_similarities):\n",
    "    \"\"\"\n",
    "    :sense_similarities: dataframe with columns\n",
    "        cosine_sims \n",
    "        wup_sims\n",
    "    \"\"\"\n",
    "    #print(sense_similarities.head())\n",
    "    \n",
    "    if sense_similarities is None:\n",
    "        # not really sure why we're getting none values here it should be impossible\n",
    "        return (float(\"nan\"), float(\"nan\"), float(\"nan\"), float(\"nan\"))\n",
    "    elif len(sense_similarities['wup_sim'].unique()) == 1:\n",
    "        # the correlation will be garbage with a constant y value; skip to avoid warnings\n",
    "        return (float(\"nan\"), float(\"nan\"), float(\"nan\"), float(\"nan\"))\n",
    "\n",
    "    if len(sense_similarities) > 1 :\n",
    "        #print(word)\n",
    "\n",
    "        cos_sims = sense_similarities['cos_sim']\n",
    "        wup_sims = sense_similarities['wup_sim']\n",
    "\n",
    "        pearson, pearson_p = pearsonr(cos_sims, wup_sims )\n",
    "        #print('Pearsons correlation: %.3f, p-value: %s'  % (pearson, pearson_p))\n",
    "\n",
    "        spearman, spearman_p = spearmanr(cos_sims, wup_sims )\n",
    "        #print('Spearmans correlation: %.3f, p-value: %s'  % (spearman, spearman_p))\n",
    "\n",
    "        return (pearson, pearson_p, spearman, spearman_p)\n",
    "    return (float(\"nan\"), float(\"nan\"), float(\"nan\"), float(\"nan\"))\n",
    "\n",
    "def plot_sims():\n",
    "    cos_sims = sense_similarities['cos_sim']\n",
    "    wup_sims = sense_similarities['wup_sim']\n",
    "    plt.scatter(wup_sims, cos_sims)\n",
    "    plt.title(\"Wordnet similarity of homonymous senses plotted against cosine similarity of predicted vectors of two tokens in semantic feature space\")\n",
    "    plt.xlabel(\"Wu and Palmer Similarity\")\n",
    "    plt.ylabel(\"Cosine Similarity\")\n",
    "    plt.show()\n",
    "        \n",
    "# i think this is obsolete 11/16/21\n",
    "# df = pd.read_csv('semcor_wu_palmer_eval_data.csv', names = [\"word_form\", \"context\", \"wn_lemma\"])\n",
    "\n",
    "\n",
    "\n",
    "# for save_path in models:\n",
    "#     print(\"****************************************\")\n",
    "#     print(\"*** Evaluating %s model ***\" % save_path)\n",
    "#     print(\"****************************************\")\n",
    "#     model = torch.load(save_path)\n",
    "#     run_stats = run_wu_palmer_analysis(model, df, bert)\n",
    "#     #run_stats = run_correlation(similarities)\n",
    "#     out_path = 'results/semcor_analysis_' + os.path.split(save_path)[1] + '.csv'\n",
    "#     run_stats.to_csv(out_path)\n",
    "#     print(run_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************\n",
      "*** Running correlation on  trained_models/model.plsr.mc_rae_real.1k.50components.500max_iters model ***\n",
      "****************************************\n",
      "****************************************\n",
      "*** Running correlation on  trained_models/model.plsr.mc_rae_real.5k.100components.500max_iters model ***\n",
      "****************************************\n",
      "****************************************\n",
      "*** Running correlation on  trained_models/model.ffnn.mc_rae_real.1k.50epochs.0.5dropout.lr1e-4.hsize300 model ***\n",
      "****************************************\n",
      "****************************************\n",
      "*** Running correlation on  trained_models/model.ffnn.mc_rae_real.5k.50epochs.0.5dropout.lr1e-4.hsize300 model ***\n",
      "****************************************\n",
      "****************************************\n",
      "*** Running correlation on  trained_models/model.modabs.mc_rae_real.1k model ***\n",
      "****************************************\n",
      "****************************************\n",
      "*** Running correlation on  trained_models/model.modabs.mc_rae_real.5k model ***\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "for save_path in models:\n",
    "\n",
    "    print(\"****************************************\")\n",
    "    print(\"*** Running correlation on  %s model ***\" % save_path)\n",
    "    print(\"****************************************\")\n",
    "    infile = '../results/semcor_pairwise_data_' + os.path.split(save_path)[1] + '.csv'\n",
    "    outfile = '../results/semcor_analysis_' + os.path.split(save_path)[1] + '.csv'\n",
    "\n",
    "    out_data = []\n",
    "    \n",
    "    names = ['index', 'lemma', 'token_sense_1', 'token_sense_2',  'cos_sim',  'wup_sim',  'n_senses', 'wn_bin', 'sense1_pos', 'sense2_pos',  'Conc.M',  'conc_bin']\n",
    "    df = pd.read_csv(infile, names=names)\n",
    "    #print(df.head())\n",
    "\n",
    "    \n",
    "    lemmas = df.lemma.unique()\n",
    "    #print(lemmas[:10])\n",
    "\n",
    "    for word in lemmas:\n",
    "        word_data = df[df.lemma == word]\n",
    "        \n",
    "        n_senses = len(word_data.token_sense_1.unique())\n",
    "        \n",
    "        pearson, pearson_p, spearman, spearman_p = run_correlation(word_data)\n",
    "        row = (word, len(word_data), n_senses, pearson, pearson_p, spearman, spearman_p)\n",
    "        #print(corr)\n",
    "        out_data.append(row)\n",
    "        \n",
    "        #raise Exception(\"hfjesh\")\n",
    "    \n",
    "    out_df = pd.DataFrame.from_records(out_data, columns = ['word', 'n', 'n_senses', 'pearson', 'pearson_p', 'spearman', 'spearman_p'] )\n",
    "    out_df.to_csv(outfile)\n",
    "    \n",
    "    \n",
    "    #for word in \n",
    "    #run_correlation()\n",
    "    #cols = [\"lemma\", \"token_sense_1\", \"token_sense_2\", \"cos_sim\", \"wup_sim\", \"n_senses\"]\n",
    "    #df = csv_input = pd.read_csv(infile, names=cols)\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
