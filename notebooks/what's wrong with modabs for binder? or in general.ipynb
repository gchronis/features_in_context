{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a9e198b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from nltk.corpus import semcor\n",
    "#from nltk.tree import Tree\n",
    "#import itertools\n",
    "#import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "from bert import *\n",
    "from feature_data import *\n",
    "from multiprototype import *\n",
    "import classifier_main\n",
    "import csv\n",
    "from nltk.corpus.reader.wordnet import Lemma\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import inflect\n",
    "import os\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "import re\n",
    "from src.utils import *\n",
    "\n",
    "from scipy import spatial\n",
    "from sklearn.preprocessing import normalize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5459593",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.modeling:loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /Users/gabriellachronis/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "INFO:pytorch_pretrained_bert.modeling:extracting archive file /Users/gabriellachronis/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /var/folders/9m/vzvx58rs51v_x5nm620fz4xr0000gn/T/tmpkqg_v6e1\n",
      "INFO:pytorch_pretrained_bert.modeling:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/gabriellachronis/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "bert = BERTBase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d4a69b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = '../trained_models/model.modabs.binder.1k.mu1_1.mu2_0.1.mu3_0.001.mu4_5.nnk_4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7af72e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "617c26d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['lamb', 'bullet', 'crayon', 'nylons', 'blackbird']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36d819d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Drive', 'Arousal', 'Happy', 'Pleasant', 'Benefit', 'Communication', 'Human', 'UpperLimb', 'Attention', 'Vision']\n",
      "[0.25565978 0.04547585 0.02691238 0.04820498 0.07585004 0.07171238\n",
      " 0.09937557 0.08843739 0.16135066 0.06175159 0.05333497 0.16013099\n",
      " 0.1543207  0.17885093 0.13458334 0.05276306 0.10478829 0.12434189\n",
      " 0.02147458 0.09850257 0.04154495 0.0264197  0.02597206 0.07632457\n",
      " 0.02222012 0.14396524 0.04843606 0.04921625 0.15691896 0.22622066\n",
      " 0.05074005 0.04892288 0.07090239 0.12622128 0.16942215 0.08425869\n",
      " 0.05372874 0.03326537 0.06549076 0.09347617 0.07675372 0.1065434\n",
      " 0.15143853 0.16815375 0.19166436 0.19968061 0.1591993  0.16870069\n",
      " 0.24861656 0.06741136 0.22169416 0.04566761 0.19292754 0.06584526\n",
      " 0.0507757  0.0368286  0.04592472 0.09765196 0.18777226 0.16316648\n",
      " 0.19521484 0.1911226 ]\n",
      "['Drive', 'Arousal', 'Happy', 'Pleasant', 'Benefit', 'Communication', 'Human', 'UpperLimb', 'Attention', 'Vision']\n",
      "[0.25565978 0.04547585 0.02691238 0.04820498 0.07585004 0.07171238\n",
      " 0.09937557 0.08843739 0.16135066 0.06175159 0.05333497 0.16013099\n",
      " 0.1543207  0.17885093 0.13458334 0.05276306 0.10478829 0.12434189\n",
      " 0.02147458 0.09850257 0.04154495 0.0264197  0.02597206 0.07632457\n",
      " 0.02222012 0.14396524 0.04843606 0.04921625 0.15691896 0.22622066\n",
      " 0.05074005 0.04892288 0.07090239 0.12622128 0.16942215 0.08425869\n",
      " 0.05372874 0.03326537 0.06549076 0.09347617 0.07675372 0.1065434\n",
      " 0.15143853 0.16815375 0.19166436 0.19968061 0.1591993  0.16870069\n",
      " 0.24861656 0.06741136 0.22169416 0.04566761 0.19292754 0.06584526\n",
      " 0.0507757  0.0368286  0.04592472 0.09765196 0.18777226 0.16316648\n",
      " 0.19521484 0.1911226 ]\n",
      "['Drive', 'Arousal', 'Happy', 'Pleasant', 'Benefit', 'Communication', 'Human', 'UpperLimb', 'Attention', 'Vision']\n",
      "[0.25565978 0.04547585 0.02691238 0.04820498 0.07585004 0.07171238\n",
      " 0.09937557 0.08843739 0.16135066 0.06175159 0.05333497 0.16013099\n",
      " 0.1543207  0.17885093 0.13458334 0.05276306 0.10478829 0.12434189\n",
      " 0.02147458 0.09850257 0.04154495 0.0264197  0.02597206 0.07632457\n",
      " 0.02222012 0.14396524 0.04843606 0.04921625 0.15691896 0.22622066\n",
      " 0.05074005 0.04892288 0.07090239 0.12622128 0.16942215 0.08425869\n",
      " 0.05372874 0.03326537 0.06549076 0.09347617 0.07675372 0.1065434\n",
      " 0.15143853 0.16815375 0.19166436 0.19968061 0.1591993  0.16870069\n",
      " 0.24861656 0.06741136 0.22169416 0.04566761 0.19292754 0.06584526\n",
      " 0.0507757  0.0368286  0.04592472 0.09765196 0.18777226 0.16316648\n",
      " 0.19521484 0.1911226 ]\n",
      "['Drive', 'Arousal', 'Happy', 'Pleasant', 'Benefit', 'Communication', 'Human', 'UpperLimb', 'Attention', 'Vision']\n",
      "[0.25565978 0.04547585 0.02691238 0.04820498 0.07585004 0.07171238\n",
      " 0.09937557 0.08843739 0.16135066 0.06175159 0.05333497 0.16013099\n",
      " 0.1543207  0.17885093 0.13458334 0.05276306 0.10478829 0.12434189\n",
      " 0.02147458 0.09850257 0.04154495 0.0264197  0.02597206 0.07632457\n",
      " 0.02222012 0.14396524 0.04843606 0.04921625 0.15691896 0.22622066\n",
      " 0.05074005 0.04892288 0.07090239 0.12622128 0.16942215 0.08425869\n",
      " 0.05372874 0.03326537 0.06549076 0.09347617 0.07675372 0.1065434\n",
      " 0.15143853 0.16815375 0.19166436 0.19968061 0.1591993  0.16870069\n",
      " 0.24861656 0.06741136 0.22169416 0.04566761 0.19292754 0.06584526\n",
      " 0.0507757  0.0368286  0.04592472 0.09765196 0.18777226 0.16316648\n",
      " 0.19521484 0.1911226 ]\n",
      "['Drive', 'Arousal', 'Happy', 'Pleasant', 'Benefit', 'Communication', 'Human', 'UpperLimb', 'Attention', 'Vision']\n",
      "[0.25565978 0.04547585 0.02691238 0.04820498 0.07585004 0.07171238\n",
      " 0.09937557 0.08843739 0.16135066 0.06175159 0.05333497 0.16013099\n",
      " 0.1543207  0.17885093 0.13458334 0.05276306 0.10478829 0.12434189\n",
      " 0.02147458 0.09850257 0.04154495 0.0264197  0.02597206 0.07632457\n",
      " 0.02222012 0.14396524 0.04843606 0.04921625 0.15691896 0.22622066\n",
      " 0.05074005 0.04892288 0.07090239 0.12622128 0.16942215 0.08425869\n",
      " 0.05372874 0.03326537 0.06549076 0.09347617 0.07675372 0.1065434\n",
      " 0.15143853 0.16815375 0.19166436 0.19968061 0.1591993  0.16870069\n",
      " 0.24861656 0.06741136 0.22169416 0.04566761 0.19292754 0.06584526\n",
      " 0.0507757  0.0368286  0.04592472 0.09765196 0.18777226 0.16316648\n",
      " 0.19521484 0.1911226 ]\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "        feats = model.predict_top_n_features(word, 10)\n",
    "        logits = model.predict(word)\n",
    "        print(feats)\n",
    "        print(logits)\n",
    "        #for feat in feats:\n",
    "        #    print(\"     \", feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c113ff10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fearful',\n",
       " 'Short',\n",
       " 'Shape',\n",
       " 'Motion',\n",
       " 'Arousal',\n",
       " 'Harm',\n",
       " 'Bright',\n",
       " 'Fast',\n",
       " 'Attention',\n",
       " 'Vision']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_top_n_features('lightning', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "048e858e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bagpipe_0', 'restaurant_0', 'driver_0', 'guilt_0', 'zoo_0', 'empty_0', 'bar_0', 'bay_0', 'television_0', 'clever_0', 'grievance_0', 'actor_0', 'priest_0', 'desk_0', 'love_0', 'drum_0', 'arm_0', 'matinee_0', 'protest_0', 'ran_0', 'riot_0', 'jealousy_0', 'chime_0', 'sin_0', 'bell_0', 'small_0', 'folly_0', 'jungle_0', 'night_0', 'snub_0', 'winter_0', 'deceit_0', 'island_0', 'computer_0', 'verb_0', 'stole_0', 'foot_0', 'soccer_0', 'satire_0', 'zone_0', 'dropped_0', 'crow_0', 'mouse_0', 'nose_0', 'camera_0', 'limousine_0', 'spaghetti_0', 'green_0', 'worth_0', 'company_0', 'awe_0', 'black_0', 'landslide_0', 'legality_0', 'dandelion_0', 'belch_0', 'cherry_0', 'saxophone_0', 'symphony_0', 'voter_0', 'key_0', 'xylophone_0', 'destroyed_0', 'rake_0', 'landed_0', 'dictation_0', 'election_0', 'cafeteria_0', 'dime_0', 'evening_0', 'rum_0', 'truce_0', 'denial_0', 'cranberry_0', 'hawk_0', 'used_0', 'train_0', 'flew_0', 'envy_0', 'ticket_0', 'banjo_0', 'council_0', 'broccoli_0', 'table_0', 'sum_0', 'hoe_0', 'pie_0', 'sun_0', 'whole_0', 'famous_0', 'excuse_0', 'bicycle_0', 'mob_0', 'injured_0', 'butterfly_0', 'parent_0', 'fiddle_0', 'cabbage_0', 'coffee_0', 'yellow_0', 'oration_0', 'subway_0', 'hope_0', 'stone_0', 'applause_0', 'celebrated_0', 'football_0', 'number_0', 'street_0', 'bonfire_0', 'salmon_0', 'semester_0', 'piano_0', 'faucet_0', 'trust_0', 'new_0', 'penguin_0', 'heroism_0', 'umbrella_0', 'funeral_0', 'plane_0', 'leg_0', 'oak_0', 'noun_0', 'stayed_0', 'school_0', 'lived_0', 'opened_0', 'dusty_0', 'tea_0', 'man_0', 'pineapple_0', 'cloud_0', 'spiritual_0', 'lab_0', 'finger_0', 'garden_0', 'handshake_0', 'comb_0', 'advice_0', 'malice_0', 'harp_0', 'moose_0', 'broke_0', 'mosquito_0', 'river_0', 'liked_0', 'megaphone_0', 'tomato_0', 'torment_0', 'lake_0', 'expensive_0', 'summer_0', 'carnival_0', 'hurricane_0', 'peaceful_0', 'saw_0', 'built_0', 'gave_0', 'tobacco_0', 'gunshot_0', 'worker_0', 'sandpaper_0', 'asparagus_0', 'white_0', 'mushroom_0', 'office_0', 'animosity_0', 'duck_0', 'family_0', 'gong_0', 'arrested_0', 'dangerous_0', 'beach_0', 'fixed_0', 'hailstorm_0', 'victim_0', 'old_0', 'joviality_0', 'bought_0', 'artist_0', 'big_0', 'tuba_0', 'eye_0', 'luck_0', 'bread_0', 'angry_0', 'fallacy_0', 'gratitude_0', 'kiss_0', 'tribute_0', 'bribe_0', 'vice_0', 'cash_0', 'stapler_0', 'apricot_0', 'church_0', 'hair_0', 'spatula_0', 'ivy_0', 'approached_0', 'soft_0', 'pumpkin_0', 'battle_0', 'bugle_0', 'ham_0', 'editor_0', 'met_0', 'couple_0', 'truck_0', 'cab_0', 'toe_0', 'hand_0', 'plum_0', 'cheese_0', 'listened (to)_0', 'flute_0', 'attribute_0', 'magazine_0', 'field_0', 'chocolate_0', 'ended_0', 'snake_0', 'squeal_0', 'rally_0', 'cabinet_0', 'hamster_0', 'mayor_0', 'bird_0', 'paradox_0', 'ball_0', 'ate_0', 'found_0', 'politician_0', 'heredity_0', 'walked_0', 'debate_0', 'loan_0', 'pea_0', 'moral_0', 'author_0', 'survived_0', 'rose_0', 'highway_0', 'explosion_0', 'speech_0', 'plot_0', 'threw_0', 'whale_0', 'reality_0', 'fate_0', 'festival_0', 'drank_0', 'book_0', 'shouted_0', 'dolphin_0', 'boy_0', 'embassy_0', 'spoke (to)_0', 'year_0', 'theater_0', 'teacher_0', 'boat_0', 'avalanche_0', 'infinity_0', 'fun_0', 'clue_0', 'tornado_0', 'blocked_0', 'blueberry_0', 'storm_0', 'honeymoon_0', 'vacation_0', 'played_0', 'quantity_0', 'ice_0', 'volcano_0', 'pilot_0', 'beer_0', 'pan_0', 'morning_0', 'apology_0', 'submarine_0', 'journalist_0', 'elephant_0', 'tulip_0', 'grief_0', 'tired_0', 'accident_0', 'college_0', 'lost_0', 'water_0', 'helped_0', 'dog_0', 'aggressive_0', 'patent_0', 'pen_0', 'ricochet_0', 'optimism_0', 'whine_0', 'choir_0', 'crib_0', 'era_0', 'trial_0', 'cucumber_0', 'newspaper_0', 'motive_0', 'policeman_0', 'insult_0', 'egg_0', 'home_0', 'window_0', 'bus_0', 'tiger_0', 'majority_0', 'team_0', 'tangerine_0', 'spring_0', 'kitchen_0', 'radish_0', 'patient_0', 'peace_0', 'bed_0', 'red_0', 'pig_0', 'laughed_0', 'sled_0', 'took_0', 'park_0', 'rocket_0', 'meeting_0', 'girl_0', 'clang_0', 'glass_0', 'hotel_0', 'put_0', 'intellect_0', 'chair_0', 'harmonica_0', 'barbecue_0', 'child_0', 'van_0', 'business_0', 'banker_0', 'hall_0', 'gum_0', 'cheetah_0', 'tourist_0', 'flower_0', 'corkscrew_0', 'escalator_0', 'plea_0', 'horse_0', 'long_0', 'embrace_0', 'thunder_0', 'dread_0', 'baseball_0', 'student_0', 'joke_0', 'party_0', 'doctor_0', 'store_0', 'minister_0', 'forest_0', 'airport_0', 'lightning_0', 'lemonade_0', 'apartment_0', 'bridge_0', 'fish_0', 'flood_0', 'camel_0', 'ketchup_0', 'grew_0', 'sailboat_0', 'diplomat_0', 'chestnut_0', 'went_0', 'drew_0', 'theme_0', 'cold_0', 'guard_0', 'woman_0', 'axe_0', 'fireworks_0', 'farmer_0', 'wealthy_0', 'hiked_0', 'chicken_0', 'car_0', 'elevator_0', 'lust_0', 'negotiated_0', 'downpour_0', 'interviewed_0', 'scientist_0', 'wanted_0', 'blue_0', 'perjury_0', 'activist_0', 'dead_0', 'woe_0', 'scream_0', 'damaged_0', 'planned_0', 'role_0', 'law_0', 'soldier_0', 'advantage_0', 'judge_0', 'problem_0', 'jam_0', 'lip_0', 'monkey_0', 'corn_0', 'elm_0', 'mountain_0', 'young_0', 'powerful_0', 'truth_0', 'wit_0', 'theory_0', 'criminal_0', 'toaster_0', 'shelves_0', 'carriage_0', 'turtle_0', 'sympathy_0', 'alligator_0', 'door_0', 'curse_0', 'witness_0', 'cyclone_0', 'day_0', 'cellphone_0', 'scissors_0', 'shoulder_0', 'joy_0', 'ambulance_0', 'left_0', 'power_0', 'fountain_0', 'lonely_0', 'parade_0', 'dinner_0', 'screech_0', 'chipmunk_0', 'hairbrush_0', 'clarinet_0', 'kicked_0', 'belief_0', 'happy_0', 'hot_0', 'audience_0', 'court_0', 'hospital_0', 'wonder_0', 'fed_0', 'stampede_0', 'trumpet_0', 'lawyer_0', 'jaw_0', 'shame_0', 'mustard_0', 'marched_0', 'accordion_0', 'cough_0', 'raspberry_0', 'friendly_0', 'slept_0', 'prairie_0', 'shiny_0', 'pencil_0', 'commander_0', 'agreement_0', 'heavy_0', 'mystery_0', 'analogy_0', 'mandolin_0', 'fence_0', 'delivered_0', 'tax_0', 'carrot_0', 'army_0', 'delirium_0', 'trombone_0', 'banana_0', 'eggplant_0', 'medicine_0', 'knowledge_0', 'mercy_0', 'goldfish_0', 'prison_0', 'rumor_0', 'muscle_0', 'tree_0', 'ant_0', 'engineer_0', 'sick_0', 'musical_0', 'scooter_0', 'hierarchy_0', 'watched_0', 'feather_0', 'feared_0', 'worked_0', 'farm_0', 'treaty_0', 'held_0', 'carried_0', 'honey_0', 'testimony_0', 'cathedral_0', 'circus_0', 'reporter_0', 'terrorist_0', 'wrote_0', 'bee_0', 'etiquette_0', 'dark_0', 'visited_0', 'jury_0', 'ire_0', 'fee_0', 'irony_0', 'businessman_0', 'keyboard_0', 'gasp_0', 'crossed_0', 'hygiene_0', 'loud_0', 'mouth_0', 'read_0']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.word_indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd4efd95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "488\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.47290126, 0.22699028, 0.08035515, 0.41039638, 0.29407955,\n",
       "       0.059381  , 0.36117564, 0.06790745, 0.06886969, 0.03547469,\n",
       "       0.05774522, 0.44821897, 0.0128413 , 0.024642  , 0.31319399,\n",
       "       0.19000451, 0.32533432, 0.40289222, 0.01401412, 0.03693731,\n",
       "       0.02778014, 0.0159645 , 0.0225674 , 0.05831063, 0.01016182,\n",
       "       0.00728875, 0.32683974, 0.28750357, 0.22595652, 0.21669962,\n",
       "       0.01443539, 0.04480074, 0.03379261, 0.2009119 , 0.191606  ,\n",
       "       0.14461531, 0.04641018, 0.06446503, 0.04355739, 0.05133219,\n",
       "       0.02448051, 0.0757761 , 0.0592469 , 0.0443634 , 0.00999563,\n",
       "       0.0134318 , 0.02374603, 0.0117896 , 0.21030497, 0.04866745,\n",
       "       0.28468243, 0.03837175, 0.20287386, 0.00879221, 0.0063505 ,\n",
       "       0.02623113, 0.01063226, 0.02403504, 0.06332732, 0.10110849,\n",
       "       0.12150125, 0.07932962])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here is a copu of the predict function from modabs.py, for debugging\n",
    "\n",
    "def predicament(model, word: str):\n",
    "        labels = [word + '_' + str(i) for i in range(0, model.num_prototypes)]\n",
    "        #print(labels)\n",
    "        vectors = np.empty([model.num_prototypes, model.output_dims])\n",
    "        #print(vectors.shape)\n",
    "        for i in range(0,len(labels)):\n",
    "            label = labels[i]\n",
    "            #print(label)\n",
    "            #print(self.word_indexer.objs_to_ints)\n",
    "            word_index = model.word_indexer.index_of(label)\n",
    "            print(word_index)\n",
    "            #print(word_index)\n",
    "            vec = model.predictions[word_index]\n",
    "            #print(vec.shape)\n",
    "\n",
    "            vectors[i,:] = vec\n",
    "        #vectors = self.predictions[word_index:(word_index + self.num_prototypes), :]\n",
    "        #print(vectors.shape)\n",
    "        avg = np.average(vectors, axis=0)\n",
    "        return avg\n",
    "    \n",
    "predicament(model, 'eggplant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1105692e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.25565978, 0.04547585, 0.02691238, 0.04820498, 0.07585004,\n",
       "       0.07171238, 0.09937557, 0.08843739, 0.16135066, 0.06175159,\n",
       "       0.05333497, 0.16013099, 0.1543207 , 0.17885093, 0.13458334,\n",
       "       0.05276306, 0.10478829, 0.12434189, 0.02147458, 0.09850257,\n",
       "       0.04154495, 0.0264197 , 0.02597206, 0.07632457, 0.02222012,\n",
       "       0.14396524, 0.04843606, 0.04921625, 0.15691896, 0.22622066,\n",
       "       0.05074005, 0.04892288, 0.07090239, 0.12622128, 0.16942215,\n",
       "       0.08425869, 0.05372874, 0.03326537, 0.06549076, 0.09347617,\n",
       "       0.07675372, 0.1065434 , 0.15143853, 0.16815375, 0.19166436,\n",
       "       0.19968061, 0.1591993 , 0.16870069, 0.24861656, 0.06741136,\n",
       "       0.22169416, 0.04566761, 0.19292754, 0.06584526, 0.0507757 ,\n",
       "       0.0368286 , 0.04592472, 0.09765196, 0.18777226, 0.16316648,\n",
       "       0.19521484, 0.1911226 ])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the issue, that words we haven't put in our model from the beginnning don't have an index and therefore \n",
    "# when we look them up the index is negative 1 which duhhhhh - we cannot predict them. \n",
    "\n",
    "# May 19, we have to start over with all of our modebs/ label propagation results\n",
    "\n",
    "\n",
    "predicament(model, 'blackbird')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6af12599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eggplant_0\n",
      "[ 0.23117015 -0.19057544  0.18597732  0.69712703  1.46094282  0.20722125\n",
      "  0.2089079   0.85664674 -0.35170985 -0.03976732 -0.24099093 -0.4730898\n",
      " -0.1233874  -0.20889232 -0.43851542  1.00292381 -0.06688743  0.46208362\n",
      "  0.44808395 -0.04423582  0.5396985  -0.23075066 -0.85934351  0.69117325\n",
      "  0.61964989]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.47290126, 0.22699028, 0.08035515, 0.41039638, 0.29407955,\n",
       "       0.059381  , 0.36117564, 0.06790745, 0.06886969, 0.03547469,\n",
       "       0.05774522, 0.44821897, 0.0128413 , 0.024642  , 0.31319399,\n",
       "       0.19000451, 0.32533432, 0.40289222, 0.01401412, 0.03693731,\n",
       "       0.02778014, 0.0159645 , 0.0225674 , 0.05831063, 0.01016182,\n",
       "       0.00728875, 0.32683974, 0.28750357, 0.22595652, 0.21669962,\n",
       "       0.01443539, 0.04480074, 0.03379261, 0.2009119 , 0.191606  ,\n",
       "       0.14461531, 0.04641018, 0.06446503, 0.04355739, 0.05133219,\n",
       "       0.02448051, 0.0757761 , 0.0592469 , 0.0443634 , 0.00999563,\n",
       "       0.0134318 , 0.02374603, 0.0117896 , 0.21030497, 0.04866745,\n",
       "       0.28468243, 0.03837175, 0.20287386, 0.00879221, 0.0063505 ,\n",
       "       0.02623113, 0.01063226, 0.02403504, 0.06332732, 0.10110849,\n",
       "       0.12150125, 0.07932962])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's see... is this also a problem for predict in context?\n",
    "\n",
    "def predicament_in_context(model, word, sentence, bert, glove=False):\n",
    "    if glove:\n",
    "        return self.predict(word)\n",
    "    # generate bert vector for word\n",
    "    vec = bert.get_bert_vectors_for(word, sentence)\n",
    "    # get the layer we care about\n",
    "    vec = vec[8]\n",
    "\n",
    "    # reshape to be vertical\n",
    "    vec = vec.reshape(1, -1)\n",
    "    #print(\"after reshape\")\n",
    "    #print(vec.shape)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    find the nearest neighbor to the input vector\n",
    "    \"\"\"\n",
    "    # TODO to predict the nearest neighbor by word we have to fix the counter to an indexer and retrain all the saved models =/\n",
    "\n",
    "    # find the closest input embedding to our context vector\n",
    "    label, vec = model.multipro_embeddings.find_nearest_neighbor(vec)\n",
    "\n",
    "    print(label)\n",
    "    print(vec[:25])\n",
    "    \n",
    "    # use the label to find the projection we have stored for that embedding\n",
    "    index = model.word_indexer.index_of(label)\n",
    "\n",
    "    logits = model.predictions[index]\n",
    "\n",
    "    return logits\n",
    "\n",
    "predicament_in_context(model, 'eggplant', 'The room was the color of eggplant.', bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "06c6c690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blackbird_0\n",
      "[ 0.19835691 -0.26989769  0.20713212  0.07631599  1.18570936  0.70383808\n",
      "  0.12653709  0.78838764 -0.79630775 -0.01981011 -0.0343557  -0.3067641\n",
      "  0.1258918   0.01801877 -0.74435992  0.56796731  0.05499426  0.58305214\n",
      " -0.31193427  0.11832664  0.46985487 -0.21807169 -0.52067759  0.42566902\n",
      "  0.42490983]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.25565978, 0.04547585, 0.02691238, 0.04820498, 0.07585004,\n",
       "       0.07171238, 0.09937557, 0.08843739, 0.16135066, 0.06175159,\n",
       "       0.05333497, 0.16013099, 0.1543207 , 0.17885093, 0.13458334,\n",
       "       0.05276306, 0.10478829, 0.12434189, 0.02147458, 0.09850257,\n",
       "       0.04154495, 0.0264197 , 0.02597206, 0.07632457, 0.02222012,\n",
       "       0.14396524, 0.04843606, 0.04921625, 0.15691896, 0.22622066,\n",
       "       0.05074005, 0.04892288, 0.07090239, 0.12622128, 0.16942215,\n",
       "       0.08425869, 0.05372874, 0.03326537, 0.06549076, 0.09347617,\n",
       "       0.07675372, 0.1065434 , 0.15143853, 0.16815375, 0.19166436,\n",
       "       0.19968061, 0.1591993 , 0.16870069, 0.24861656, 0.06741136,\n",
       "       0.22169416, 0.04566761, 0.19292754, 0.06584526, 0.0507757 ,\n",
       "       0.0368286 , 0.04592472, 0.09765196, 0.18777226, 0.16316648,\n",
       "       0.19521484, 0.1911226 ])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicament_in_context(model, 'blackbird', 'The room was the color of a blackbird.', bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2253bb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's see if we can fic both of these functions...so we can update the model code\n",
    "# leaving broken code above and copying to edit below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0a6578dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.99441347  0.66744632  0.14816417 ... -0.09310469  0.01299667\n",
      "  -0.72792843]\n",
      " [ 1.01469038 -0.54340926  0.10346673 ... -0.24732851 -0.39735755\n",
      "  -0.85013201]\n",
      " [ 0.37705096  0.25009722 -0.16348471 ...  0.1304165   0.66947503\n",
      "  -0.7857151 ]\n",
      " [ 0.87506026 -0.28650017  0.1026062  ...  0.21925569 -0.02151393\n",
      "  -0.57753804]\n",
      " [ 0.19835691 -0.26989769  0.20713212 ... -0.24054401 -0.02871272\n",
      "  -0.94904848]]\n",
      "[[ 3.51490117e-03  2.35918752e-03  5.23708139e-04 ... -3.29092264e-04\n",
      "   4.59386348e-05 -2.57297047e-03]\n",
      " [ 3.41010250e-03 -1.82625294e-03  3.47723944e-04 ... -8.31204872e-04\n",
      "  -1.33541225e-03 -2.85706592e-03]\n",
      " [ 1.34441681e-03  8.91749260e-04 -5.82922789e-04 ...  4.65014415e-04\n",
      "   2.38708712e-03 -2.80155393e-03]\n",
      " [ 3.28645247e-03 -1.07600496e-03  3.85356784e-04 ...  8.23455751e-04\n",
      "  -8.07996018e-05 -2.16905211e-03]\n",
      " [ 7.94906590e-04 -1.08160313e-03  8.30072850e-04 ... -9.63969563e-04\n",
      "  -1.15064948e-04 -3.80327010e-03]]\n"
     ]
    }
   ],
   "source": [
    "# we're going to need to put a kdtree in the model to easily look up cosine; lets make a toy one\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "The ranking your would get with cosine similarity is equivalent\n",
    "to the rank order of the euclidean distance when you normalize all \n",
    "the data points first. So you can use a KD tree to the the k nearest neighbors with KDTrees,\n",
    "but you will need to recompute what the cosine similarity is.\n",
    "\"\"\"\n",
    "\n",
    "train_exs = ['lamb', 'bullet', 'crayon', 'nylons', 'blackbird']\n",
    "\n",
    "word_indexer = Indexer()\n",
    "num_samples = len(train_exs) * model.multipro_embeddings.num_prototypes\n",
    "X = np.empty([num_samples, model.multipro_embeddings.dim])\n",
    "i=0\n",
    "\n",
    "for word in train_exs:\n",
    "\n",
    "    # look up the embedding\n",
    "    emb = model.multipro_embeddings.get_embedding(word)\n",
    "\n",
    "    # iterate through the prototypes for each word\n",
    "    #print(emb.shape)        \n",
    "    for index in range(0, emb.shape[0]):\n",
    "        # this adds something to the index like 'apple_3' so we know what the prediction is for each thing\n",
    "        word_indexer.add_and_get_index(word + '_' + str(index))\n",
    "\n",
    "\n",
    "        # add vector to training xs\n",
    "        vector = emb[index, :]\n",
    "        #print(vector.shape)\n",
    "        X[i] = vector\n",
    "        i+=1\n",
    "\n",
    "        \n",
    "print(X)\n",
    "X_normed = normalize(X, axis=1, norm='l1')\n",
    "print(X_normed)\n",
    "\n",
    "model.kd_tree = spatial.KDTree(X)\n",
    "        #     print(\"done building kdtree for nneighbor queries for modabs model\")\n",
    "        #     d, i = self.kd_tree.query(vector, 1)\n",
    "        # vector = self.kd_tree.data[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2395de1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.kd_tree.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "17be40d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def _find_nearest_neighbor(model, vector):\n",
    "    vector = np.array(vector)\n",
    "    vector = np.expand_dims(vector, axis=1)\n",
    "    print(\"calculating similarities with neighbors\")\n",
    "    print(vector[:10])\n",
    "    vector = normalize(vector, axis=0, norm='l1')\n",
    "    print(vector[:10])\n",
    "    vector = vector.reshape(1, -1)\n",
    "\n",
    "    distance, index = model.kd_tree.query(vector, 1)\n",
    "    \n",
    "    print(index)\n",
    "    neighbor = model.word_indexer.get_object(index[0])\n",
    "    \n",
    "    neighbor \n",
    "    print(neighbor)\n",
    "\n",
    "    return neighbor\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predicament(model, word: str):\n",
    "        labels = [word + '_' + str(i) for i in range(0, model.num_prototypes)]\n",
    "        #print(labels)\n",
    "        vectors = np.empty([model.num_prototypes, model.output_dims])\n",
    "        #print(vectors.shape)\n",
    "        \n",
    "        \"\"\"\n",
    "        get prediction for each prototype; or,\n",
    "        get nearest neighbor of each prototype and get prediction for that\n",
    "        \"\"\"\n",
    "        for i in range(0,len(labels)):\n",
    "            label = labels[i]\n",
    "            #print(label)\n",
    "            #print(self.word_indexer.objs_to_ints)\n",
    "            word_index = model.word_indexer.index_of(label)\n",
    "            print(word_index)\n",
    "            \n",
    "            \"\"\"\n",
    "            if this word is not in the graph, we need to find the nearest vector\n",
    "            in the graph and use the predictions we have stored for that instead. \n",
    "            really, it seems we need more words in the graph for label prop to do its work the best\n",
    "            \"\"\"\n",
    "            if word_index == -1:\n",
    "                # get multipro vector\n",
    "                mpro_vec = model.multipro_embeddings.get_embedding(word)\n",
    "                # get vector at current index\n",
    "                mpro_vec = mpro_vec[i]\n",
    "                \n",
    "                \"\"\"\n",
    "                then, we need to calculate the similarity between this and all of the mpro embeddings in our model, on the fly\n",
    "                would help if we already had them stored in the model\n",
    "                \"\"\"\n",
    "                neighbor = _find_nearest_neighbor(model, mpro_vec)\n",
    "                word_index = model.word_indexer.index_of(neighbor)\n",
    "\n",
    "                vec = model.predictions[word_index]\n",
    "            \n",
    "            # otherwise we have the word in our graph already\n",
    "            else:\n",
    "                vec = model.predictions[word_index]\n",
    "            #print(vec.shape)\n",
    "\n",
    "            vectors[i,:] = vec\n",
    "        #vectors = self.predictions[word_index:(word_index + self.num_prototypes), :]\n",
    "        #print(vectors.shape)\n",
    "        \n",
    "        # then average the predictions\n",
    "        avg = np.average(vectors, axis=0)\n",
    "        return avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "82115c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "calculating similarities with neighbors\n",
      "[[ 0.19835691]\n",
      " [-0.26989769]\n",
      " [ 0.20713212]\n",
      " [ 0.07631599]\n",
      " [ 1.18570936]\n",
      " [ 0.70383808]\n",
      " [ 0.12653709]\n",
      " [ 0.78838764]\n",
      " [-0.79630775]\n",
      " [-0.01981011]]\n",
      "[[ 7.94906590e-04]\n",
      " [-1.08160313e-03]\n",
      " [ 8.30072850e-04]\n",
      " [ 3.05832980e-04]\n",
      " [ 4.75167816e-03]\n",
      " [ 2.82060017e-03]\n",
      " [ 5.07091812e-04]\n",
      " [ 3.15942884e-03]\n",
      " [-3.19116830e-03]\n",
      " [-7.93881502e-05]]\n",
      "[4]\n",
      "zoo_0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.99207544, 0.47264081, 0.27811963, 0.31520452, 0.38215232,\n",
       "       1.98124412, 0.04186151, 1.24152188, 0.92320346, 0.44768503,\n",
       "       0.3094939 , 0.81204462, 0.19309208, 0.34722986, 0.33279608,\n",
       "       0.1875583 , 0.40608759, 0.73290347, 0.15462763, 1.21529142,\n",
       "       0.98138945, 0.36767651, 0.45974189, 1.51569218, 0.20244071,\n",
       "       0.23843347, 0.0790597 , 1.41820999, 0.24079055, 0.44271498,\n",
       "       0.78759004, 2.06506282, 0.30945685, 1.92956247, 0.52015694,\n",
       "       0.22882335, 0.15846404, 0.24849243, 0.62172014, 0.55269538,\n",
       "       0.82020296, 0.25220971, 0.35855163, 1.29555002, 0.21475112,\n",
       "       0.34847984, 0.05211903, 0.26044125, 0.94657834, 0.39099905,\n",
       "       1.71113926, 0.18111072, 1.68201104, 0.20655957, 0.12476559,\n",
       "       0.20434929, 0.36665846, 0.70354248, 0.51334339, 0.26368636,\n",
       "       1.58520261, 1.344804  ])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicament(model, 'blackbird')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f3722c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating similarities with neighbors\n",
      "[[ 0.0991118 ]\n",
      " [-0.39250192]\n",
      " [ 0.16791524]\n",
      " [-0.0382616 ]\n",
      " [ 1.4904007 ]\n",
      " [ 0.34370324]\n",
      " [-0.12062914]\n",
      " [ 1.1955184 ]\n",
      " [-0.28820708]\n",
      " [-0.34894058]]\n",
      "[[ 0.00029143]\n",
      " [-0.00115414]\n",
      " [ 0.00049375]\n",
      " [-0.00011251]\n",
      " [ 0.00438247]\n",
      " [ 0.00101065]\n",
      " [-0.00035471]\n",
      " [ 0.00351538]\n",
      " [-0.00084746]\n",
      " [-0.00102605]]\n",
      "[4]\n",
      "zoo_0\n",
      "zoo_0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.99207544, 0.47264081, 0.27811963, 0.31520452, 0.38215232,\n",
       "       1.98124412, 0.04186151, 1.24152188, 0.92320346, 0.44768503,\n",
       "       0.3094939 , 0.81204462, 0.19309208, 0.34722986, 0.33279608,\n",
       "       0.1875583 , 0.40608759, 0.73290347, 0.15462763, 1.21529142,\n",
       "       0.98138945, 0.36767651, 0.45974189, 1.51569218, 0.20244071,\n",
       "       0.23843347, 0.0790597 , 1.41820999, 0.24079055, 0.44271498,\n",
       "       0.78759004, 2.06506282, 0.30945685, 1.92956247, 0.52015694,\n",
       "       0.22882335, 0.15846404, 0.24849243, 0.62172014, 0.55269538,\n",
       "       0.82020296, 0.25220971, 0.35855163, 1.29555002, 0.21475112,\n",
       "       0.34847984, 0.05211903, 0.26044125, 0.94657834, 0.39099905,\n",
       "       1.71113926, 0.18111072, 1.68201104, 0.20655957, 0.12476559,\n",
       "       0.20434929, 0.36665846, 0.70354248, 0.51334339, 0.26368636,\n",
       "       1.58520261, 1.344804  ])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def predicament_in_context(model, word, sentence, bert, glove=False):\n",
    "    if glove:\n",
    "        return self.predict(word)\n",
    "    # generate bert vector for word\n",
    "    vec = bert.get_bert_vectors_for(word, sentence)\n",
    "    # get the layer we care about\n",
    "    vec = vec[8]\n",
    "\n",
    "    # reshape to be vertical\n",
    "    #print(\"after reshape\")\n",
    "    #print(vec.shape)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    find the nearest neighbor to the input vector\n",
    "    \"\"\"\n",
    "    # TODO to predict the nearest neighbor by word we have to fix the counter to an indexer and retrain all the saved models =/\n",
    "\n",
    "    # find the closest input embedding to our context vector\n",
    "    label = _find_nearest_neighbor(model, vec)\n",
    "\n",
    "    print(label)\n",
    "    \n",
    "    # use the label to find the projection we have stored for that embedding\n",
    "    index = model.word_indexer.index_of(label)\n",
    "\n",
    "    logits = model.predictions[index]\n",
    "\n",
    "    return logits\n",
    "\n",
    "predicament_in_context(model, 'eggplant', 'The room was the color of eggplant.', bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56e42d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
