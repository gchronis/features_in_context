{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79ae64f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gsc685/.conda/envs/gabriella_cwr4lsc/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import clustering\n",
    "\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0fd90a-d404-4076-adc2-b0b5d1602338",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28ee9bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/cwr4lsc/feature_prediction/usages_len128.clustering.2.dict', 'rb') as f:\n",
    "    clusters = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2351ddf3",
   "metadata": {},
   "source": [
    "these are the clusters for virtual in coha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a860d2ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KMeans(n_clusters=2, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KMeans</label><div class=\"sk-toggleable__content\"><pre>KMeans(n_clusters=2, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KMeans(n_clusters=2, random_state=42)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters['virtual']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435b8d08",
   "metadata": {},
   "source": [
    "mario has provided us with a function to sample the most prototypical usages from the cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a9b8c0",
   "metadata": {},
   "source": [
    "for that we need to load the usages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24bf7adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/cwr4lsc/feature_prediction/matrix_usages_16_len128.dict', 'rb') as f:\n",
    "    usages = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c297ada1-116d-4ab6-9192-6d623767d0ad",
   "metadata": {},
   "source": [
    "Now we can pull some of the sentences closest to the centroids of the clusters for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf3ffa3-75bf-4e02-95e8-90613bc40433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3981)\n",
      "(1921,)\n",
      "(1921, 3981)\n",
      "[ 888  958  857 1041  916 1632  913  933 1218 1042]\n",
      "[',', '\"', 'or', '\"', 'vr', ',', '\"', 'seems', 'to', 'have', 'won', 'out', 'in', 'common', 'par', '##lance', '.', 'the', 'term', 'hooks', 'us', 'with', 'the', 'excitement', 'of', 'creating', 'and', 'experiencing', 'different', 'realities', '.', 'there', 'is', 'also', 'an', 'ongoing', 'debate', 'over', 'exactly', 'what', 'virtual', 'reality', 'is', 'and', 'what', 'it', 'is', 'not', '.', 'most', 'observers', 'agree', 'that', 'one', 'necessary', 'characteristic', 'is', 'that', 'you', 'can', 'navigate', 'in', 'a', 'virtual', 'world', 'with', 'some', 'degree', 'of', 'immersion', ',', 'interact', '##ivity', ',', 'and', 'a', 'speed', 'close', 'to', 'real', 'time', '.', 'h', '##ype', 'and', 'reality', 'right', 'now', ',', 'there', 'is', 'a', 'great', 'deal', 'of', 'h', '##ype', 'surrounding', 'virtual', 'reality', '.', 'the', 'technology', ',', 's', 'present', 'state', 'of', 'advancement', 'has', 'been', 'overs', '##tated', '.', 'coverage', 'in', 'numerous', 'magazines', 'and', 'newspaper', 'articles', ',', 'on', 'tv', 'shows', ',']\n",
      "63\n",
      "you can navigate in a [[virtual]] world with some degree of\n",
      "['in', 'virtual', 'worlds', '.', 'sm', 'virtual', 'sex', 'is', 'a', 'hot', 'topic', '.', 'it', 'has', 'been', 'labeled', 'tel', '##ed', '##il', '##don', '##ics', 'by', 'several', 'authors', '.', 'at', 'least', 'one', 'virtual', '-', 'reality', 'company', ',', 'thinking', 'software', 'of', 'woods', '##ide', ',', 'new', 'york', ',', 'is', 'selling', 'a', '\"', 'cyber', '##se', '##x', 'machine', ',', '\"', 'and', 'there', 'are', 'some', 'multimedia', 'sexual', 'experiences', 'that', 'come', 'close', 'to', 'virtual', 'reality', '.', 'these', 'products', 'are', 'not', 'very', 'advanced', ',', 'however', ',', 'and', 'many', 'obstacles', 'must', 'be', 'overcome', 'to', 'produce', 'a', 'satisfying', 'ta', '##ct', '##ile', 'experience', '.', 'but', 'expect', 'great', 'strides', 'to', 'be', 'made', 'in', 'creating', 'advanced', '\"', 'sex', 'machines', '\"', 'using', 'virtual', '##real', '##ity', 'technology', '.', 'there', 'is', 'too', 'much', 'potential', 'profit', 'in', 'these', 'applications', 'for', 'them', 'not', 'to', 'be', 'pursued', '.']\n",
      "63\n",
      "experiences that come close to [[virtual]] reality . these products are\n",
      "['will', 'have', 'many', 'servants', '.', 'but', 'the', 'servants', 'will', 'be', 'electronic', '.', 'already', ',', 'television', 'gives', 'each', 'home', 'the', 'equivalent', 'of', 'a', 'court', 'je', '##ster', '.', 'in', '2020', ',', 'your', 'personal', 'court', 'je', '##ster', 'will', 'have', 'a', 'better', 'sense', 'of', 'humor', ';', 'it', 'will', 'respond', 'to', 'you', 'and', 'even', 'laugh', 'at', 'your', 'jokes', '.', 'in', 'the', 'amusement', 'room', ',', 'your', 'children', 'may', 'play', 'virtual', '-', 'reality', 'games', ',', 'which', 'will', 'allow', 'them', 'to', 'fight', 'violent', 'battles', 'or', 'explore', 'the', 'outer', 'regions', 'of', 'space', '.', 'every', 'home', 'will', 'have', 'the', 'equivalent', 'of', 'a', 'theme', 'park', '.', 'photograph', 'photograph', '/', '/', 'there', 'may', 'be', 'an', 'information', 'room', 'where', 'you', 'can', 'learn', ',', 'teach', ',', 'and', 'communicate', '.', 'the', 'information', 'room', 'would', 'develop', 'from', 'today', \"'\", 's', 'home', 'office']\n",
      "63\n",
      ", your children may play [[virtual]] - reality games , which\n",
      "['may', 'think', 'they', 'understand', 'relationships', 'based', 'on', 'online', 'experiences', ',', 'and', 'that', 'can', 'be', 'damaging', '.', '\"', '<', 'p', '>', 'l', ':', 'smart', 'rules', 'for', 'virtual', 'play', 'realistic', '##ally', ',', 'we', 'ca', 'n', \"'\", 't', 'raise', 'internet', '-', 'free', 'kids', '-', '-', 'nor', 'should', 'we', '.', 'but', 'here', \"'\", 's', 'what', 'i', \"'\", 've', 'learned', 'about', 'helping', 'your', 'child', 'find', 'a', 'balance', 'between', 'virtual', 'worlds', 'and', 'the', 'real', 'one', '.', '<', 'p', '>', '*', 'be', 'his', 'cop', '##ilo', '##t', 'understanding', 'your', 'kid', \"'\", 's', 'virtual', 'world', 'is', 'a', 'must', ',', 'even', 'if', 'it', 'means', 'going', 'undercover', 'yourself', '.', 'then', 'go', 'online', 'with', 'him', 'as', 'he', 'explores', 'the', 'site', '.', '\"', 'you', 'would', 'n', \"'\", 't', 'put', 'your', 'child', 'in', 'a', 'car', ',', 'hand', 'him', 'the', 'keys']\n",
      "63\n",
      "child find a balance between [[virtual]] worlds and the real one\n",
      "['and', 'productivity', ',', 'can', 'be', 'represented', 'in', 'three', 'dimensions', 'for', 'analysis', '.', 'various', '\"', 'what', '-', 'if', '\"', 'scenarios', 'could', 'be', 'proposed', '.', 'a', 'company', 'could', 'also', 'use', 'this', 'system', 'to', 'watch', 'its', 'actual', 'operation', 'in', 'real', 'time', 'rather', 'than', 'in', 'simulation', '.', 'disabilities', '.', 'several', 'organizations', ',', 'such', 'as', 'prairie', 'software', 'and', 'hines', 'veterans', \"'\", 'hospital', 'in', 'illinois', ',', 'are', 'experimenting', 'with', 'virtual', 'reality', 'to', 'confirm', 'the', 'accessibility', 'of', 'buildings', 'for', 'people', 'with', 'disabilities', '.', 'one', 'university', ',', 'oregon', 'research', 'institute', ',', 'has', 'created', 'a', 'program', 'that', 'teaches', 'children', 'to', 'operate', 'wheelchair', '##s', '.', 'another', ',', 'the', '@', '@', '@', '@', '@', '@', '@', '@', '@', '@', 'mentally', 're', '##tar', '##ded', 'students', 'how', 'to', 'ride', 'a', 'bus', '.', 'just', 'beginning', 'are', 'many', 'other', 'applications', 'aimed']\n",
      "63\n",
      "illinois , are experimenting with [[virtual]] reality to confirm the accessibility\n",
      "['.', '<', 'p', '>', 'the', 'surreal', 'hr', '##c', 'classroom', '.', 'participants', \"'\", 'voices', 'portrayed', 'the', 'hr', '##c', 'classroom', 'as', 'a', 'surreal', 'and', 'int', '##ang', '##ible', 'place', '.', 'they', 'articulated', 'the', 'difficulty', 'of', 'interpreting', 'an', 'abstract', 'concept', 'in', 'a', 'virtual', 'world', 'and', 'then', 'attempting', 'to', 'apply', 'it', 'in', 'their', 'real', 'world', '.', 'to', 'make', 'the', 'classroom', 'feel', 'real', ',', 'they', 'needed', 'to', 'connect', 'the', 'virtual', 'world', 'to', 'their', 'real', 'world', 'through', 'an', 'ina', '##ni', '##mate', 'object', ',', 'a', 'computer', ',', 'and', 'int', '##ang', '##ible', 'technology', '.', 'as', 'they', 'logged', 'into', 'their', 'course', 'management', 'system', '@', '@', '@', '@', '@', '@', '@', '@', '@', '@', 'course', 'documents', ',', 'interacting', 'with', 'other', 'students', 'on', 'the', 'class', 'discussion', 'board', ',', 'interacting', 'with', 'their', 'instructors', 'through', 'the', 'course', '##ware', 'email', ',']\n",
      "63\n",
      "they needed to connect the [[virtual]] world to their real world\n",
      "['trading', 'will', 'greatly', 'increase', 'in', 'the', 'future', '.', 'those', 'companies', 'trading', 'on', 'various', 'stock', 'markets', 'globally', 'will', 'require', 'this', 'virtual', '-', 'reality', 'application', 'to', 'identify', 'trends', 'and', 'make', 'trades', 'more', 'rapidly', '.', 'they', 'will', ',', 'in', 'fact', ',', 'be', 'interacting', 'with', 'the', 'stock', 'market', 'in', 'real', 'time', '.', 'their', 'work', 'will', 'be', 'much', 'like', 'playing', 'a', 'large', 'and', 'complex', 'video', 'game', '.', 'some', 'virtual', '-', 'reality', 'software', 'developers', 'have', 'been', 'working', 'on', 'a', 'product', 'called', 'flows', '##hee', '##t', '.', 'it', 'will', 'be', 'like', 'a', 'spreads', '##hee', '##t', ',', 'but', 'will', 'show', 'more', 'than', 'mere', 'numbers', 'displayed', 'in', 'two', '-', 'dimensional', 'columns', 'and', 'rows', '.', 'rather', ',', 'it', 'will', 'give', 'a', 'three', '##dim', '##ens', '##ional', 'depiction', 'of', 'numbers', 'with', 'varying', 'sizes', ',', 'shapes', ',', 'colors', ',', 'and']\n",
      "63\n",
      "complex video game . some [[virtual]] - reality software developers have\n",
      "['a', 'lack', 'of', 'computing', 'power', 'and', 'the', 'high', 'cost', 'of', 'most', 'virtual', '-', 'reality', 'equipment', ',', 'but', 'advanced', 'virtual', 'reality', 'is', 'set', 'to', 'invade', 'the', 'home', 'entertainment', 'scene', 'in', 'the', 'years', 'ahead', '.', 'while', 'stand', '-', 'alone', 'entertainment', 'systems', 'will', 'be', 'offered', ',', 'perhaps', 'the', 'most', 'important', 'form', 'of', 'home', 'vr', 'will', 'come', 'over', 'the', 'internet', ',', 'and', 'with', 'it', 'the', 'potential', 'for', 'virtual', 'reality', 'to', 'promote', 'human', 'interaction', 'over', 'wide', 'distances', '.', 'imagine', 'an', 'adventure', 'game', 'in', 'which', 'you', 'are', 'immersed', 'in', 'a', 'three', '##dim', '##ens', '##ional', 'world', ',', 'interacting', 'with', 'other', 'participants', '.', 'it', 'can', 'become', 'a', 'real', ',', '@', '@', '@', '@', '@', '@', '@', '@', '@', '@', 'a', 'participant', 'interacting', 'with', 'the', 'plot', 'and', 'other', 'characters', '.', 'while', 'these', 'kinds', 'of', 'entertainment']\n",
      "63\n",
      "with it the potential for [[virtual]] reality to promote human interaction\n",
      "['technology', 'that', 'allowed', 'you', 'to', 'ride', 'the', 'spiders', 'in', 'brazil', '.', '\"', '<', 'p', '>', '\"', 'that', 'was', 'n', \"'\", 't', 'fantasy', ',', '\"', 'i', 'said', '.', '<', 'p', '>', '\"', 'no', ',', 'no', ',', 'of', 'course', 'not', '.', 'i', \"'\", 'm', 'not', 'explaining', 'this', 'well', ',', 'am', 'i', '?', '\"', 'he', 'rubbed', 'his', 'forehead', '.', '\"', 'the', 'sophisticated', 'sensory', 'array', 'of', 'the', 'virtual', '##s', ',', 'together', 'with', 'the', 'use', 'of', 'virtual', 'remote', '@', '@', '@', '@', '@', '@', '@', '@', '@', '@', 'research', ',', 'new', 'applications', 'of', 'virtual', 'reality', '.', 'what', 'i', \"'\", 'm', 'trying', 'to', 'say', '-', '-', '\"', '<', 'p', '>', '\"', 'what', 'he', \"'\", 's', 'trying', 'to', 'say', ',', '\"', 'said', 'pang', '##born', ',', 'her', 'voice', 'dry', ',', '\"', 'is', 'that', 'we']\n",
      "63\n",
      "sophisticated sensory array of the [[virtual]] ##s , together with the\n",
      "[':', 'smart', 'rules', 'for', 'virtual', 'play', 'realistic', '##ally', ',', 'we', 'ca', 'n', \"'\", 't', 'raise', 'internet', '-', 'free', 'kids', '-', '-', 'nor', 'should', 'we', '.', 'but', 'here', \"'\", 's', 'what', 'i', \"'\", 've', 'learned', 'about', 'helping', 'your', 'child', 'find', 'a', 'balance', 'between', 'virtual', 'worlds', 'and', 'the', 'real', 'one', '.', '<', 'p', '>', '*', 'be', 'his', 'cop', '##ilo', '##t', 'understanding', 'your', 'kid', \"'\", 's', 'virtual', 'world', 'is', 'a', 'must', ',', 'even', 'if', 'it', 'means', 'going', 'undercover', 'yourself', '.', 'then', 'go', 'online', 'with', 'him', 'as', 'he', 'explores', 'the', 'site', '.', '\"', 'you', 'would', 'n', \"'\", 't', 'put', 'your', 'child', 'in', 'a', 'car', ',', 'hand', 'him', 'the', 'keys', ',', 'and', 'say', \"'\", 'see', 'ya', ',', \"'\", '\"', 'says', 'per', '##le', '.', '\"', 'do', 'n', \"'\", 't', 'do', 'it', 'with']\n",
      "63\n",
      "understanding your kid ' s [[virtual]] world is a must ,\n"
     ]
    }
   ],
   "source": [
    "# now i want ot get the nearest neighbors of each cluster...the prototypes function doesnt work so here is my bversion of it. \n",
    "\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "\n",
    "word = 'virtual'\n",
    "window = 5\n",
    "U_w, contexts, positions, t_labels = usages[word]\n",
    "cluster_id = 1\n",
    "n = 10\n",
    "\n",
    "\n",
    "print(clusters['virtual'].cluster_centers_.shape)\n",
    "\n",
    "centers = clusters['virtual'].cluster_centers_.shape\n",
    "\n",
    "print(clusters['virtual'].labels_.shape)\n",
    "\n",
    "labels = clusters['virtual'].labels_.shape\n",
    "\n",
    "print(usages['virtual'][0].shape)\n",
    "\n",
    "nearest = np.argsort(cdist([clusters['virtual'].cluster_centers_[cluster_id]], usages['virtual'][0]), axis=1)[:, -n:]\n",
    "\n",
    "print(nearest[0])\n",
    "\n",
    "\n",
    "for i in nearest[0]:\n",
    "    sent = contexts[i]\n",
    "    print(sent)\n",
    "    pos = positions[i]\n",
    "    print(pos)\n",
    "\n",
    "    #assert sent[pos] == word\n",
    "    sent[pos] = '[[{}]]'.format(word)\n",
    "    sent = sent[pos - window: pos + window + 1]\n",
    "    sent = ' '.join(sent)\n",
    "\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83db453f-bd39-4fe8-b86e-4180fa689265",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prototypes_1(word, clustering, usages, n=5, window=10):\n",
    "    num_clusters = len(clustering.cluster_centers_)\n",
    "    U_w, contexts, positions, t_labels = usages\n",
    "\n",
    "    \n",
    "    prototypes = []\n",
    "\n",
    "    for cluster_id in range(0, num_clusters):\n",
    "        #print(\"cluster_id\", cluster_id)\n",
    "        #print(clustering.cluster_centers_.shape)\n",
    "\n",
    "        centers = clustering.cluster_centers_.shape\n",
    "\n",
    "        #print(clustering.labels_.shape)\n",
    "\n",
    "        labels = clustering.labels_.shape\n",
    "\n",
    "        #print(usages[0].shape) # 0 index is the vectors\n",
    "\n",
    "        nearest = np.argsort(cdist([clustering.cluster_centers_[cluster_id]], U_w), axis=1)[:, -n:]\n",
    "\n",
    "        #print(nearest[0])\n",
    "        \n",
    "        this_cluster_prototypes = []\n",
    "        for i in nearest[0]:\n",
    "            sent = contexts[i]\n",
    "            #print(sent)\n",
    "            pos = positions[i]\n",
    "            #print(pos)\n",
    "\n",
    "            #assert sent[pos] == word\n",
    "            sent[pos] = '[[{}]]'.format(word)\n",
    "            sent = sent[pos - window: pos + window + 1]\n",
    "            sent = ' '.join(sent)\n",
    "            this_cluster_prototypes.append(sent)\n",
    "        prototypes.append(this_cluster_prototypes)\n",
    "        \n",
    "    return(prototypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0536e1c-715a-4fa4-8753-82d5795ea8f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['by boo ##ing : a three - story red - [[brick]] general motors assembly plant sp ##routed workers at every window',\n",
       "  'len ##ox avenue , a large u - shaped red [[brick]] apartment building seventeen stories high . the front garden was',\n",
       "  'to be seen were the fl ##ori ##d red - [[brick]] catholic church and rectory at the end of main street',\n",
       "  'standing on the porch of his two - story red [[brick]] house ready to give them an official welcome to this',\n",
       "  'the fence , at the rear of the red - [[brick]] house , then at dora , and back at the'],\n",
       " ['with the ten ##eb ##rous significance of dreams - every [[brick]] and stone , the scrap ##s of news ##print tumbling',\n",
       "  'drove through little towns , secure and complete in their [[brick]] and white simplicity . they passed the lake several times',\n",
       "  'the most visible component of police presence , the imposing [[brick]] and stone structures , was no less threatening to residents',\n",
       "  \"##s of all - too - familiar sets line the [[brick]] and cobb ##les ##tone streets . is n ' t\",\n",
       "  'as i had half expected , simply one of many [[brick]] and timber colonial ##s bunker ##ed in the surrounding hills'],\n",
       " ['of the nurse , and refrain ##ed from throwing a [[brick]] at the bu ##x ##om lady , which was a',\n",
       "  'go again , \" said donald , \" throwing a [[brick]] into the most delicate mechanism of my profound thought .',\n",
       "  'did me a good turn by hitting me with a [[brick]] because that way i got to be a scout ?',\n",
       "  'i \\' m going to hit you back with a [[brick]] , \" sit ##rick snort ##s in his 1998 self',\n",
       "  \"she ' d been struck on the forehead with a [[brick]] . the person ' s face seemed lit from within\"]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_prototypes_1('brick', clusters['brick'], usages['brick'], n=5, window=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2f08ca-8088-499a-a59d-4f3cad8c490a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['@ @ carlos romero bar ##cel ##o finished in a [[virtual]] dead heat with former governor rafael hernandez colon , who',\n",
       "  'production in war - rack ##ed italy came to a [[virtual]] halt in 1942 , the year bra ##zzi turned twenty',\n",
       "  '1980 , latin american income growth has slowed to a [[virtual]] halt - to less than 6 percent over twenty years',\n",
       "  'of new york . the popular vote ended in a [[virtual]] tie , but cleveland took the electoral college , 219',\n",
       "  '; reductions in the city university budget , and a [[virtual]] halt to any new @ @ @ @ @ @'],\n",
       " ['the classroom feel real , they needed to connect the [[virtual]] world to their real world through an ina ##ni ##mate',\n",
       "  'like playing a large and complex video game . some [[virtual]] - reality software developers have been working on a product',\n",
       "  'over the internet , and with it the potential for [[virtual]] reality to promote human interaction over wide distances . imagine',\n",
       "  'his forehead . \" the sophisticated sensory array of the [[virtual]] ##s , together with the use of virtual remote @',\n",
       "  \"be his cop ##ilo ##t understanding your kid ' s [[virtual]] world is a must , even if it means going\"]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_prototypes_1('virtual', clusters['virtual'], usages['virtual'], n=5, window=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afda6b5-713c-46ec-824e-098cf7402646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\"software . once installed on a user ' s hard [[disk]] drive , the software creates a large file ( 900\",\n",
       "  '. using the work ##ben ##ch , you can copy [[disk]] ##ettes , view files and load programs by pointing and',\n",
       "  '- 22 . computer owners will not buy the special [[disk]] drives required to play cds on their desktop machines until',\n",
       "  \"photo ( color ) : monster lair ' s compact [[disk]] attachment makes its sound th ##ro ##b . < p\",\n",
       "  'and copy the . ps ##t file from the floppy [[disk]] over it , changing the name if necessary . of'],\n",
       " ['been torn back to make way for the huge gray [[disk]] that blocked its entrance . each day someone swept the',\n",
       "  'they , themselves , were concerned , on a swaying [[disk]] of steel at the centre of an equatorial waste of',\n",
       "  'the fr ##isi ##an islands , i saw a golden [[disk]] on the superstructure of the ship , as if the',\n",
       "  \"' clock when the moon rose , a pale yellow [[disk]] above the hills that rim ##med the valley of the\",\n",
       "  ', and rising slowly over its crest a great silver [[disk]] appeared , bright ##ening as it came and pouring a']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_prototypes_1('disk', clusters['disk'], usages['disk'], n=5, window=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5e6dff-671e-4b81-becb-e19871016708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['er ##udi ##te and highly paid legal staff of the [[parent]] company . cases were started simultaneously before the \" push',\n",
       "  'except for a few bridges ##tones shipped in from the [[parent]] company in japan . fires ##tone argues that its tire',\n",
       "  'gives iran a seat on the supervisory board of the [[parent]] company as well . gradually , cautiously , major oil',\n",
       "  'of subsidiaries of consolidated gas , but includes only the [[parent]] company , as a manufacturer and distributor of artificial gas',\n",
       "  'plan afford ##s an opportunity to purchase stock of the [[parent]] company , standard oil of new jersey , a so'],\n",
       " ['of high - profile child - rear ##ing experts and [[parent]] educators advise against span ##king . the canadian pa ##ed',\n",
       "  'make the situation control the behavior , rather than the [[parent]] controlling it , \" she said . in other words',\n",
       "  'rules , guidelines and boundaries . it means becoming the [[parent]] god intended when he blessed you with the gift of',\n",
       "  'the child needed to see that play more than the [[parent]] needed a help ##er in @ @ @ @ @',\n",
       "  ', despite attacks being made upon it by teacher and [[parent]] groups , will remain unchanged . of the $ 60'],\n",
       " ['abnormal . at the adolescent period those un ##lit for [[parent]] ##hood should be guarded ? girls and boys ? and',\n",
       "  'the children juice boxes . once , these images of [[parent]] ##hood had troubled shu ##kumar , adding to his anxiety',\n",
       "  'all again . \" the couple \\' s devotion to [[parent]] ##hood is hardly surprising , since kids were on michael',\n",
       "  \"and now garrett is left questioning the strict covenant of [[parent]] ##hood : perhaps there ' s some loop ##hole since\",\n",
       "  'that now , is n \\' t it ? \" [[parent]] ##hood changes many things ; a couple \\' s choice'],\n",
       " [\"is . the focus becomes a lot tighter between a [[parent]] and child when there ' s to one else in\",\n",
       "  'loans , most of these scholarships do not require a [[parent]] to submit tax forms or wade through lengthy applications .',\n",
       "  \"two involved problems in the home setting : an abusive [[parent]] and concern about mother ' s live - in boyfriend\",\n",
       "  'about it the other day , and asked if a [[parent]] could legally de ##pr ##ive a daughter of a share',\n",
       "  'something the question def ##lated her . how bad a [[parent]] would she have to be to raise an armed eight'],\n",
       " ['supply , communication , and exchange of personnel with the [[parent]] laboratory in cambridge , and it won the commendation of',\n",
       "  'follows the hard - sell evan ##gel ##ism of its [[parent]] organization , campus crusade for christ . it has a',\n",
       "  \"denver radio consultant not connected with the station or its [[parent]] , jefferson pilot communications . henry calls ky ##go '\",\n",
       "  'council for solid waste solutions shares office space with its [[parent]] , the society of the plastics industry , inc .',\n",
       "  'be able to make significant contributions to education that neither [[parent]] company could make separately . < p > the joint']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_prototypes_1('parent', clusters['parent'], usages['parent'], n=5, window=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b5a90c-fc21-4927-b0df-5e54e389b38a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d15bdd5e-d6a7-4ba2-845a-95d91c446a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'clustering' from '/home/gsc685/features_in_context/notebooks/../clustering.py'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bda1796b-ad12-491b-9435-8ff06e16e268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00812746  0.04887854  0.07403399 ... -0.00152213  0.00082821\n",
      "  -0.00655399]\n",
      " [ 0.02108257  0.06928941  0.02662214 ... -0.00482794  0.00472053\n",
      "  -0.00557994]\n",
      " [ 0.02375785  0.03040661  0.05543758 ... -0.00719667 -0.00230765\n",
      "  -0.00475294]\n",
      " [ 0.00580683 -0.01956674  0.03241314 ... -0.00418058 -0.0047539\n",
      "  -0.00660921]\n",
      " [-0.01225548  0.02018052  0.00766065 ... -0.00272244  0.00297417\n",
      "  -0.0048099 ]]\n",
      "[['the', 'door', 'and', 'window', '@', '@', '@', '@', '@', '@', '@', '@', '@', '@', 'cas', '##ings', 'and', 'the', 'roof', 'are', 'stained', 'brown', '.', 'the', 'leaders', 'are', 'an', 'ox', '##idi', '##zed', 'copper', '.', 'the', 'shutter', '##s', 'and', 'the', 'box', 'tree', 'tub', '##s', 'are', 'a', 'dark', ',', 'vivid', 'green', ',', 'and', 'the', 'walks', 'and', 'terrace', 'are', 'laid', 'with', 'a', \"'\", 'trim', '##ly', 'edged', 'bright', 'red', 'brick', '.', 'the', 'outdoor', 'living', 'room', 'or', 'inc', '##los', '##ed', 'piazza', 'has', 'become', 'an', 'essential', 'in', 'our', 'home', 'building', '.', 'the', 'illustration', 'shows', 'an', 'interesting', 'example', 'of', 'how', 'it', 'can', '-', 'be', 'made', 'an', 'integral', 'part', 'of', 'the', 'house', '.', 'too', 'often', 'a', 'piazza', 'is', 'a', 'flat', '-', 'roofed', 'extension', 'without', 'any', 'real', 'relationship', 'to', 'the', 'general', 'design', '.', 'in', 'another', 'and', 'much'], [',', 'he', 'searches', 'in', 'vain', 'for', 'the', 'state', '##ly', 'mansions', 'of', 'his', 'fancy', '.', 'surely', 'they', 'were', 'not', 'all', 'burned', 'by', 'yankee', 'raiders', 'or', 'riot', '##ous', 'freed', '##men', '.', 'state', '##ly', 'mansions', \"'\", 'is', ',', 'in', 'fact', ',', 'very', 'strong', 'language', '.', 'the', 'traveler', 'would', 'not', 'immediately', 'recognize', 'as', 'des', '##erving', 'it', 'the', 'large', 'two', '-', 'st', '##ori', '##ed', 'house', 'of', 'wood', 'or', 'brick', ',', 'with', 'its', 'double', 'gallery', ',', 'that', 'formed', 'the', 'well', '-', 'to', '-', 'do', 'plant', '##er', \"'\", 's', 'residence', '.', 'the', 'archaic', 'lady', 'of', 'the', 'south', 'obeyed', 'a', 'law', 'of', 'her', 'being', 'in', 'leaving', 'very', 'little', 'written', 'record', 'of', 'herself', '.', 'ladies', 'from', 'the', 'real', 'world', 'penetrated', 'into', 'her', 'territory', 'from', 'time', 'to', 'time', ',', 'and', 'gave', 'accounts', 'of', 'what', 'they', 'saw'], ['.', 'down', 'pearl', 'street', ',', 'to', 'the', 'south', ',', 'the', 'cotton', 'broker', '##s', 'spread', 'their', 'white', 'samples', 'on', 'their', 'hue', '-', 'topped', 'tables', ';', 'to', 'the', 'north', 'were', 'the', 'merchants', 'of', 'tobacco', 'and', 'spice', ';', 'and', 'down', 'the', 'slope', 'of', 'wall', 'street', 'the', 'great', 'east', 'and', 'west', 'india', 'merchants', 'of', 'tea', 'and', 'coffee', 'and', 'sugar', 'sat', 'in', 'their', 'di', '##sma', '##l', 'red', '-', 'brick', 'counting', '-', 'houses', '.', 'below', 'them', ',', 'at', 'the', 'end', 'of', 'the', 'street', ',', 'they', 'saw', 'the', 'spa', '##rs', 'of', 'their', 'square', '-', 'rig', '##gers', 'moore', '##d', 'along', 'south', 'street', ',', 'which', 'ran', 'like', 'a', 'long', 'piazza', 'in', 'front', 'of', 'old', 'new', 'york', '.', 'half', 'way', 'down', 'wall', 'street', ',', 'on', 'top', 'of', 'the', 'hill', 'behind', 'the', 'foreign', 'merchants', ',', 'lay', 'the'], ['@', '@', '@', '@', '@', '@', 'but', 'at', 'length', ',', 'in', '1882', ',', 'in', 'the', 'fu', '##ln', '##ess', 'of', 'his', 'years', ',', 'the', 'last', 'of', 'the', 'great', 'merchant', 'princes', 'died', '.', 'the', 'tow', '-', 'headed', 'boy', 'who', 'had', 'watched', 'the', 'ships', 'come', 'in', ',', 'from', 'the', 'pretty', 'residential', 'park', 'on', 'the', 'battery', ',', 'when', 'new', 'york', 'had', 'been', 'the', 'low', ',', 'red', '-', 'brick', 'town', 'of', 'the', 'sea', 'traders', ',', 'had', 'lived', 'to', 'the', 'day', 'of', 'the', 'great', 'modern', 'corporation', ',', 'and', 'to', 'head', 'the', 'greatest', 'federation', 'of', 'merchants', 'which', 'controlled', 'these', 'great', 'business', 'structures', '.', 'around', 'the', 'city', 'bank', 'were', 'assembled', 'still', 'the', 'sugar', 'merchants', ',', 'the', 'cotton', 'broker', '##s', ',', 'the', 'metal', 'merchants', ',', 'the', 'strongest', 'of', 'the', 'interests', 'of', 'ant', '##hra', '##cite', 'coal'], ['and', 'we', 'all', 'suffered', 'the', 'last', 'ind', '##ign', '##ity', 'of', 'having', 'our', 'persons', 'searched', '.', 'we', 'had', 'nothing', 'left', 'for', 'the', 'policeman', 'in', 'castle', 'garden', '.', 'this', 'last', 'place', 'of', 'detention', 'turned', 'out', 'to', 'be', 'a', 'prison', '.', 'qu', '##aran', '##tine', 'they', 'called', 'it', ',', 'and', 'there', 'was', 'a', 'great', 'deal', 'of', 'it', '?', 'two', 'weeks', 'of', 'it', '.', 'two', 'weeks', 'within', 'high', 'brick', 'walls', ',', 'several', 'hundred', 'of', 'us', 'herd', '##ed', 'in', 'half', 'a', 'dozen', 'compartments', ',', '?', 'numbered', 'compartments', ',', '?', 'sleeping', 'in', 'rows', ',', 'like', 'sick', 'people', 'in', 'a', 'hospital', ';', 'with', 'roll', '-', 'call', 'morning', 'and', 'night', ',', 'and', 'short', 'rations', 'three', 'times', 'a', 'day', ';', 'with', 'never', 'a', 'sign', 'of', 'the', 'free', 'world', 'beyond', 'our', 'barred', 'windows', ';', 'with', 'anxiety', 'and']]\n",
      "[63, 63, 63, 63, 63]\n",
      "[1910, 1910, 1910, 1910, 1910]\n"
     ]
    }
   ],
   "source": [
    "U_w, contexts, positions, t_labels = usages['brick']\n",
    "print(U_w[:5])\n",
    "print(contexts[:5])\n",
    "print(positions[:5])\n",
    "print(t_labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b3e7018-de80-49a8-b87a-3c0cf6dc858e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({2: 3833, 0: 3292, 1: 396})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(clusters['brick'].labels_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1147bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of cluster centers: (3, 3981)\n",
      "{0: None, 1: None, 2: None}\n",
      "{0: [], 1: [], 2: []}\n",
      "number of usages:  7521\n",
      "getting the distances between labels and cluster centers\n",
      "[[ 0.28595546 -0.30922326 -0.29719776 ...  0.1476648   0.47021779\n",
      "   0.505579  ]\n",
      " [-0.43143812 -0.66463988  0.26886116 ... -0.90330876 -0.26752028\n",
      "   0.85099612]\n",
      " [-0.20102162  0.33424481  0.22747352 ... -0.03349915 -0.37621157\n",
      "  -0.52213946]]\n",
      "[[-1.01086269e-02 -7.30435634e-02 -1.70028330e-02 ... -1.88235209e-03\n",
      "  -4.12591029e-04 -8.96971381e-04]\n",
      " [ 1.24214187e-02 -2.99085947e-04 -1.02361564e-02 ... -2.44219559e-03\n",
      "   8.99620583e-04  1.37535246e-03]\n",
      " [-9.10892536e-03 -1.49765026e-03 -4.25341522e-02 ...  5.14178541e-04\n",
      "   9.58790441e-05  5.05714455e-03]\n",
      " ...\n",
      " [-7.76622563e-03 -8.41825636e-04 -3.22426317e-02 ... -1.68196793e-03\n",
      "  -7.87630541e-04  8.05641161e-04]\n",
      " [ 1.69681619e-02  3.23044108e-02  3.42822247e-02 ... -6.20273528e-03\n",
      "  -2.93256973e-03 -6.60937523e-03]\n",
      " [-6.52541525e-03  6.16317176e-03 -9.00135491e-04 ... -6.09091462e-03\n",
      "   3.12260136e-03  2.62637069e-03]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "XA must be a 2-dimensional array.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mclustering\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_prototypes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbrick\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclusters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbrick\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musages\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbrick\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/features_in_context/notebooks/../clustering.py:796\u001b[0m, in \u001b[0;36mget_prototypes\u001b[0;34m(word, clustering, usages, n, window)\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[38;5;28mprint\u001b[39m(clustering\u001b[38;5;241m.\u001b[39mcluster_centers_)\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28mprint\u001b[39m(clusters[\u001b[38;5;28mcls\u001b[39m])\n\u001b[0;32m--> 796\u001b[0m nearest \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(\u001b[43mcdist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclustering\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcluster_centers_\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclusters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[:, \u001b[38;5;241m-\u001b[39mn:]\n\u001b[1;32m    798\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    800\u001b[0m \u001b[38;5;66;03m# loop through cluster-specific indices\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/gabriella_cwr4lsc/lib/python3.10/site-packages/scipy/spatial/distance.py:2916\u001b[0m, in \u001b[0;36mcdist\u001b[0;34m(XA, XB, metric, out, **kwargs)\u001b[0m\n\u001b[1;32m   2913\u001b[0m sB \u001b[38;5;241m=\u001b[39m XB\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m   2915\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(s) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m-> 2916\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mXA must be a 2-dimensional array.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   2917\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sB) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   2918\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mXB must be a 2-dimensional array.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: XA must be a 2-dimensional array."
     ]
    }
   ],
   "source": [
    "clustering.get_prototypes('brick', clusters['brick'], usages['brick'], n=5, window=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74da05cb-1b64-4e9e-be65-deabd4c9a3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering.get_prototypes('virtual', clusters['virtual'], usages['virtual'], n=5, window=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48622e2c-a457-4ff4-9964-44b42f2154f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering.get_prototypes('disk', clusters['disk'], usages['disk'], n=5, window=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a351416a-4555-46d8-80d2-053d7b7ed84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_prototypes('parent', clusters['parent'], usages['parent'], n=5, window=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0332c49-ea8e-4590-9457-f618037a34c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "prototypes for  leaf\n",
      " \n",
      "prototypes for  sleep\n",
      "prototype  0\n",
      "\n",
      "prototype  1\n",
      "\n",
      "prototype  2\n",
      "\n",
      "prototype  3\n",
      "to jim dandy mumbling and moaning in his [[sleep]] , then looks at jim crow . p . 42 and as norden watched he groaned again in his [[sleep]] , and shifted his head on the two pillows under haphazardly swipes at it in his [[sleep]] , wakes @ @ @ @ @ @ @ @ and his dog and little fox jones twitched in their [[sleep]] . the reverend birdsong accepted mrs . likewise ' the man snored lightly and turned in his [[sleep]] . max froze for a moment , then the figure\n",
      "prototype  4\n",
      "\n",
      "prototype  5\n",
      "\n",
      "prototype  6\n",
      "\n",
      "prototype  7\n",
      "\n",
      "prototype  8\n",
      "\n",
      "prototype  9\n",
      "\n",
      " \n",
      "prototypes for  energy\n",
      "prototype  0\n",
      "\n",
      "prototype  1\n",
      "cancellation was provided for in the contract between the atomic [[energy]] commission and the dixon - yates combination , known as the t . v . a . to the atomic [[energy]] commission . then there will be the hardy perennials was responsible for 18 percent . in april the international [[energy]] agency ' s chief economist declared that china would sur john a . mccone , chairman of the atomic [[energy]] commission , disclosed tonight that he would resign when the t . ramey commissioner u . s . atomic [[energy]] commission washington , oct . 2 , 1970 nuclear power\n",
      " \n",
      "prototypes for  coach\n",
      "prototype  0\n",
      "little before one on monday morning . got into the [[coach]] along with mrs lister about half - past one , , glinting silver . behind them came the royal [[coach]] , drawn by four grey horses , with the driver at lerwick , scotland , we boarded a [[coach]] and drove past peat - lined hills to one of the oath of office , traveling in an imposing gilded [[coach]] , and accompanied by his chaplain , mace @ @ closed to boats , nicholas sent her in the light [[coach]] with dick , the second coachman , to drive\n",
      "prototype  1\n",
      "\n",
      " \n",
      "prototypes for  parent\n",
      "prototype  0\n",
      "erudite and highly paid legal staff of the [[parent]] company . cases were started simultaneously before the \" push except for a few bridgestones shipped in from the [[parent]] company in japan . firestone argues that its tire gives iran a seat on the supervisory board of the [[parent]] company as well . gradually , cautiously , major oil of subsidiaries of consolidated gas , but includes only the [[parent]] company , as a manufacturer and distributor of artificial gas plan affords an opportunity to purchase stock of the [[parent]] company , standard oil of new jersey , a so\n",
      "prototype  1\n",
      "\n",
      "prototype  2\n",
      "\n",
      "prototype  3\n",
      "\n",
      "prototype  4\n",
      "\n",
      " \n",
      "prototypes for  signal\n",
      "prototype  0\n",
      "\n",
      "prototype  1\n",
      "\n",
      "prototype  2\n",
      "size or speed . with digital tv , a digital [[signal]] is implanted in a carrier wave , which is years and previously worked for sears and served in the [[signal]] corps ( radar ) . nowadays , the networks make corps area carries out the plan of coordination between the [[signal]] corps and the ham members in that area . while this type is now supplied with some generators used in [[signal]] corps sets . there are also various other kinds of\n",
      "prototype  3\n",
      "\n",
      " \n",
      "prototypes for  brick\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleaf\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m prototypes \u001b[38;5;241m=\u001b[39m \u001b[43mclustering\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_prototypes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclusters\u001b[49m\u001b[43m[\u001b[49m\u001b[43mword\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musages\u001b[49m\u001b[43m[\u001b[49m\u001b[43mword\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, prototype \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(prototypes):\n\u001b[1;32m     10\u001b[0m       \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprototype \u001b[39m\u001b[38;5;124m\"\u001b[39m, i)\n",
      "File \u001b[0;32m~/features_in_context/notebooks/../clustering.py:778\u001b[0m, in \u001b[0;36mget_prototypes\u001b[0;34m(word, clustering, usages, n, window)\u001b[0m\n\u001b[1;32m    774\u001b[0m sent, pos \u001b[38;5;241m=\u001b[39m snippet_coords[\u001b[38;5;28mcls\u001b[39m][i]\n\u001b[1;32m    775\u001b[0m \u001b[38;5;66;03m# sent = sent.split()\u001b[39;00m\n\u001b[1;32m    776\u001b[0m \n\u001b[1;32m    777\u001b[0m \u001b[38;5;66;03m#sent = tokenizer.convert_ids_to_tokens(sent)\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sent[pos] \u001b[38;5;241m==\u001b[39m word\n\u001b[1;32m    779\u001b[0m sent[pos] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[[\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m]]\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(word)\n\u001b[1;32m    780\u001b[0m sent \u001b[38;5;241m=\u001b[39m sent[pos \u001b[38;5;241m-\u001b[39m window: pos \u001b[38;5;241m+\u001b[39m window \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tz = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "for word in clusters.keys():\n",
    "    print(\" \")\n",
    "    print(\"prototypes for \", word)\n",
    "    if word == 'leaf':\n",
    "        continue\n",
    "    prototypes = clustering.get_prototypes(word, clusters[word], usages[word], n=5, window=10)\n",
    "    for i, prototype in enumerate(prototypes):\n",
    "          print(\"prototype \", i)\n",
    "          print(tz.convert_tokens_to_string(prototype))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b36c8dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "virtual_clusters = clusters['virtual']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "432192d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.30642665,  0.22339732,  0.20428522, ..., -0.03316359,\n",
       "        -0.04161427,  0.35041528],\n",
       "       [-0.3265256 , -0.23805026, -0.21768457, ...,  0.03533884,\n",
       "         0.04434381, -0.37339951]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "virtual_clusters.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c842f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 3981 features, but PLSRegression is expecting 768 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# print top ten features for each prototype\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word_centroid \u001b[38;5;129;01min\u001b[39;00m virtual_clusters\u001b[38;5;241m.\u001b[39mcluster_centers_:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# take each prototype in the bundle and predict features separately\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m#predict_for_single_context_vector(apple_centroid, bert, mpro_model)\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     feats \u001b[38;5;241m=\u001b[39m \u001b[43mbuchanan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_top_n_features_from_single_context_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvirtual\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_centroid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_vec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(feats)\n",
      "File \u001b[0;32m~/features_in_context/notebooks/../src/models.py:108\u001b[0m, in \u001b[0;36mFeatureClassifier.predict_top_n_features_from_single_context_vector\u001b[0;34m(self, word, n, input_vec, output_vec)\u001b[0m\n\u001b[1;32m    106\u001b[0m     logits \u001b[38;5;241m=\u001b[39m output_vec\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 108\u001b[0m     word, logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_from_single_context_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_vec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# https://stackoverflow.com/questions/6910641/how-do-i-get-indices-of-n-maximum-values-in-a-numpy-array\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# Newer NumPy versions (1.8 and up) have a function called argpartition for this. To get the indices of the four largest elements, do\u001b[39;00m\n\u001b[1;32m    112\u001b[0m ind \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margpartition(logits, \u001b[38;5;241m-\u001b[39mn)[\u001b[38;5;241m-\u001b[39mn:]\n",
      "File \u001b[0;32m~/features_in_context/notebooks/../src/plsr.py:63\u001b[0m, in \u001b[0;36mPLSRClassifier.predict_from_single_context_vector\u001b[0;34m(self, word, vec)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# the model needs a vertical vector\u001b[39;00m\n\u001b[1;32m     62\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 63\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# and it returns a verticla vector. our code wants it to be horizontal again, so we reshape again\u001b[39;00m\n\u001b[1;32m     65\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/gabriella_cwr4lsc/lib/python3.10/site-packages/sklearn/cross_decomposition/_pls.py:467\u001b[0m, in \u001b[0;36m_PLS.predict\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Predict targets of given samples.\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \n\u001b[1;32m    447\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;124;03mspace.\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    466\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 467\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;66;03m# Normalize\u001b[39;00m\n\u001b[1;32m    469\u001b[0m X \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_x_mean\n",
      "File \u001b[0;32m~/.conda/envs/gabriella_cwr4lsc/lib/python3.10/site-packages/sklearn/base.py:588\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    585\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 588\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.conda/envs/gabriella_cwr4lsc/lib/python3.10/site-packages/sklearn/base.py:389\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[0;32m--> 389\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    390\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    391\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    392\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: X has 3981 features, but PLSRegression is expecting 768 features as input."
     ]
    }
   ],
   "source": [
    "buchanan = buchanan = torch.load('../trained_models/model.plsr.buchanan.allbuthomonyms.5k.300components.500max_iters')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6223a9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "now we remap the data to have one row for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ca8569e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['desert', 'give', 'leave', 'up', 'withdraw', 'belly', 'body', 'middle', 'muscle', 'organ']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "feature_map = buchanan.feature_norms.feature_map\n",
    "feature_labels = [str(feature_map.get_object(i)) for i in range(0, len(feature_map))]\n",
    "print(feature_labels[:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02e7d0a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3981"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# model dimensions dont have full 65 oops who knows what happened if our original downloaded data was bunk\n",
    "len(feature_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bfbf96a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = []\n",
    "prototype_ids =[]\n",
    "#nouns = []\n",
    "#positions = []\n",
    "features = []\n",
    "values = []\n",
    "\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for word in clusters.keys():\n",
    "    centroids = clusters[word].cluster_centers_\n",
    "    for cluster_id, cluster in enumerate(centroids):\n",
    "\n",
    "        prediction = centroids[cluster_id]\n",
    "        i=0\n",
    "        for value in prediction:\n",
    "            targets.append(word)\n",
    "            prototype_ids.append(cluster_id)\n",
    "            features.append(feature_labels[i])\n",
    "            values.append(value)\n",
    "            i+=1\n",
    "\n",
    "        #prototypes = clustering.get_prototypes(word, clusters[word], usages[word], n=5, window=10)\n",
    "\n",
    "    tidy_df = pd.DataFrame.from_records(\n",
    "        {\"target\": targets,\n",
    "        \"prototype_id\": prototype_ids, \n",
    "        #\"prototype_sentences\": prototypes, \n",
    "        \"feature\": features,\n",
    "        \"value\": values,\n",
    "        }\n",
    "    )\n",
    "    tidy_df.to_csv('../data/cwr4lsc/results/cluster_centroid_features.csv')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "13734499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>prototype_id</th>\n",
       "      <th>target</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>desert</td>\n",
       "      <td>0</td>\n",
       "      <td>leaf</td>\n",
       "      <td>-1.186869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>give</td>\n",
       "      <td>0</td>\n",
       "      <td>leaf</td>\n",
       "      <td>-1.121639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>leave</td>\n",
       "      <td>0</td>\n",
       "      <td>leaf</td>\n",
       "      <td>-1.951784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>up</td>\n",
       "      <td>0</td>\n",
       "      <td>leaf</td>\n",
       "      <td>0.149296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>withdraw</td>\n",
       "      <td>0</td>\n",
       "      <td>leaf</td>\n",
       "      <td>-1.602879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    feature  prototype_id target     value\n",
       "0    desert             0   leaf -1.186869\n",
       "1      give             0   leaf -1.121639\n",
       "2     leave             0   leaf -1.951784\n",
       "3        up             0   leaf  0.149296\n",
       "4  withdraw             0   leaf -1.602879"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tidy_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762f3ce4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gabriella_cwr4lsc",
   "language": "python",
   "name": "gabriella_cwr4lsc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
